# 双节点智能体开发示例

## 概述

双节点智能体在单节点智能体的基础上，通过节点串联实现更复杂的工作流。本示例展示如何将两个 Chat Node 节点串联：第一个节点（`generate_node`）负责生成内容并调用工具，第二个节点（`polish_node`）负责对第一个节点的输出进行润色和风格化处理。

**工作流示意图：**
```
输入 → generate_node（生成+工具调用） → polish_node（润色） → 输出
```

## 开发步骤

### 1. 注册工具

与单节点智能体相同，需要先注册所需的工具。详细步骤请参考单节点智能体开发文档。

### 2. 配置第一个节点（Generate Node）

创建并配置第一个 Chat Node，用于生成内容和调用工具：

```c++
chat_node::chat_node_settings s_generate;
s_generate.model = "gpt-4.1";           // 使用功能更强的模型
s_generate.temperature = 0.7;
s_generate.top_p = 0.95;
s_generate.max_tokens = 4096;
s_generate.tool_choice = "auto";        // 允许自动调用工具
s_generate.tools_json = tool_node::tool_node::get_all_tools_json();
const auto generate_node = std::make_shared<chat_node::EchoChatNode<std::string, std::string>>(s_generate);
```

### 3. 配置第二个节点（Polish Node）

创建并配置第二个 Chat Node，用于润色第一个节点的输出：

```c++
chat_node::chat_node_settings s_polish;
s_polish.model = "gpt-4o-mini";         // 使用更经济的模型进行润色
s_polish.temperature = 0.7;
s_polish.top_p = 0.95;
s_polish.max_tokens = 4096;
s_polish.tool_choice = "none";          // 润色节点不需要调用工具
const auto polish_node = std::make_shared<chat_node::EchoChatNode<std::string, std::string>>(s_polish);
```

### 4. 配置节点预处理和后处理`高级应用`

HADK 支持为每个节点设置预处理（Preprocessor）和后处理（Postprocessor）函数，用于在节点执行前后对数据进行定制化处理。

#### 4.1 设置预处理函数

预处理函数在节点执行前对输入数据进行修改，常用于 Prompt Engineering（PE）优化：

```c++
polish_node->setPreprocessor([](const std::string& in) -> std::string {
    HYB_LOG_INFO("polish_node input: {}", in);
    
    // 解析输入 JSON
    nlohmann::json inJson = nlohmann::json::parse(in);
    
    // 提取最后一个消息的内容
    std::string content = inJson.back()["content"].get<std::string>();
    
    // 构建润色提示词
    std::string prompt = R"(
### CONTEXT
polish the following content:
Content: )" + content + R"(
## YOUR ANSWER:
Provide a comprehensive answer using the content.)";

    // 更新消息内容
    inJson.back()["content"] = prompt;
    return inJson.dump();
});
```

#### 4.2 设置后处理函数

后处理函数在节点执行后对输出数据进行修改，常用于结果解析和格式化：

```c++
polish_node->setPostprocessor([](const std::string& output) -> std::string {
    HYB_LOG_INFO("polish_node output: {}", output);
    // 可以在这里对输出进行解析、格式化等操作
    return output;
});
```

### 5. 串联节点

使用 `chain` 函数将两个节点串联，建立数据流向：

```c++
chain(generate_node, polish_node, "polish");
```

**参数说明：**
- 第一个参数：源节点（`generate_node`）
- 第二个参数：目标节点（`polish_node`）
- 第三个参数：连接名称（可选，用于标识连接）

**执行流程：**
1. `generate_node` 接收输入并生成内容（可能调用工具）
2. `generate_node` 的输出自动传递给 `polish_node`
3. `polish_node` 对输入进行润色处理
4. `polish_node` 的输出作为最终结果返回

### 6. 创建工作流并执行

```c++
// 创建工作流
auto f = std::make_shared<nodeflow::Flow>();

// 设置起始节点（必须是工作流的第一个节点）
f->start(generate_node);

// 执行工作流
auto result = f->runWithInput<std::string, std::string>(message);
```

**参数说明：**
- 模板参数 `IN`：输入数据类型
- 模板参数 `OUT`：输出数据类型
- `message`：实际的输入数据（OpenAI Chat Completion 格式的 JSON 字符串）

**返回值：**
- 工作流的最终输出结果，类型为 `OUT`

## 完整示例

以下是一个完整的双节点智能体实现示例：

```c++
#include <chat_node.h>
#include <log_util.hpp>
#include <nodeflow.hpp>
#include <tool_node.h>
#include <web_search.h>
#include <nlohmann/json.hpp>

// 注册本地工具
tool_node::tool_node::add_function_call<search_news_tool>(
    R"({"type":"function","function":{"name":"search_news","description":"搜索最新新闻信息","parameters":{"type":"object","properties":{"query":{"type":"string","description":"搜索关键词"},"time_range":{"type":"string","enum":["day","week"]},"country":{"type":"string","enum":["china","usa","japan"]}},"required":["query"]}}})"
);

// 注册远程工具服务器
tool_node::tool_node::add_server(
    R"({"WeatherServer":{"url":"http://18.119.131.41:8006","sse_endpoint":"/sse"}})"
);

std::string call_tool_impl_cpp(const std::string& message)
{
    try
    {
        HYB_LOG_INFO("===== call_tool_impl_cpp BEGIN =====");

        // 创建工作流
        auto f = std::make_shared<nodeflow::Flow>();

        // 配置第一个节点：生成节点（支持工具调用）
        chat_node::chat_node_settings s_generate;
        s_generate.model = "gpt-4.1";
        s_generate.temperature = 0.7;
        s_generate.top_p = 0.95;
        s_generate.max_tokens = 4096;
        s_generate.tool_choice = "auto";
        s_generate.tools_json = tool_node::tool_node::get_all_tools_json();
        const auto generate_node = std::make_shared<chat_node::EchoChatNode<std::string, std::string>>(s_generate);

        // 配置第二个节点：润色节点（不调用工具）
        chat_node::chat_node_settings s_polish;
        s_polish.model = "gpt-4o-mini";
        s_polish.temperature = 0.7;
        s_polish.top_p = 0.95;
        s_polish.max_tokens = 4096;
        s_polish.tool_choice = "none";
        const auto polish_node = std::make_shared<chat_node::EchoChatNode<std::string, std::string>>(s_polish);

        // 设置润色节点的预处理函数
        polish_node->setPreprocessor([](const std::string& in) -> std::string {
            HYB_LOG_INFO("polish_node input: {}", in);
            
            // 解析输入 JSON
            nlohmann::json inJson = nlohmann::json::parse(in);
            
            // 提取最后一个消息的内容
            std::string content = inJson.back()["content"].get<std::string>();
            
            // 构建润色提示词
            std::string prompt = R"(
### CONTEXT
polish the following content:
Content: )" + content + R"(
## YOUR ANSWER:
Provide a comprehensive answer using the content.)";

            // 更新消息内容
            inJson.back()["content"] = prompt;
            return inJson.dump();
        });

        // 设置润色节点的后处理函数
        polish_node->setPostprocessor([](const std::string& output) -> std::string {
            HYB_LOG_INFO("polish_node output: {}", output);
            // 可以在这里对输出进行解析、格式化等操作
            return output;
        });

        // 串联节点：generate_node -> polish_node
        chain(generate_node, polish_node, "polish");

        // 设置工作流的起始节点
        f->start(generate_node);

        // 执行工作流
        auto result = f->runWithInput<std::string, std::string>(message);

        return result;
    }
    catch (const std::exception& ex)
    {
        HYB_LOG_ERROR(std::string("call_tool_impl_cpp exception: ") + ex.what());
        return R"({"ok":false,"error":"exception"})";
    }
    catch (...)
    {
        HYB_LOG_ERROR("call_tool_impl_cpp unknown exception");
        return R"({"ok":false,"error":"unknown exception"})";
    }
}

int main()
{
    // 构建输入消息（OpenAI Chat Completion 格式）
    nlohmann::json inputJson = nlohmann::json::array();
    inputJson.push_back({
        {"role", "system"},
        {"content", "you are a helper"}
    });
    inputJson.push_back({
        {"role", "user"},
        {"content", "hello there"}
    });
    
    // 调用智能体并获取响应
    std::string response = call_tool_impl_cpp(inputJson.dump());
    
    // 响应包含完整的对话历史
    std::cout << response << std::endl;

    return 0;
}
```
