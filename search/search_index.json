{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"HADK Development Documentation","text":"<p>Welcome to the HADK framework development documentation. </p> <p>We came here to code a Hybrid Agent and chew bubblegum... and I'm all out of bubblegum !</p>"},{"location":"#what-it-is","title":"What It Is","text":"<p>HADK (Hybrid Agent Development Kit) is a C++-based, cross-platform framework for building intelligent agents. Built on computational graph principles, it offers a modular architecture with rich functional nodes and flexible orchestration. Developers can quickly create agents with advanced reasoning, tool integration, and multimodal capabilities that run seamlessly across Windows, Linux, and Android.</p>"},{"location":"#its-advantages","title":"Its Advantages","text":"<ul> <li>Cross-platform: Write once, run on Windows, Linux, and Android with a unified API</li> <li>Node-based Architecture: Computational graph-based orchestration with conditional routing, nested flows, and loops</li> <li>Type Safety: Compile-time type checking via C++ templates to catch errors early</li> <li>High Performance: Native compilation with zero-copy optimization and concurrent processing</li> <li>Tool Ecosystem: Integrate local tools, remote servers (MCP/SSE), or build your own</li> <li>Modular: Clean separation of concerns for easy extension and reuse</li> </ul>"},{"location":"#development-environment","title":"Development Environment","text":"<ul> <li>C++ Compiler: Supports C++17 or higher</li> <li>CMake: Version 3.24 or higher</li> <li>Operating System: Windows 10+, Linux (Ubuntu 20.04+), Android (API 21+)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#basic-concepts","title":"Basic Concepts","text":"<ul> <li>Chat Node - Chat node integrated with large language models</li> <li>Tool Node - Tool management node</li> <li>Custom Node - User-defined node for custom logic</li> <li>CE Node - Context engine node for managing conversation history</li> <li>Chain and Flow - Workflow management</li> <li>Route - Routing node</li> </ul>"},{"location":"#application-development","title":"Application Development","text":"<ul> <li>Single Node Agent - Single node agent development example</li> <li>Normal Agent - Normal agent development example</li> <li>Three Node Agent - Three node agent development example</li> <li>Inja Template Formatting - Inja template engine tutorial</li> <li>CoT Agent - CoT (Chain of Thought) agent development example</li> <li>Batch Node - Batch Node development example</li> <li>Chat Bot - Simple Chat Bot development example</li> </ul>"},{"location":"#android","title":"Android","text":"<ul> <li>Android Platform - Cross Compilation and Android Project</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Please download the HADK dynamic library from this link</p> <p>Selecting the chapter you're interested in from the left navigation bar to start reading. </p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>Thanks to the following open-source projects for their support:</p> <ul> <li>cpr - HTTP client</li> <li>nlohmann/json - JSON library</li> <li>spdlog - Logging library</li> <li>yaml-cpp - YAML parsing library</li> <li>inja - Template engine</li> </ul>"},{"location":"#contact","title":"Contact","text":"<ul> <li>liusong9@lenovo.com</li> <li>zengjl1@lenovo.com</li> <li>moutz1@lenovo.com</li> <li>xufeng8@lenovo.com</li> </ul>"},{"location":"1-1_BasicConceptsChatNode/","title":"Chat Node","text":"<p>Chat Node is a node component that integrates large language models (LLM) and function calling capabilities, supporting both online and offline model invocation. Its interface strictly follows the OpenAI Chat Completion API specification.</p>"},{"location":"1-1_BasicConceptsChatNode/#building-a-chat-node","title":"Building a Chat Node","text":"<p>Build a chat node in the following way, allowing parameter configuration for model invocation:</p> <pre><code>chat_node::chat_node_settings s_generate;\ns_generate.llm_mode = chat_node::LLMMode::OpenAI;  //default\ns_generate.llm_url = \"http://3rd/api/chat\";\ns_generate.llm_key = \"3rdkey\"; \ns_generate.model = \"gpt-4.1\";\ns_generate.temperature = 0.7;\ns_generate.top_p = 0.95;\ns_generate.max_tokens = 4096;\ns_generate.tool_choice = \"none\";\nconst auto generate_node = std::make_shared&lt;chat_node::ChatNode&lt;std::string, std::string&gt;&gt;(s_generate);\n</code></pre> <p>Currently, two LLM Modes are supported, allowing you to use a local Ollama model via:</p> <pre><code>chat_node::chat_node_settings s_generate;\ns_generate.llm_mode = chat_node::LLMMode::Ollama;\ns_generate.model = \"qwen3-vl:4b\"; // model to use\ns_generate.llm_url = \"http://127.0.0.1:11434/api/chat\";\ns_generate.llm_key = \"\";\ns_generate.temperature = 0.7;\ns_generate.top_p = 0.95;\ns_generate.max_tokens = 4096;\ns_generate.tool_choice = \"none\";\nconst auto generate_node = std::make_shared&lt;chat_node::ChatNode&lt;std::string, std::string&gt;&gt;(s_generate);\n</code></pre> <p>Otherwise , you can configure the following environment variables for LLM Model:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\n</code></pre>"},{"location":"1-1_BasicConceptsChatNode/#chat-node-input-and-output","title":"Chat Node Input and Output","text":"<p>The default data type for both input and output is <code>std::string</code> (for custom data types, please refer to <code>Advanced Usage</code>).</p>"},{"location":"1-1_BasicConceptsChatNode/#input-format","title":"Input Format","text":"<p>The input data structure strictly follows the OpenAI Chat Completion API specification, with the following format:</p> <pre><code>[\n  {\"role\":\"system\",\"content\":\"you are a helpful assistant\"},\n  {\"role\":\"user\",\"content\":\"who are you?\"}\n]\n</code></pre> <p>It also supports multimodal input (if the model supports it):</p> <pre><code>[\n    {\n        \"content\": \"You are a helpful assistant with multiple tools.\",\n        \"role\": \"system\"\n    },\n    {\n        \"content\": [\n            {\n                \"text\": \"what is this?\",\n                \"type\": \"text\"\n            },\n            {\n                \"image_url\": {\n                    \"detail\": \"low\",\n                    \"url\": \"https://1.bp.blogspot.com/529.jpg\"\n                },\n                \"type\": \"image_url\"\n            }\n        ],\n        \"role\": \"user\"\n    }\n]\n</code></pre> <p>Alternatively, images can be represented in base64 format:</p> <pre><code>[\n    {\n        \"content\": \"You are a helpful assistant with multiple tools.\",\n        \"role\": \"system\"\n    },\n    {\n        \"content\": [\n            {\n                \"text\": \"what is this?\",\n                \"type\": \"text\"\n            },\n            {\n                \"image_url\": {\n                    \"detail\": \"low\",\n                    \"url\": \"data:image/png;base64,dfadfnakjenqlkmdcklasjdflkadslfkadsokfqmldf\"\n                },\n                \"type\": \"image_url\"\n            }\n        ],\n        \"role\": \"user\"\n    }\n]\n</code></pre>"},{"location":"1-1_BasicConceptsChatNode/#output-format","title":"Output Format","text":"<p>The output data structure strictly follows the OpenAI Chat Completion API specification, with the following format:</p> <pre><code>[\n  {\"role\":\"system\",\"content\":\"you are a helpful assistant\"},\n  {\"role\":\"user\",\"content\":\"who are you?\"},\n  {\"role\":\"assistant\",\"content\":\"my name is bob.\"}\n]\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/","title":"Tool Node","text":"<p>Tool Node is a tool management node that supports the following features:</p> <ul> <li>Tool Registration and Management: Supports registration, execution, and destruction of MCP (Model Context Protocol) tools (SSE/STDIO) and local tools</li> <li>Function Call Integration: Can serve as the foundation for Chat Node's Function Call, or execute independently</li> <li>Custom Extensions: Supports user-defined local functions registered to the Tool Node</li> </ul>"},{"location":"1-2_BasicConceptsToolNode/#registering-tools","title":"Registering Tools","text":""},{"location":"1-2_BasicConceptsToolNode/#registering-local-tools","title":"Registering Local Tools","text":"<p>For local tool development specifications, please refer to <code>Local Tool Development Specifications</code>.</p> <pre><code>common_tools::tools::add_function_call(search_news_tool,\n    R\"({\"type\":\"function\",\"function\":{\"name\":\"search_news\",\"description\":\"Search for the latest news information using keywords\",\"parameters\":{\"type\":\"object\",\"properties\":{\"query\":{\"type\":\"string\",\"description\":\"Search keywords\",\"minLength\":1},\"time_range\":{\"type\":\"string\",\"enum\":[\"day\",\"week\"]},\"country\":{\"type\":\"string\",\"enum\":[\"china\",\"usa\",\"japan\"]}},\"required\":[\"query\"]}}})\"\n);\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/#registering-mcp-tools","title":"Registering MCP Tools","text":"<pre><code>const auto r1 = common_tools::tools::add_server(\n    R\"({\"WeatherServer\":{\"url\":\"http://18.119.131.41:8006\",\"sse_endpoint\":\"/sse\"}})\"\n);\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/#getting-tool-status-mcp-tools-only","title":"Getting Tool Status (MCP Tools Only)","text":"<p>The function to get the initialization status of MCP tools is as follows:</p> <pre><code>int status = common_tools::tools::get_server_init_status(std::string(name));\n</code></pre> <p>Parameter Description: - <code>name</code>: The unique identifier name of the tool (case-sensitive)</p> <p>Return Value: - <code>1</code>: Initialization successful - <code>2</code>: Initialization failed - <code>0</code>: Initialization in progress - <code>-1</code>: Unknown status</p>"},{"location":"1-2_BasicConceptsToolNode/#closing-tools-mcp-tools-only","title":"Closing Tools (MCP Tools Only)","text":"<p>The function to close all MCP tools is as follows:</p> <pre><code>common_tools::tools::shutdown_all_servers();\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/#executing-tools","title":"Executing Tools","text":""},{"location":"1-2_BasicConceptsToolNode/#automatic-invocation-via-chat-node","title":"Automatic Invocation via Chat Node","text":"<p>Tools can serve as Chat Node's Function Call functions, automatically judged and invoked by the model. By configuring as follows, Chat Node will have tool invocation capabilities:</p> <pre><code>chat_node::chat_node_settings s;\ns.model = \"gpt-4o-mini\";\ns.temperature = 0.7;\ns.max_tokens = 4096;\ns.tool_choice = \"auto\";  // Enable automatic tool selection\ns.tools_json = common_tools::tools::get_all_tools_json();  // Get all registered tools\nconst auto node = std::make_shared&lt;chat_node::ChatNode&lt;std::string, std::string&gt;&gt;(s);\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/#manually-invoking-a-specified-tool","title":"Manually Invoking a Specified Tool","text":"<p>Manually execute a specified tool in the following way:</p> <pre><code>std::string ws_out = common_tools::tools::call_tool(\"search_web2\", ws_in_json.dump());\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/#tool-input-format","title":"Tool Input Format","text":"<p>Tool input parameters strictly follow the OpenAI Chat Completion API's Function Call parameter format, passed as <code>std::string</code> type. The input is a JSON-formatted parameter string:</p> <pre><code>{\n  \"query\": \"Search keywords\",\n  \"time_range\": \"day\",\n  \"country\": \"china\"\n}\n</code></pre> <p>Example Code:</p> <pre><code>nlohmann::json params;\nparams[\"query\"] = \"Latest tech news\";\nparams[\"time_range\"] = \"day\";\nparams[\"country\"] = \"china\";\nstd::string arguments = params.dump();\n\nstd::string result = common_tools::tools::call_tool(\"search_news\", arguments);\n</code></pre> <p>For local tool development sample, please refer to link.</p>"},{"location":"1-2_BasicConceptsToolNode/#tool-input-format_1","title":"Tool Input Format","text":"<p>The tool input format should follow the function call response format defined by the OpenAI Chat Completions API, as shown below:</p> <pre><code> {\"arguments\":\"{\\\"query\\\":\\\"San Francisco weather December 1 2025\\\"}\",\"name\":\"search_web2\"}\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/#tool-output-format","title":"Tool Output Format","text":"<p>Tool output results strictly follow the OpenAI Chat Completion API's Function Call response format, returned as <code>std::string</code> type. The output is in JSON format:</p> <pre><code>{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Tool execution result content\"\n    }\n  ],\n  \"isError\": false\n}\n</code></pre> <p>Field Description: - <code>content</code>: Result content array, each element contains <code>type</code> and <code>text</code> fields - <code>isError</code>: Boolean value indicating whether an error occurred</p>"},{"location":"1-2_BasicConceptsToolNode/#optional-call-the-tool-in-custom-code","title":"Optional: Call the Tool in Custom Code**","text":"<p>You can invoke the custom tool\u2019s API directly, as shown below, ref link about Custom Code:</p> <pre><code>\n// \u6784\u5efa\u5de5\u5177\u53c2\u6570 JSON\nnlohmann::json arguments = {\n  {\"name\", \"search_web2\"},\n  {\"arguments\", nlohmann::json{{\"query\", in}}.dump()}\n};\n\nstd::string result = common_tools::tools::call_tool(\"search_news\", arguments.dump());\nnlohmann::json result_json = nlohmann::json::parse(result);\n\nif (!result_json[\"isError\"].get&lt;bool&gt;()) {\n    std::string text = result_json[\"content\"][0][\"text\"].get&lt;std::string&gt;();\n    // Process result\n}\n</code></pre> <p>For local tool development specifications, please refer to <code>Local Tool Development Specifications</code>.</p>"},{"location":"1-3_BasicConceptsDIYNode/","title":"Custom Node","text":""},{"location":"1-3_BasicConceptsDIYNode/#building-a-custom-node","title":"Building a Custom Node","text":"<p>You can build custom processing nodes using <code>OneFuncNode</code> to implement arbitrary business logic. Nodes define processing functions through Lambda expressions and support custom input and output types.</p>"},{"location":"1-3_BasicConceptsDIYNode/#basic-usage","title":"Basic Usage","text":"<pre><code>auto custom_node = std::make_shared&lt;nodeflow::OneFuncNode&lt;std::string, std::string&gt;&gt;(\n    [&amp;](const std::string&amp; input) -&gt; std::string {\n        // Custom processing logic\n        std::string processed = process_input(input);\n        return processed;\n    }\n);\n</code></pre>"},{"location":"1-3_BasicConceptsDIYNode/#practical-example","title":"Practical Example","text":"<p>The following example shows how to build a text polishing prompt generation node:</p> <pre><code>auto polish_prompt_node = std::make_shared&lt;nodeflow::OneFuncNode&lt;std::string, std::string&gt;&gt;(\n    [&amp;](const std::string&amp; draft) -&gt; std::string {\n        std::string polish_prompt = \n            \"Please rewrite the following draft to make it more friendly and engaging:\\n\\n\" +\n            draft + \"\\n\\n\"\n            \"Rewriting requirements:\\n\"\n            \"- The tone should be natural, like chatting, warm and infectious\\n\"\n            \"- You can add some rhetorical questions to guide readers to think\\n\"\n            \"- Appropriately add metaphors or analogies to make the content more vivid\\n\"\n            \"- The opening should catch the eye, and the ending should be powerful and memorable\\n\"\n            \"Final language: English.\\n\";\n\n        return polish_prompt;\n    }\n);\n</code></pre> <p>Notes: - <code>OneFuncNode&lt;IN, OUT&gt;</code>: Template parameters specify input and output types respectively - Lambda expression: Receives input parameters and returns processed output - Type safety: Compile-time checking of input and output type matching</p>"},{"location":"1-4_BasicConceptsCENode/","title":"CE Node","text":"<p>CE Node (Context Engine Node) is a context engine node component for managing and compressing conversation history. It supports two context management strategies: Trimming and Summarizing, which effectively control conversation history length and avoid exceeding model context window limits.</p>"},{"location":"1-4_BasicConceptsCENode/#building-a-ce-node","title":"Building a CE Node","text":"<p>Build a CE node as follows, allowing configuration of context management strategy and related parameters:</p> <pre><code>ce_node::ce_node_settings s_ce;\ns_ce.strategy = ContextStrategy::SUMMARIZING;\ns_ce.context_limit = 3; // Compression trigger threshold, number of history turns\ns_ce.keep_last_n_turns = 1; // Number of recent original message turns to keep\ns_ce.tool_trim_limit = 600; // Tool results are not very important in history messages, so the first 600 characters of tool results will be kept\ns_ce.summarizer_model = \"gpt-4o-mini\";\ns_ce.summarizer_max_tokens = 400;\nconst auto ce_node = std::make_shared&lt;ce_node::CeNode&lt;std::string, std::string&gt;&gt;(s_ce);\n</code></pre>"},{"location":"1-4_BasicConceptsCENode/#context-management-strategies","title":"Context Management Strategies","text":"<p>CE Node supports two context management strategies:</p>"},{"location":"1-4_BasicConceptsCENode/#trimming-strategy","title":"TRIMMING Strategy","text":"<p>The trimming strategy directly deletes history messages that exceed the limit, keeping the most recent <code>max_turns</code> conversation turns.</p> <pre><code>ce_node::ce_node_settings s_ce;\ns_ce.strategy = ContextStrategy::TRIMMING;\ns_ce.max_turns = 3; // Maximum number of history turns to keep\nconst auto ce_node = std::make_shared&lt;ce_node::CeNode&lt;std::string, std::string&gt;&gt;(s_ce);\n</code></pre>"},{"location":"1-4_BasicConceptsCENode/#summarizing-strategy","title":"SUMMARIZING Strategy","text":"<p>The summarizing strategy uses LLM to compress old history messages into summaries, keeping the most recent <code>keep_last_n_turns</code> original message turns. When the number of history turns exceeds <code>context_limit</code>, summarization compression is triggered.</p> <pre><code>ce_node::ce_node_settings s_ce;\ns_ce.strategy = ContextStrategy::SUMMARIZING;\ns_ce.context_limit = 3; // Compression trigger threshold, number of history turns\ns_ce.keep_last_n_turns = 1; // Number of recent original message turns to keep\ns_ce.tool_trim_limit = 600; // Number of characters to keep for tool results\ns_ce.summarizer_model = \"gpt-4o-mini\";\ns_ce.summarizer_max_tokens = 400;\nconst auto ce_node = std::make_shared&lt;ce_node::CeNode&lt;std::string, std::string&gt;&gt;(s_ce);\n</code></pre>"},{"location":"1-4_BasicConceptsCENode/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"1-4_BasicConceptsCENode/#common-parameters","title":"Common Parameters","text":"<ul> <li><code>strategy</code>: Context management strategy, options are <code>ContextStrategy::TRIMMING</code> or <code>ContextStrategy::SUMMARIZING</code>, default is <code>SUMMARIZING</code></li> </ul>"},{"location":"1-4_BasicConceptsCENode/#trimming-strategy-parameters","title":"TRIMMING Strategy Parameters","text":"<ul> <li><code>max_turns</code>: Maximum number of history turns to keep, default is 8</li> </ul>"},{"location":"1-4_BasicConceptsCENode/#summarizing-strategy-parameters","title":"SUMMARIZING Strategy Parameters","text":"<ul> <li><code>context_limit</code>: Compression trigger threshold, triggers summarization compression when history turns exceed this value, default is 5</li> <li><code>keep_last_n_turns</code>: Number of recent original message turns to keep, these messages won't be compressed, default is 2</li> <li><code>tool_trim_limit</code>: Number of characters to keep for tool results in history messages, default is 600</li> <li><code>summarizer_model</code>: Model name for summarization, default is \"gpt-4o-mini\"</li> <li><code>summarizer_max_tokens</code>: Maximum tokens for summarizer model, default is 400</li> <li><code>summarizer_tools_json</code>: Tool configuration for summarizer (JSON string), default is \"[]\"</li> <li><code>summarizer_tool_choice</code>: Tool selection strategy for summarizer, default is \"none\"</li> </ul>"},{"location":"1-4_BasicConceptsCENode/#ce-node-input-and-output","title":"CE Node Input and Output","text":"<p>The default data type is <code>std::string</code> (for custom data types, please refer to <code>Advanced Usage</code>).</p>"},{"location":"1-4_BasicConceptsCENode/#input-format","title":"Input Format","text":"<p>Input data format strictly follows the OpenAI Chat Completion API specification, format as follows:</p> <pre><code>[\n  {\"role\":\"system\",\"content\":\"you are a helpful assistant\"},\n  {\"role\":\"user\",\"content\":\"who are you?\"},\n  {\"role\":\"assistant\",\"content\":\"my name is bob.\"},\n  {\"role\":\"user\",\"content\":\"what can you do?\"}\n]\n</code></pre>"},{"location":"1-4_BasicConceptsCENode/#output-format","title":"Output Format","text":"<p>Output data format also follows the OpenAI Chat Completion API specification. Depending on the configured strategy, output may be:</p> <ul> <li>TRIMMING Strategy: Keeps the most recent <code>max_turns</code> original message turns</li> <li>SUMMARIZING Strategy: Compresses old messages beyond <code>keep_last_n_turns</code> into summaries, keeping the most recent <code>keep_last_n_turns</code> original message turns</li> </ul> <p>Output format example:</p> <pre><code>[\n  {\"role\":\"system\",\"content\":\"you are a helpful assistant\"},\n  {\"role\":\"assistant\",\"content\":\"[Summary of previous conversation]\"},\n  {\"role\":\"user\",\"content\":\"what can you do?\"}\n]\n</code></pre>"},{"location":"1-5_BasicConceptsChainFlow/","title":"Chain","text":"<p>The <code>chain</code> function is used to connect two nodes, creating a data flow path between nodes. An optional <code>action</code> parameter can be specified to create conditional connections.</p> <p>Function Signature:</p> <pre><code>template &lt;typename NodeA, typename NodeB&gt;\nvoid chain(\n    const std::shared_ptr&lt;NodeA&gt;&amp; a,      // Source node\n    const std::shared_ptr&lt;NodeB&gt;&amp; b,      // Target node\n    std::optional&lt;std::string&gt; action = std::nullopt  // Optional action identifier\n);\n</code></pre> <p>How It Works:</p> <ul> <li>When the <code>action</code> parameter is <code>std::nullopt</code>, it creates a default connection (unconditional connection)</li> <li>When the <code>action</code> parameter has a value, it creates a conditional connection, which only executes when the action value returned by the source node's <code>route</code> function matches the action specified in <code>chain</code></li> </ul> <p>Usage Example:</p> <pre><code>// Set routing: determine the returned action based on node output\nroute(decide_node, [](const std::string&amp;, const std::string&amp;) -&gt; std::optional&lt;std::string&gt; {\n    // Return different actions based on business logic\n    return \"search\";\n});\n\n// Create conditional connection: execute this connection when route returns \"search\"\nchain(decide_node, search_node, \"search\");\n\n// Create conditional connection: execute this connection when route returns \"answer\"\nchain(decide_node, answer_node, \"answer\");\n\n// Create default connection: execute unconditionally (used when route returns std::nullopt)\nchain(decide_node, default_node);\n</code></pre> <p>Notes: - The route function is called after the node execution completes - The returned action value must match the action specified in <code>chain</code> - When returning <code>std::nullopt</code>, use the default connection (chain without action parameter)</p>"},{"location":"1-5_BasicConceptsChainFlow/#flow","title":"Flow","text":"<p>Flow is the execution container for workflows, used in conjunction with Chain. It is responsible for managing node execution order and data flow. Through Flow, multiple nodes can be organized into a complete execution flow.</p>"},{"location":"1-5_BasicConceptsChainFlow/#basic-concepts-of-flow","title":"Basic Concepts of Flow","text":"<ul> <li>Workflow Container: Flow is the execution container for nodes, managing node lifecycle and execution order</li> <li>Start Node: Each Flow must specify a start node as the workflow entry point</li> <li>Automatic Execution: Flow automatically executes the workflow based on connections between nodes</li> <li>Type Safety: Flow supports typed input and output, ensuring type safety</li> </ul>"},{"location":"1-5_BasicConceptsChainFlow/#creating-and-executing-workflows","title":"Creating and Executing Workflows","text":""},{"location":"1-5_BasicConceptsChainFlow/#1-create-flow-object","title":"1. Create Flow Object","text":"<p>Use <code>std::make_shared</code> to create a Flow object:</p> <pre><code>auto f = std::make_shared&lt;nodeflow::Flow&gt;();\n</code></pre>"},{"location":"1-5_BasicConceptsChainFlow/#2-set-start-node","title":"2. Set Start Node","text":"<p>Specify the workflow's start node through the <code>start</code> method:</p> <pre><code>f-&gt;start(decide_node);  // decide_node as the workflow entry point\n</code></pre>"},{"location":"1-5_BasicConceptsChainFlow/#3-execute-workflow","title":"3. Execute Workflow","text":"<p>Use the <code>runWithInput</code> method to execute the workflow:</p> <pre><code>auto result = f-&gt;runWithInput&lt;std::string, std::string&gt;(input);\n</code></pre> <p>Function Signature:</p> <pre><code>template &lt;typename IN, typename OUT&gt;\nOUT runWithInput(const IN&amp; input);\n</code></pre> <p>Parameter Description: - Template parameter <code>IN</code>: Input data type (i.e., the first node's input) - Template parameter <code>OUT</code>: Output data type - <code>input</code>: Actual input data</p> <p>Return Value: - The workflow's final output result, type is <code>OUT</code></p>"},{"location":"1-6_BasicConceptsRoute/","title":"Route","text":"<p>The routing mechanism is used to dynamically determine the execution path of a workflow based on node input and output. Through the <code>route</code> function combined with the <code>chain</code> function, conditional branching and loop control can be implemented.</p>"},{"location":"1-6_BasicConceptsRoute/#route-function","title":"Route Function","text":"<p>Each node supports setting routing logic through the <code>route</code> function. The route function is implemented based on Lambda expressions, receiving the node's input and output as parameters, and returning the action identifier for the next step.</p>"},{"location":"1-6_BasicConceptsRoute/#function-signature","title":"Function Signature","text":"<pre><code>template &lt;typename Node, typename Selector&gt;\nvoid route(\n    const std::shared_ptr&lt;Node&gt;&amp; node, \n    Selector selector\n);\n</code></pre> <p>Parameter Description: - <code>node</code>: The node to set routing for - <code>selector</code>: Route selector function, type is <code>std::function&lt;std::optional&lt;std::string&gt;(const IN&amp;, const OUT&amp;)&gt;</code></p> <p>Return Value: - <code>std::optional&lt;std::string&gt;</code>: Returns an action string representing the next execution path, returns <code>std::nullopt</code> to use the default connection</p>"},{"location":"1-6_BasicConceptsRoute/#example-1-routing-based-on-decision-results","title":"Example 1: Routing Based on Decision Results","text":"<pre><code>route(decide_node, [&amp;](const std::string&amp; input, const std::string&amp; output) -&gt; std::optional&lt;std::string&gt; {\n    // Parse decision information from output\n    if (g_yaml_node[\"action\"].as&lt;std::string&gt;() == \"search\") {\n        return \"search\";  // Route to search node\n    }\n    if (g_yaml_node[\"action\"].as&lt;std::string&gt;() == \"answer\") {\n        return \"answer\";  // Route to answer node\n    }\n    return std::nullopt;  // Use default route\n});\n</code></pre>"},{"location":"1-6_BasicConceptsRoute/#example-2-routing-based-on-validation-results","title":"Example 2: Routing Based on Validation Results","text":"<pre><code>route(supervisor_node, [&amp;](const std::string&amp; input, const std::string&amp; output) -&gt; std::optional&lt;std::string&gt; {\n    if (g_validation_result.valid) {\n        return \"done\";   // Validation passed, route to completion node\n    } else {\n        return \"retry\"; // Validation failed, route to retry node\n    }\n});\n</code></pre>"},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/","title":"Single Node Agent Development Example","text":""},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/#overview","title":"Overview","text":"<p>In this example, we will construct only one Chat Node from HADK to materialize an Agent that can understand or invoke functions automatically according to user's query. ref. link</p>"},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p><code>LLM_API_URL</code> and  <code>LLM_API_KEY</code> can be configured using <code>chat_node::chat_node_settings</code>, For more details, please refer to BasicConceptsChatNode </p>"},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/#api-functions","title":"API Functions","text":""},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/#1-init_loc_tools","title":"1. init_loc_tools()","text":"<p>Initialize and register local tools.</p> <p>Function Signature:</p> <pre><code>bool init_loc_tools();\n</code></pre> <p>Description: - Registers local tools (e.g., <code>search_web2</code> for web search) - Returns <code>true</code> on success, <code>false</code> on failure</p> <p>Example:</p> <pre><code>#include \"src/single_node/koba_agent.h\"\n\nbool reg_loc_tools = init_loc_tools();\n</code></pre> <p>Note: The function internally registers tools using the OpenAI Function Call format. The simplest function call format example:</p> <pre><code>{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"search_web2\",\n    \"description\": \"Search the web\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"query\": {\n          \"type\": \"string\",\n          \"description\": \"Search query\"\n        }\n      },\n      \"required\": [\"query\"]\n    }\n  }\n}\n</code></pre>"},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/#2-init_mcp_tools","title":"2. init_mcp_tools()","text":"<p>Initialize and register MCP (Model Context Protocol) tools by connecting to remote tool servers.</p> <p>Function Signature:</p> <pre><code>bool init_mcp_tools(const char* params);\n</code></pre> <p>Parameters: - <code>params</code>: JSON string containing server configuration. Format: <code>{\"ServerName\":{\"url\":\"http://host:port/sse\"}}</code></p> <p>Description: - Registers remote MCP tool servers - Waits 10 seconds for server initialization - Returns <code>true</code> on success, <code>false</code> on failure</p> <p>Example:</p> <pre><code>#include \"src/single_node/koba_agent.h\"\n\nbool reg_mcp_tools = init_mcp_tools(R\"({\"WeatherServer\":{\"url\":\"http://18.119.131.41:8006/sse\"}})\");\n</code></pre>"},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/#3-get_tools_init_status","title":"3. get_tools_init_status()","text":"<p>Get the initialization status of a specific tool server.</p> <p>Function Signature:</p> <pre><code>int get_tools_init_status(const char* name);\n</code></pre> <p>Parameters: - <code>name</code>: Name of the tool server</p> <p>Return Value: - Returns initialization status code (0 for success, -1 for error or not found)</p> <p>Example:</p> <pre><code>#include \"src/single_node/koba_agent.h\"\n\nint status = get_tools_init_status(\"WeatherServer\");\n</code></pre>"},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/#4-koba_agent","title":"4. koba_agent()","text":"<p>Core agent function that processes user queries and returns responses.</p> <p>Function Signature:</p> <pre><code>const char* koba_agent(const char* message);\n</code></pre> <p>Parameters: - <code>message</code>: JSON string in OpenAI Chat Completion format (array of message objects)</p> <p>Return Value: - Returns a JSON string containing the conversation response</p> <p>Message Format: The input message should be a JSON array following OpenAI Chat Completion format:</p> <pre><code>[\n  {\n    \"role\": \"system\",\n    \"content\": \"You are a helpful assistant.\"\n  },\n  {\n    \"role\": \"user\",\n    \"content\": \"What is the weather like in San Francisco today?\"\n  }\n]\n</code></pre> <p>Example:</p> <pre><code>#include \"src/single_node/koba_agent.h\"\n#include &lt;nlohmann/json.hpp&gt;\n\nnlohmann::json inputJson = nlohmann::json::array({\n    {{\"role\", \"system\"}, {\"content\", \"You are a helpful assistant.\"}},\n    {{\"role\", \"user\"}, {\"content\", \"What is the weather like in San Francisco today?\"}}\n});\n\nconst char* response = koba_agent(inputJson.dump().c_str());\n</code></pre> <p>Node Configuration: The agent uses a single Chat Node with the following default settings: - Model: <code>gpt-4o-mini</code> - Temperature: <code>0.7</code> - Top-p: <code>0.95</code> - Max tokens: <code>4096</code> - Tool choice: <code>auto</code> (automatically decides when to use tools)</p>"},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/#5-close_tools","title":"5. close_tools()","text":"<p>Close all registered tool servers and clean up resources.</p> <p>Function Signature:</p> <pre><code>void close_tools();\n</code></pre> <p>Description: - Shuts down all MCP tool servers - Should be called before program exit</p> <p>Example:</p> <pre><code>#include \"src/single_node/koba_agent.h\"\n\nclose_tools();\n</code></pre>"},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/#usage-example","title":"Usage Example","text":"<pre><code>#include \"src/single_node/koba_agent.h\"\n#include &lt;nlohmann/json.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // Initialize tools\n    bool reg_loc_tools = init_loc_tools();\n    bool reg_mcp_tools = init_mcp_tools(R\"({\"WeatherServer\":{\"url\":\"http://18.119.131.41:8006/sse\"}})\");\n\n    // Build input message\n    nlohmann::json inputJson = nlohmann::json::array({\n        {{\"role\", \"user\"}, {\"content\", \"What is the weather like in San Francisco today?\"}}\n    });\n\n    // Call agent\n    const char* response = koba_agent(inputJson.dump().c_str());\n\n    // Parse and display response\n    nlohmann::json responseJson = nlohmann::json::parse(response);\n    std::cout &lt;&lt; responseJson.back()[\"content\"].get&lt;std::string&gt;() &lt;&lt; std::endl;\n\n    // Clean up\n    close_tools();\n\n    return 0;\n}\n</code></pre>"},{"location":"2-2_ApplicationDevelopmentNormalAgent/","title":"Normal Agent Development Example","text":""},{"location":"2-2_ApplicationDevelopmentNormalAgent/#overview","title":"Overview","text":"<p>The normal agent demonstrates how to use HADK's sequential workflow with context compression to implement a three-stage question-answering system. This example implements an agent that compresses conversation context, generates responses using LLM with tool support, and then polishes the final answer.ref. link</p> <p>Workflow Diagram:</p> <pre><code>Input \u2192 ce_node (Context Compression) \u2192 generate_node (Generate Response) \u2192 polish_node (Polish Response) \u2192 Output\n</code></pre> <p>Core Features: - Context compression: Automatically compresses conversation history when it exceeds the limit using SUMMARIZING strategy - Tool support: Generate node can use tools (e.g., search_web2) to gather information - Response polishing: Final polish node refines the generated response for better quality - Sequential workflow: Linear flow through three specialized nodes</p>"},{"location":"2-2_ApplicationDevelopmentNormalAgent/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p><code>LLM_API_URL</code> and  <code>LLM_API_KEY</code> can be configured using <code>chat_node::chat_node_settings</code>, For more details, please refer to BasicConceptsChatNode </p>"},{"location":"2-2_ApplicationDevelopmentNormalAgent/#api-functions","title":"API Functions","text":""},{"location":"2-2_ApplicationDevelopmentNormalAgent/#1-init_loc_tools","title":"1. init_loc_tools()","text":"<p>Initialize and register local tools.</p> <p>Function Signature:</p> <pre><code>bool init_loc_tools();\n</code></pre> <p>Description: - Registers local tools (e.g., <code>search_web2</code> for web search) - Returns <code>true</code> on success, <code>false</code> on failure</p> <p>Example:</p> <pre><code>#include \"src/normal_agent/koba_agent.h\"\n\nbool reg_loc_tools = init_loc_tools();\n</code></pre> <p>Note: The function internally registers tools using the OpenAI Function Call format. The simplest function call format example:</p> <pre><code>{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"search_web2\",\n    \"description\": \"Search the web\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"query\": {\n          \"type\": \"string\",\n          \"description\": \"Search query\"\n        }\n      },\n      \"required\": [\"query\"]\n    }\n  }\n}\n</code></pre>"},{"location":"2-2_ApplicationDevelopmentNormalAgent/#2-init_mcp_tools","title":"2. init_mcp_tools()","text":"<p>Initialize and register MCP (Model Context Protocol) tools by connecting to remote tool servers.</p> <p>Function Signature:</p> <pre><code>bool init_mcp_tools(const char* params);\n</code></pre> <p>Parameters: - <code>params</code>: JSON string containing server configuration. Format: <code>{\"ServerName\":{\"url\":\"http://host:port/sse\"}}</code></p> <p>Description: - Registers remote MCP tool servers - Waits 10 seconds for server initialization - Returns <code>true</code> on success, <code>false</code> on failure</p> <p>Example:</p> <pre><code>#include \"src/normal_agent/koba_agent.h\"\n\nbool reg_mcp_tools = init_mcp_tools(R\"({\"WeatherServer\":{\"url\":\"http://18.119.131.41:8006/sse\"}})\");\n</code></pre>"},{"location":"2-2_ApplicationDevelopmentNormalAgent/#3-get_tools_init_status","title":"3. get_tools_init_status()","text":"<p>Get the initialization status of a specific tool server.</p> <p>Function Signature:</p> <pre><code>int get_tools_init_status(const char* name);\n</code></pre> <p>Parameters: - <code>name</code>: Name of the tool server</p> <p>Return Value: - Returns initialization status code (0 for success, -1 for error or not found)</p> <p>Example:</p> <pre><code>#include \"src/normal_agent/koba_agent.h\"\n\nint status = get_tools_init_status(\"WeatherServer\");\n</code></pre>"},{"location":"2-2_ApplicationDevelopmentNormalAgent/#4-koba_agent","title":"4. koba_agent()","text":"<p>Core agent function that processes user queries through a three-node workflow and returns responses.</p> <p>Function Signature:</p> <pre><code>const char* koba_agent(const char* message);\n</code></pre> <p>Parameters: - <code>message</code>: JSON string in OpenAI Chat Completion format (array of message objects)</p> <p>Return Value: - Returns a JSON string containing the conversation response</p> <p>Message Format: The input message should be a JSON array following OpenAI Chat Completion format:</p> <pre><code>[\n  {\n    \"role\": \"system\",\n    \"content\": \"You are a helpful assistant.\"\n  },\n  {\n    \"role\": \"user\",\n    \"content\": \"What is the weather like in San Francisco today?\"\n  }\n]\n</code></pre> <p>Example:</p> <pre><code>#include \"src/normal_agent/koba_agent.h\"\n#include &lt;nlohmann/json.hpp&gt;\n\nnlohmann::json inputJson = nlohmann::json::array({\n    {{\"role\", \"system\"}, {\"content\", \"You are a helpful assistant.\"}},\n    {{\"role\", \"user\"}, {\"content\", \"What is the weather like in San Francisco today?\"}}\n});\n\nconst char* response = koba_agent(inputJson.dump().c_str());\n</code></pre> <p>Workflow Details:</p> <p>The agent implements a three-node sequential workflow:</p> <ol> <li>CE Node (Context Compression): Compresses conversation history when it exceeds the limit</li> <li>Strategy: <code>SUMMARIZING</code> (alternative: <code>TRIMMING</code>)</li> <li>Context limit: 3 turns</li> <li>Keeps last 1 turn in original form</li> <li>Tool trim limit: 600 characters</li> <li>Summarizer model: <code>gpt-4o-mini</code></li> <li> <p>Summarizer max tokens: 400</p> </li> <li> <p>Generate Node: Generates responses using LLM with tool support</p> </li> <li>Model: <code>gpt-4o-mini</code></li> <li>Temperature: <code>0.7</code></li> <li>Top-p: <code>0.95</code></li> <li>Max tokens: <code>4096</code></li> <li> <p>Tool choice: <code>auto</code> (automatically decides when to use tools)</p> </li> <li> <p>Polish Node: Refines the generated response</p> </li> <li>Model: <code>gpt-4o-mini</code></li> <li>Temperature: <code>0.7</code></li> <li>Top-p: <code>0.95</code></li> <li>Max tokens: <code>4096</code></li> <li>Tool choice: <code>none</code> (no tool support for polishing)</li> <li>Uses a preprocessor to modify the input prompt for polishing</li> </ol> <p>Node Connection: - Nodes are connected sequentially using the <code>chain</code> function - Routing values are set using the <code>route</code> function - Flow: <code>ce_node</code> \u2192 <code>generate_node</code> \u2192 <code>polish_node</code></p>"},{"location":"2-2_ApplicationDevelopmentNormalAgent/#5-close_tools","title":"5. close_tools()","text":"<p>Close all registered tool servers and clean up resources.</p> <p>Function Signature:</p> <pre><code>void close_tools();\n</code></pre> <p>Description: - Shuts down all MCP tool servers - Should be called before program exit</p> <p>Example:</p> <pre><code>#include \"src/normal_agent/koba_agent.h\"\n\nclose_tools();\n</code></pre>"},{"location":"2-2_ApplicationDevelopmentNormalAgent/#usage-example","title":"Usage Example","text":"<pre><code>#include \"src/normal_agent/koba_agent.h\"\n#include &lt;nlohmann/json.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // Initialize tools\n    bool reg_loc_tools = init_loc_tools();\n    bool reg_mcp_tools = init_mcp_tools(R\"({\"WeatherServer\":{\"url\":\"http://18.119.131.41:8006/sse\"}})\");\n\n    // Build input message\n    nlohmann::json inputJson = nlohmann::json::array({\n        {{\"role\", \"user\"}, {\"content\", \"What is the capital of France?\"}}\n    });\n\n    // Call agent\n    const char* response = koba_agent(inputJson.dump().c_str());\n\n    // Parse and display response\n    nlohmann::json responseJson = nlohmann::json::parse(response);\n    std::cout &lt;&lt; responseJson.back()[\"content\"].get&lt;std::string&gt;() &lt;&lt; std::endl;\n\n    // Clean up\n    close_tools();\n\n    return 0;\n}\n</code></pre>"},{"location":"2-2_ApplicationDevelopmentNormalAgent/#key-points","title":"Key Points","text":"<ol> <li>Context Compression: The CE node automatically compresses conversation history when it exceeds the limit, using either SUMMARIZING or TRIMMING strategy</li> <li>Sequential Workflow: Nodes are connected in a linear sequence using the <code>chain</code> function</li> <li>Routing Values: Use the <code>route</code> function to set routing values for node outputs, which are used by <code>chain</code> to determine connections</li> <li>Tool Support: The generate node supports tool calling, allowing it to use tools like <code>search_web2</code> to gather information</li> <li>Response Polishing: The polish node refines the generated response without tool support, ensuring high-quality final output</li> <li>Preprocessor Usage: The polish node uses a preprocessor to modify the input prompt, instructing the LLM to polish the content</li> </ol>"},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/","title":"Conditional Routing Agent Development Example","text":""},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/#overview","title":"Overview","text":"<p>The conditional routing agent demonstrates how to use HADK's conditional routing (<code>route</code>) functionality to implement complex workflows. This example implements a research assistant agent that can dynamically decide whether to continue searching for information or directly answer questions based on the current context, supporting iterative searches until sufficient information is obtained. ref. link</p> <p>Workflow Diagram:</p> <pre><code>Input \u2192 decide_node (Decision) \u2192 [Conditional Routing]\n                                  \u251c\u2500 \"search\" \u2192 web_search_node (Web Search) \u2192 decide_node (Loop)\n                                  \u2514\u2500 \"answer\" \u2192 answer_node (Generate Answer) \u2192 Output\n</code></pre> <p>Core Features: - Conditional routing: Dynamically select the next node based on the decision node's output - Loop workflow: Supports looping between decision node and search node until sufficient information is obtained - Context accumulation: Each search result accumulates into the context for subsequent decisions</p>"},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p><code>LLM_API_URL</code> and  <code>LLM_API_KEY</code> can be configured using <code>chat_node::chat_node_settings</code>, For more details, please refer to BasicConceptsChatNode </p>"},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/#api-functions","title":"API Functions","text":""},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/#1-init_loc_tools","title":"1. init_loc_tools()","text":"<p>Initialize and register local tools.</p> <p>Function Signature:</p> <pre><code>bool init_loc_tools();\n</code></pre> <p>Description: - Registers local tools (e.g., <code>search_web2</code> for web search) - Returns <code>true</code> on success, <code>false</code> on failure</p> <p>Example:</p> <pre><code>#include \"src/reflector_agent/koba_agent.h\"\n\nbool reg_loc_tools = init_loc_tools();\n</code></pre> <p>Note: The function internally registers tools using the OpenAI Function Call format. The simplest function call format example:</p> <pre><code>{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"search_web2\",\n    \"description\": \"Search the web\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"query\": {\n          \"type\": \"string\",\n          \"description\": \"Search query\"\n        }\n      },\n      \"required\": [\"query\"]\n    }\n  }\n}\n</code></pre>"},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/#2-init_mcp_tools","title":"2. init_mcp_tools()","text":"<p>Initialize and register MCP (Model Context Protocol) tools by connecting to remote tool servers.</p> <p>Function Signature:</p> <pre><code>bool init_mcp_tools(const char* params);\n</code></pre> <p>Parameters: - <code>params</code>: JSON string containing server configuration. Format: <code>{\"ServerName\":{\"url\":\"http://host:port/sse\"}}</code></p> <p>Description: - Registers remote MCP tool servers - Waits 10 seconds for server initialization - Returns <code>true</code> on success, <code>false</code> on failure</p> <p>Example:</p> <pre><code>#include \"src/reflector_agent/koba_agent.h\"\n\nbool reg_mcp_tools = init_mcp_tools(R\"({\"WeatherServer\":{\"url\":\"http://18.119.131.41:8006/sse\"}})\");\n</code></pre>"},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/#3-get_tools_init_status","title":"3. get_tools_init_status()","text":"<p>Get the initialization status of a specific tool server.</p> <p>Function Signature:</p> <pre><code>int get_tools_init_status(const char* name);\n</code></pre> <p>Parameters: - <code>name</code>: Name of the tool server</p> <p>Return Value: - Returns initialization status code (0 for success, -1 for error or not found)</p> <p>Example:</p> <pre><code>#include \"src/reflector_agent/koba_agent.h\"\n\nint status = get_tools_init_status(\"WeatherServer\");\n</code></pre>"},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/#4-koba_agent","title":"4. koba_agent()","text":"<p>Core agent function that processes user queries through an iterative decision-making and search workflow.</p> <p>Function Signature:</p> <pre><code>const char* koba_agent(const char* message);\n</code></pre> <p>Parameters: - <code>message</code>: JSON string in OpenAI Chat Completion format (array of message objects)</p> <p>Return Value: - Returns a JSON string containing the conversation response</p> <p>Message Format: The input message should be a JSON array following OpenAI Chat Completion format:</p> <pre><code>[\n  {\n    \"role\": \"user\",\n    \"content\": \"What is the capital of France?\"\n  }\n]\n</code></pre> <p>Example:</p> <pre><code>#include \"src/reflector_agent/koba_agent.h\"\n#include &lt;nlohmann/json.hpp&gt;\n\nnlohmann::json inputJson = nlohmann::json::array({\n    {{\"role\", \"user\"}, {\"content\", \"What is the capital of France?\"}}\n});\n\nconst char* response = koba_agent(inputJson.dump().c_str());\n</code></pre> <p>Workflow Details:</p> <p>The agent implements a three-node workflow with conditional routing and loops:</p> <ol> <li>Decide Node (Decision Node): Analyzes context and decides next action</li> <li>Model: <code>gpt-4o-mini</code></li> <li>Temperature: <code>0.7</code></li> <li>Top-p: <code>0.95</code></li> <li>Max tokens: <code>2048</code></li> <li>Tool choice: <code>none</code></li> <li>Output format: YAML with fields <code>thinking</code>, <code>action</code>, <code>reason</code>, <code>search_query</code>/<code>answer</code></li> <li>Preprocessor: Builds decision prompt with question, previous research context, and current time</li> <li> <p>Postprocessor: Parses YAML response and routes to search or answer based on action</p> </li> <li> <p>Web Search Node (Tool Node): Executes web search using <code>search_web2</code> tool</p> </li> <li>Uses <code>ToolNode</code> for structured tool invocation</li> <li>Preprocessor: Converts search query to tool input format</li> <li>Postprocessor: Extracts search results (title, URL, snippet) and accumulates context</li> <li> <p>Automatically routes back to decide_node after search completion</p> </li> <li> <p>Answer Node: Generates final answer based on accumulated research</p> </li> <li>Model: <code>gpt-4o-mini</code></li> <li>Temperature: <code>0.7</code></li> <li>Top-p: <code>0.95</code></li> <li>Max tokens: <code>4096</code></li> <li>Tool choice: <code>none</code></li> <li>Preprocessor: Converts input to Chat Completion format</li> </ol> <p>Node Connection: - Conditional routing: <code>decide_node</code> routes to <code>web_search_node</code> (route: \"search\") or <code>answer_node</code> (route: \"answer\") - Loop: <code>web_search_node</code> routes back to <code>decide_node</code> (route: \"decide\") - Flow: <code>decide_node</code> \u2192 (conditional) \u2192 <code>web_search_node</code> \u2192 <code>decide_node</code> (loop) OR <code>answer_node</code> \u2192 output</p> <p>Key Implementation Details: - Uses YAML parsing to extract decision information from decide node output - Context accumulation: Each search result is appended to global context - State management: Uses global variables to track question, context, and YAML node</p>"},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/#5-close_tools","title":"5. close_tools()","text":"<p>Close all registered tool servers and clean up resources.</p> <p>Function Signature:</p> <pre><code>void close_tools();\n</code></pre> <p>Description: - Shuts down all MCP tool servers - Should be called before program exit</p> <p>Example:</p> <pre><code>#include \"src/reflector_agent/koba_agent.h\"\n\nclose_tools();\n</code></pre>"},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/#usage-example","title":"Usage Example","text":"<pre><code>#include \"src/reflector_agent/koba_agent.h\"\n#include &lt;nlohmann/json.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // Initialize tools\n    bool reg_loc_tools = init_loc_tools();\n    bool reg_mcp_tools = init_mcp_tools(R\"({\"WeatherServer\":{\"url\":\"http://18.119.131.41:8006/sse\"}})\");\n\n    // Build input message\n    nlohmann::json inputJson = nlohmann::json::array({\n        {{\"role\", \"user\"}, {\"content\", \"What is the capital of France?\"}}\n    });\n\n    // Call agent\n    const char* response = koba_agent(inputJson.dump().c_str());\n\n    // Parse and display response\n    nlohmann::json responseJson = nlohmann::json::parse(response);\n    std::cout &lt;&lt; responseJson.back()[\"content\"].get&lt;std::string&gt;() &lt;&lt; std::endl;\n\n    // Clean up\n    close_tools();\n\n    return 0;\n}\n</code></pre>"},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/#key-points","title":"Key Points","text":"<ol> <li>Conditional Routing: Use the <code>route</code> function to dynamically select the next node based on node output</li> <li>Loop Workflow: Routing can implement loops between nodes, supporting iterative search</li> <li>Context Management: Use global variables or state objects to manage context information in workflows</li> <li>YAML Parsing: Decision node returns YAML format, needs to be parsed to extract action type and parameters</li> <li>Tool Invocation: Use <code>ToolNode</code> to invoke tools with structured preprocessor and postprocessor functions</li> <li>ToolNode vs OneFuncNode: <code>ToolNode</code> provides a more structured way to call tools, with automatic handling of tool invocation format, while <code>OneFuncNode</code> requires manual tool invocation using <code>call_tool</code></li> </ol>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/","title":"Inja Template Formatting Tutorial","text":""},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#overview","title":"Overview","text":"<p>Inja is a powerful C++ template engine. The HADK framework provides convenient template formatting functionality through the <code>ChatUtils::format_inja</code> function. Using Inja templates, you can easily insert variable values into template strings to achieve dynamic content generation.</p>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#basic-usage","title":"Basic Usage","text":""},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#function-signature","title":"Function Signature","text":"<pre><code>std::string ChatUtils::format_inja(\n    const std::string&amp; template_str,\n    const std::unordered_map&lt;std::string, std::any&gt;&amp; variables\n);\n</code></pre> <p>Parameter Description: - <code>template_str</code>: Template string using <code>{{variable_name}}</code> syntax to define placeholders - <code>variables</code>: Variable map, where keys are variable names and values are corresponding variable values (supports multiple types)</p> <p>Return Value: - Formatted string</p>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#template-syntax","title":"Template Syntax","text":"<p>Inja templates use double curly braces <code>{{variable_name}}</code> to define placeholders. The template engine replaces placeholders with corresponding variable values.</p>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#examples","title":"Examples","text":""},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#example-1-simple-context-formatting","title":"Example 1: Simple Context Formatting","text":"<p>This is the most basic usage, demonstrating how to use string variables for template replacement:</p> <pre><code>#include &lt;chat_utils.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;unordered_map&gt;\n#include &lt;any&gt;\n\nstd::string simple_template = R\"(\nHello, {{name}}!\n\nToday is a {{weather}} {{time}}.\n\nBased on your {{topic}}, I've prepared the following content for you:\n\n{{content}}\n\nI hope this information is helpful!\n)\";\n\nstd::unordered_map&lt;std::string, std::any&gt; simple_variables;\nsimple_variables[\"name\"] = std::string(\"Xiao Ming\");\nsimple_variables[\"weather\"] = std::string(\"sunny\");\nsimple_variables[\"time\"] = std::string(\"morning\");\nsimple_variables[\"topic\"] = std::string(\"study plan\");\nsimple_variables[\"content\"] = std::string(\"1. Complete math homework\\n2. Read English articles\\n3. Review history knowledge\");\n\ntry {\n    std::string formatted_simple = ChatUtils::format_inja(simple_template, simple_variables);\n    std::cout &lt;&lt; formatted_simple &lt;&lt; \"\\n\";\n} catch (const std::exception&amp; e) {\n    std::cerr &lt;&lt; \"Formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n}\n</code></pre> <p>Output Result:</p> <pre><code>Hello, Xiao Ming!\n\nToday is a sunny morning.\n\nBased on your study plan, I've prepared the following content for you:\n\n1. Complete math homework\n2. Read English articles\n3. Review history knowledge\n\nI hope this information is helpful!\n</code></pre>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#example-2-summary-formatting-reference-deepsearchcpp","title":"Example 2: Summary Formatting (Reference: deepsearch.cpp)","text":"<p>This example demonstrates how to build complex prompt templates, suitable for multi-iteration summary scenarios:</p> <pre><code>std::string summary_template = R\"(\nTask\n\nYou need to summarize information around a theme. Theme: `{{topic}}`\n\nWorkflow\n\n1. Carefully read all information, combine with the theme to understand and fully comprehend the context.\n\n2. Select content related to the theme and summarize the selected content.\n\n3. If the theme is question-based, you need to summarize and infer relevant answers, otherwise summarize normally based on the theme.\n\nRequirements\n\n- The word count must not be less than {{min_words}} words, must be as many as possible.\n\n- The summarized content must be from the information provided, you cannot make things up, especially time-related information.\n\n- The summarized content must be comprehensive enough.\n\n- Logical coherence, smooth sentences.\n\n- Provide the final result directly in markdown format.\n\nInformation to Summarize\n\n```{{summary_search}}```\n)\";\n\nstd::unordered_map&lt;std::string, std::any&gt; summary_variables;\nsummary_variables[\"topic\"] = std::string(\"History of Artificial Intelligence Development\");\nsummary_variables[\"min_words\"] = 3500;\n\n// Simulate multi-iteration summary content\nstd::vector&lt;std::string&gt; all_iteration_summary = {\n    \"First search: The concept of artificial intelligence was first proposed by John McCarthy in 1956.\",\n    \"Second search: Deep learning technology made breakthrough progress in the 2010s.\",\n    \"Third search: Large language models such as the GPT series attracted widespread attention in the 2020s.\"\n};\n\n// Connect multiple summaries with separators\nstd::string all_iteration_summary_str;\nfor (size_t i = 0; i &lt; all_iteration_summary.size(); ++i) {\n    all_iteration_summary_str += all_iteration_summary[i];\n    if (i &lt; all_iteration_summary.size() - 1) {\n        all_iteration_summary_str += \"\\n---\\n\";\n    }\n}\nsummary_variables[\"summary_search\"] = all_iteration_summary_str;\n\ntry {\n    std::string formatted_summary = ChatUtils::format_inja(summary_template, summary_variables);\n    std::cout &lt;&lt; formatted_summary &lt;&lt; \"\\n\";\n} catch (const std::exception&amp; e) {\n    std::cerr &lt;&lt; \"Formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n}\n</code></pre> <p>Output Result:</p> <pre><code>Task\n\nYou need to summarize information around a theme. Theme: `History of Artificial Intelligence Development`\n\nWorkflow\n\n1. Carefully read all information, combine with the theme to understand and fully comprehend the context.\n\n2. Select content related to the theme and summarize the selected content.\n\n3. If the theme is question-based, you need to summarize and infer relevant answers, otherwise summarize normally based on the theme.\n\nRequirements\n\n- The word count must not be less than 3500 words, must be as many as possible.\n\n- The summarized content must be from the information provided, you cannot make things up, especially time-related information.\n\n- The summarized content must be comprehensive enough.\n\n- Logical coherence, smooth sentences.\n\n- Provide the final result directly in markdown format.\n\nInformation to Summarize\n\n```First search: The concept of artificial intelligence was first proposed by John McCarthy in 1956.\n---\nSecond search: Deep learning technology made breakthrough progress in the 2010s.\n---\nThird search: Large language models such as the GPT series attracted widespread attention in the 2020s.```\n</code></pre>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#example-3-using-different-types-of-variables","title":"Example 3: Using Different Types of Variables","text":"<p>Inja templates support variables of multiple data types, including strings, integers, floating-point numbers, booleans, and container types:</p> <pre><code>std::string mixed_template = R\"(\nUser Information:\n\n- Name: {{name}}\n- Age: {{age}}\n- VIP Status: {{is_vip}}\n- Points: {{points}}\n- Tags: {{tags}}\n)\";\n\nstd::unordered_map&lt;std::string, std::any&gt; mixed_variables;\nmixed_variables[\"name\"] = std::string(\"Zhang San\");\nmixed_variables[\"age\"] = 28;\nmixed_variables[\"is_vip\"] = true;\nmixed_variables[\"points\"] = 1250.5;\n\nstd::vector&lt;std::string&gt; tags = {\"Active User\", \"Tech Enthusiast\", \"Early User\"};\nmixed_variables[\"tags\"] = tags;\n\ntry {\n    std::string formatted_mixed = ChatUtils::format_inja(mixed_template, mixed_variables);\n    std::cout &lt;&lt; formatted_mixed &lt;&lt; \"\\n\";\n} catch (const std::exception&amp; e) {\n    std::cerr &lt;&lt; \"Formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n}\n</code></pre> <p>Output Result:</p> <pre><code>User Information:\n\n- Name: Zhang San\n- Age: 28\n- VIP Status: true\n- Points: 1250.5\n- Tags: Active User, Tech Enthusiast, Early User\n</code></pre>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#complete-example-program","title":"Complete Example Program","text":"<p>The following is a complete example program demonstrating all three examples:</p> <pre><code>#include &lt;chat_utils.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;unordered_map&gt;\n#include &lt;any&gt;\n\nint main()\n{\n    std::cout &lt;&lt; \"=== Inja Template Formatting Examples === \\n\";\n\n    // Example 1: Simple context formatting\n    std::cout &lt;&lt; \"\u3010Example 1\u3011Simple Context Formatting\\n\";\n    std::string simple_template = R\"(\nHello, {{name}}!\n\nToday is a {{weather}} {{time}}.\n\nBased on your {{topic}}, I've prepared the following content for you:\n\n{{content}}\n\nI hope this information is helpful!\n\n)\";\n\n    std::unordered_map&lt;std::string, std::any&gt; simple_variables;\n    simple_variables[\"name\"] = std::string(\"Xiao Ming\");\n    simple_variables[\"weather\"] = std::string(\"sunny\");\n    simple_variables[\"time\"] = std::string(\"morning\");\n    simple_variables[\"topic\"] = std::string(\"study plan\");\n    simple_variables[\"content\"] = std::string(\"1. Complete math homework\\n2. Read English articles\\n3. Review history knowledge\");\n\n    try {\n        std::string formatted_simple = ChatUtils::format_inja(simple_template, simple_variables);\n        std::cout &lt;&lt; formatted_simple &lt;&lt; \"\\n\";\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"Formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n    }\n\n    std::cout &lt;&lt; \"\\n\" &lt;&lt; std::string(50, '-') &lt;&lt; \"\\n\";\n\n    // Example 2: Summary formatting similar to deepsearch.cpp\n    std::cout &lt;&lt; \"\u3010Example 2\u3011Summary Formatting (Reference: deepsearch.cpp)\\n\";\n    std::string summary_template = R\"(\nTask\n\nYou need to summarize information around a theme. Theme: `{{topic}}`\n\nWorkflow\n\n1. Carefully read all information, combine with the theme to understand and fully comprehend the context.\n\n2. Select content related to the theme and summarize the selected content.\n\n3. If the theme is question-based, you need to summarize and infer relevant answers, otherwise summarize normally based on the theme.\n\nRequirements\n\n- The word count must not be less than {{min_words}} words, must be as many as possible.\n\n- The summarized content must be from the information provided, you cannot make things up, especially time-related information.\n\n- The summarized content must be comprehensive enough.\n\n- Logical coherence, smooth sentences.\n\n- Provide the final result directly in markdown format.\n\nInformation to Summarize\n\n```{{summary_search}}```\n\n)\";\n\n    std::unordered_map&lt;std::string, std::any&gt; summary_variables;\n    summary_variables[\"topic\"] = std::string(\"History of Artificial Intelligence Development\");\n    summary_variables[\"min_words\"] = 3500;\n\n    // Simulate multi-iteration summary content\n    std::vector&lt;std::string&gt; all_iteration_summary = {\n        \"First search: The concept of artificial intelligence was first proposed by John McCarthy in 1956.\",\n        \"Second search: Deep learning technology made breakthrough progress in the 2010s.\",\n        \"Third search: Large language models such as the GPT series attracted widespread attention in the 2020s.\"\n    };\n\n    // Connect multiple summaries with separators\n    std::string all_iteration_summary_str;\n    for (size_t i = 0; i &lt; all_iteration_summary.size(); ++i) {\n        all_iteration_summary_str += all_iteration_summary[i];\n        if (i &lt; all_iteration_summary.size() - 1) {\n            all_iteration_summary_str += \"\\n---\\n\";\n        }\n    }\n    summary_variables[\"summary_search\"] = all_iteration_summary_str;\n\n    try {\n        std::string formatted_summary = ChatUtils::format_inja(summary_template, summary_variables);\n        std::cout &lt;&lt; formatted_summary &lt;&lt; \"\\n\";\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"Formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n    }\n\n    std::cout &lt;&lt; \"\\n\" &lt;&lt; std::string(50, '-') &lt;&lt; \"\\n\";\n\n    // Example 3: Using different types of variables\n    std::cout &lt;&lt; \"\u3010Example 3\u3011Using Different Types of Variables\\n\";\n    std::string mixed_template = R\"(\nUser Information:\n\n- Name: {{name}}\n- Age: {{age}}\n- VIP Status: {{is_vip}}\n- Points: {{points}}\n- Tags: {{tags}}\n\n)\";\n\n    std::unordered_map&lt;std::string, std::any&gt; mixed_variables;\n    mixed_variables[\"name\"] = std::string(\"Zhang San\");\n    mixed_variables[\"age\"] = 28;\n    mixed_variables[\"is_vip\"] = true;\n    mixed_variables[\"points\"] = 1250.5;\n\n    std::vector&lt;std::string&gt; tags = {\"Active User\", \"Tech Enthusiast\", \"Early User\"};\n    mixed_variables[\"tags\"] = tags;\n\n    try {\n        std::string formatted_mixed = ChatUtils::format_inja(mixed_template, mixed_variables);\n        std::cout &lt;&lt; formatted_mixed &lt;&lt; \"\\n\";\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"Formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n    }\n\n    std::cout &lt;&lt; \"\\n=== Examples Complete ===\\n\";\n\n    return 0;\n}\n</code></pre>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#supported-variable-types","title":"Supported Variable Types","text":"<p>The <code>ChatUtils::format_inja</code> function supports the following variable types:</p> <ul> <li>String (<code>std::string</code>): The most commonly used type, directly replaced in the template</li> <li>Integer (<code>int</code>, <code>long</code>, <code>long long</code>, etc.): Automatically converted to string</li> <li>Floating-point (<code>float</code>, <code>double</code>): Automatically converted to string</li> <li>Boolean (<code>bool</code>): Converted to \"true\" or \"false\"</li> <li>Container Types (<code>std::vector&lt;T&gt;</code>): Automatically converted to comma-separated string</li> </ul>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#error-handling","title":"Error Handling","text":"<p>When template formatting fails, the <code>format_inja</code> function throws a <code>std::exception</code> exception. It's recommended to use <code>try-catch</code> blocks to catch exceptions and perform appropriate error handling:</p> <pre><code>try {\n    std::string result = ChatUtils::format_inja(template_str, variables);\n    // Use formatted result\n} catch (const std::exception&amp; e) {\n    std::cerr &lt;&lt; \"Template formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n    // Handle error situation\n}\n</code></pre>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use Raw String Literals: Use <code>R\"(...)\"</code> syntax to define template strings, which preserves newlines and special characters, improving readability</p> </li> <li> <p>Variable Naming Conventions: Use meaningful variable names for easier understanding and maintenance</p> </li> <li> <p>Error Handling: Always use <code>try-catch</code> blocks to catch possible exceptions</p> </li> <li> <p>Template Reuse: Define commonly used templates as constants or functions for easy reuse</p> </li> <li> <p>Type Safety: Ensure variable types match the usage scenarios in templates</p> </li> </ol>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#application-scenarios","title":"Application Scenarios","text":"<p>Inja template formatting is commonly used in the HADK framework for the following scenarios:</p> <ul> <li>Prompt Construction: Dynamically build LLM prompts, generating different prompts based on context and task requirements</li> <li>Message Formatting: Format user messages, system messages, etc.</li> <li>Summary Generation: Build multi-iteration summary prompts</li> <li>Tool Invocation Parameters: Dynamically generate parameter descriptions for tool invocations</li> </ul>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#reference-resources","title":"Reference Resources","text":"<ul> <li>Inja Official Documentation</li> <li>HADK Framework Documentation</li> </ul>"},{"location":"2-5_ApplicationDevelopmentCoTAgent/","title":"CoT Agent Development Example","text":""},{"location":"2-5_ApplicationDevelopmentCoTAgent/#overview","title":"Overview","text":"<p>This example demonstrates how to use CoT (Chain of Thought) nodes from HADK to create an Agent that can solve complex problems through step-by-step reasoning. ref. link</p>"},{"location":"2-5_ApplicationDevelopmentCoTAgent/#question-ref","title":"Question Ref.","text":"<ul> <li>If a train travels 60 miles in 1 hour and then 90 miles in 1.5 hours, what is its average speed for the entire trip?</li> <li>Alice is older than Bob. Bob is older than Charlie. Who is the youngest?</li> <li>You flip a fair coin 3 times. What is the probability of getting exactly 2 heads?</li> </ul>"},{"location":"2-5_ApplicationDevelopmentCoTAgent/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p><code>LLM_API_URL</code> and  <code>LLM_API_KEY</code> can be configured using <code>chat_node::chat_node_settings</code>, For more details, please refer to BasicConceptsChatNode </p>"},{"location":"2-5_ApplicationDevelopmentCoTAgent/#api-functions","title":"API Functions","text":""},{"location":"2-5_ApplicationDevelopmentCoTAgent/#1-init_loc_tools","title":"1. init_loc_tools()","text":"<p>Initialize and register local tools.</p> <p>Function Signature:</p> <pre><code>bool init_loc_tools();\n</code></pre> <p>Description: - Registers local tools (e.g., <code>search_web2</code> for web search) - Returns <code>true</code> on success, <code>false</code> on failure</p> <p>Example:</p> <pre><code>#include \"src/cot_agent/koba_agent.h\"\n\nbool reg_loc_tools = init_loc_tools();\n</code></pre> <p>Note: The function internally registers tools using the OpenAI Function Call format. The simplest function call format example:</p> <pre><code>{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"search_web2\",\n    \"description\": \"Search the web\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"query\": {\"type\": \"string\", \"description\": \"Search query\"}\n      },\n      \"required\": [\"query\"]\n    }\n  }\n}\n</code></pre>"},{"location":"2-5_ApplicationDevelopmentCoTAgent/#2-init_mcp_tools","title":"2. init_mcp_tools()","text":"<p>Initialize and register MCP (Model Context Protocol) tools by connecting to remote tool servers.</p> <p>Function Signature:</p> <pre><code>bool init_mcp_tools(const char* params);\n</code></pre> <p>Parameters: - <code>params</code>: JSON string containing server configuration. Format: <code>{\"ServerName\":{\"url\":\"http://host:port/sse\"}}</code></p> <p>Description: - Registers remote MCP tool servers - Waits 10 seconds for server initialization - Returns <code>true</code> on success, <code>false</code> on failure</p> <p>Example:</p> <pre><code>#include \"src/cot_agent/koba_agent.h\"\n\nbool reg_mcp_tools = init_mcp_tools(R\"({\"WeatherServer\":{\"url\":\"http://18.119.131.41:8006/sse\"}})\");\n</code></pre>"},{"location":"2-5_ApplicationDevelopmentCoTAgent/#3-get_tools_init_status","title":"3. get_tools_init_status()","text":"<p>Get the initialization status of a specific tool server.</p> <p>Function Signature:</p> <pre><code>int get_tools_init_status(const char* name);\n</code></pre> <p>Parameters: - <code>name</code>: Name of the tool server</p> <p>Return Value: - Returns initialization status code (0 for success, -1 for error or not found)</p> <p>Example:</p> <pre><code>#include \"src/cot_agent/koba_agent.h\"\n\nint status = get_tools_init_status(\"WeatherServer\");\n</code></pre>"},{"location":"2-5_ApplicationDevelopmentCoTAgent/#4-koba_agent","title":"4. koba_agent()","text":"<p>Core agent function that processes user queries using CoT (Chain of Thought) reasoning and returns responses.</p> <p>Function Signature:</p> <pre><code>const char* koba_agent(const char* message);\n</code></pre> <p>Parameters: - <code>message</code>: JSON string in OpenAI Chat Completion format (array of message objects)</p> <p>Return Value: - Returns a JSON string containing the conversation response</p> <p>Message Format: The input message should be a JSON array following OpenAI Chat Completion format:</p> <pre><code>[\n  {\"role\": \"user\", \"content\": \"What is the weather like in San Francisco today?\"}\n]\n</code></pre> <p>Example:</p> <pre><code>#include \"src/cot_agent/koba_agent.h\"\n#include &lt;nlohmann/json.hpp&gt;\n\nnlohmann::json inputJson = nlohmann::json::array({\n    {{\"role\", \"user\"}, {\"content\", \"What is the weather like in San Francisco today?\"}}\n});\n\nconst char* response = koba_agent(inputJson.dump().c_str());\n</code></pre> <p>Environment Variables: This function requires the following environment variables to be set: - <code>LLM_API_URL</code>: LLM API endpoint URL - <code>LLM_API_KEY</code>: LLM API key</p> <p>Node Configuration: The agent uses CoT nodes with the following configuration: - extra_node: Extracts problem from input - cot_node: Chain of thought node for step-by-step reasoning   - Model: <code>gpt-4o-mini</code>   - Temperature: <code>0.7</code>   - Top-p: <code>0.95</code>   - Max tokens: <code>4096</code>   - Tool choice: <code>none</code> - polish_node: Polishes final answer   - Model: <code>gpt-4o-mini</code>   - Temperature: <code>0.7</code>   - Top-p: <code>0.95</code>   - Max tokens: <code>2048</code>   - Tool choice: <code>none</code></p> <p>Flow: query \u2192 extra_node \u2192 cot_node (loop) \u2192 polish_node \u2192 response</p>"},{"location":"2-5_ApplicationDevelopmentCoTAgent/#5-close_tools","title":"5. close_tools()","text":"<p>Close all registered tool servers and clean up resources.</p> <p>Function Signature:</p> <pre><code>void close_tools();\n</code></pre> <p>Description: - Shuts down all MCP tool servers - Should be called before program exit</p> <p>Example:</p> <pre><code>#include \"src/cot_agent/koba_agent.h\"\n\nclose_tools();\n</code></pre>"},{"location":"2-6_ApplicationDevelopmentBatchNode/","title":"Batch Node Agent Development Example","text":""},{"location":"2-6_ApplicationDevelopmentBatchNode/#overview","title":"Overview","text":"<p>This example demonstrates how to use HADK <code>BatchFuncNode</code> to build a simple batch processing flow, where multiple text inputs are generated, processed in batch, and then routed to the next node. ref. link</p>"},{"location":"2-6_ApplicationDevelopmentBatchNode/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent or LLM-based tools, configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p><code>LLM_API_URL</code> and  <code>LLM_API_KEY</code> can be configured using <code>chat_node::chat_node_settings</code>, For more details, please refer to BasicConceptsChatNode </p>"},{"location":"2-6_ApplicationDevelopmentBatchNode/#api-functions","title":"API Functions","text":""},{"location":"2-6_ApplicationDevelopmentBatchNode/#1-init_loc_tools","title":"1. init_loc_tools()","text":"<p>Initialize and register local tools.</p> <p>Function Signature:</p> <pre><code>bool init_loc_tools();\n</code></pre> <p>Description: - Registers local tools (e.g., <code>search_web2</code> for web search) - Returns <code>true</code> on success, <code>false</code> on failure</p> <p>Example:</p> <pre><code>#include \"src/batch_flow/koba_agent.h\"\n\nbool reg_loc_tools = init_loc_tools();\n</code></pre> <p>Note: The function internally registers tools using the OpenAI Function Call format. The simplest function call format example:</p> <pre><code>{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"search_web2\",\n    \"description\": \"Search the web\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"query\": { \"type\": \"string\", \"description\": \"Search query\" }\n      },\n      \"required\": [\"query\"]\n    }\n  }\n}\n</code></pre>"},{"location":"2-6_ApplicationDevelopmentBatchNode/#2-init_mcp_tools","title":"2. init_mcp_tools()","text":"<p>Initialize and register MCP (Model Context Protocol) tools by connecting to remote tool servers.</p> <p>Function Signature:</p> <pre><code>bool init_mcp_tools(const char* params);\n</code></pre> <p>Parameters: - <code>params</code>: JSON string containing server configuration. Format: <code>{\"ServerName\":{\"url\":\"http://host:port/sse\"}}</code></p> <p>Description: - Registers remote MCP tool servers - Waits 10 seconds for server initialization - Returns <code>true</code> on success, <code>false</code> on failure</p> <p>Example:</p> <pre><code>#include \"src/batch_flow/koba_agent.h\"\n\nbool reg_mcp_tools = init_mcp_tools(R\"({\"WeatherServer\":{\"url\":\"http://18.119.131.41:8006/sse\"}})\");\n</code></pre>"},{"location":"2-6_ApplicationDevelopmentBatchNode/#3-get_tools_init_status","title":"3. get_tools_init_status()","text":"<p>Get the initialization status of a specific tool server.</p> <p>Function Signature:</p> <pre><code>int get_tools_init_status(const char* name);\n</code></pre> <p>Parameters: - <code>name</code>: Name of the tool server</p> <p>Return Value: - Returns initialization status code (0 for success, -1 for error or not found)</p> <p>Example:</p> <pre><code>#include \"src/batch_flow/koba_agent.h\"\n\nint status = get_tools_init_status(\"WeatherServer\");\n</code></pre>"},{"location":"2-6_ApplicationDevelopmentBatchNode/#4-koba_agent","title":"4. koba_agent()","text":"<p>Core agent function that constructs a batch flow, runs it, and returns the final result.</p> <p>Function Signature:</p> <pre><code>const char* koba_agent(const char* message);\n</code></pre> <p>Parameters: - <code>message</code>: JSON string in OpenAI Chat Completion format (array of message objects). In the current demo implementation, the content is not used by the batch flow logic.</p> <p>Return Value: - Returns a C-style string containing the final result of the flow</p> <p>Message Format: The input message should be a JSON array following OpenAI Chat Completion format, for example:</p> <pre><code>[\n  {\n    \"role\": \"user\",\n    \"content\": \"Run a batch demo.\"\n  }\n]\n</code></pre> <p>Batch Flow Configuration (Demo): - <code>create_node</code>: <code>OneFuncNode&lt;std::string, std::vector&lt;std::string&gt;&gt;</code>   - Input: a single string   - Output: a vector of strings, e.g. <code>[\"I love Lenovo\", \"I love work\", \"I love life\"]</code>   - Routed with action <code>\"batch\"</code> - <code>batch_node</code>: <code>BatchFuncNode&lt;std::string, std::string&gt;</code>   - Input: each string from the vector   - Output: processed string, e.g. <code>\"I love Lenovo Stop lying.\"</code>   - Routed with action <code>\"summarize\"</code></p> <p>The current demo implementation returns a fixed string <code>\"hello world\"</code> after running the flow, which can be replaced with real business logic as needed.</p> <p>Environment Variables: When <code>koba_agent</code> is extended to call LLM or tools, the following environment variables must be set: - <code>LLM_API_URL</code>: LLM API endpoint URL - <code>LLM_API_KEY</code>: LLM API key - <code>TAVILY_API_URL</code>: Tavily search API endpoint URL - <code>TAVILY_API_KEY</code>: Tavily API key  </p>"},{"location":"2-6_ApplicationDevelopmentBatchNode/#5-close_tools","title":"5. close_tools()","text":"<p>Close all registered tool servers and clean up resources.</p> <p>Function Signature:</p> <pre><code>void close_tools();\n</code></pre> <p>Description: - Shuts down all MCP tool servers - Should be called before program exit</p> <p>Example:</p> <pre><code>#include \"src/batch_flow/koba_agent.h\"\n\nclose_tools();\n</code></pre>"},{"location":"2-7_ApplicationChatBot/","title":"Chat Bot Development Example","text":""},{"location":"2-7_ApplicationChatBot/#overview","title":"Overview","text":"<p>This example demonstrates how to construct a simple interactive chat bot (<code>loop_chat.cpp</code>) based on Single Node Agent. The chat bot supports multi-turn conversations with automatic history management. ref. link</p>"},{"location":"2-7_ApplicationChatBot/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent or LLM-based tools, configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p><code>LLM_API_URL</code> and  <code>LLM_API_KEY</code> can be configured using <code>chat_node::chat_node_settings</code>, For more details, please refer to BasicConceptsChatNode </p>"},{"location":"2-7_ApplicationChatBot/#implementation-overview","title":"Implementation Overview","text":"<p>The <code>loop_chat.cpp</code> example implements an interactive chat bot with the following key components:</p>"},{"location":"2-7_ApplicationChatBot/#chat-history-management","title":"Chat History Management","text":"<p>The <code>KBChatHistory</code> structure manages conversation history: - Automatically initializes with a system prompt that includes the current timestamp - Maintains a JSON array of messages following OpenAI Chat Completion format - Tracks message count to identify new messages from agent responses - Provides methods to add user input and update from agent responses</p>"},{"location":"2-7_ApplicationChatBot/#main-flow","title":"Main Flow","text":"<ol> <li>Initialization: Registers MCP tool servers (e.g., WeatherServer) and local tools</li> <li>Chat Loop: </li> <li>Reads user input from console</li> <li>Adds user message to chat history</li> <li>Calls <code>koba_agent()</code> with the complete conversation history</li> <li>Extracts and adds only new messages from the response to history</li> <li>Displays the assistant's reply</li> <li>Cleanup: Closes all tool servers before exit</li> </ol>"},{"location":"2-7_ApplicationChatBot/#key-features","title":"Key Features","text":"<ul> <li>Multi-turn Conversations: Maintains full conversation context across turns</li> <li>Incremental History Updates: Only adds new messages from agent responses, avoiding duplicates</li> <li>System Prompt: Automatically includes current time in system prompt for time-aware responses</li> </ul>"},{"location":"2-7_ApplicationChatBot/#api-functions","title":"API Functions","text":""},{"location":"2-7_ApplicationChatBot/#1-init_loc_tools","title":"1. init_loc_tools()","text":"<p>Initialize and register local tools.</p> <p>Function Signature:</p> <pre><code>bool init_loc_tools();\n</code></pre> <p>Description: - Registers local tools (e.g., <code>search_web2</code> for web search) - Returns <code>true</code> on success, <code>false</code> on failure</p> <p>Example:</p> <pre><code>#include \"src/single_node/koba_agent.h\"\n\nbool reg_loc_tools = init_loc_tools();\n</code></pre>"},{"location":"2-7_ApplicationChatBot/#2-add_server-init_mcp_tools","title":"2. add_server() / init_mcp_tools()","text":"<p>Initialize and register MCP (Model Context Protocol) tools by connecting to remote tool servers.</p> <p>Function Signature:</p> <pre><code>bool init_mcp_tools(const char* params);\n// Or directly use: common_tools::tools::add_server(const std::string&amp; params)\n</code></pre> <p>Parameters: - <code>params</code>: JSON string containing server configuration. Format: <code>{\"ServerName\":{\"url\":\"http://host:port\",\"sse_endpoint\":\"/sse\"}}</code></p> <p>Description: - Registers remote MCP tool servers - Returns <code>true</code> on success, <code>false</code> on failure</p> <p>Example:</p> <pre><code>#include \"src/single_node/koba_agent.h\"\n\nconst auto r1 = add_server(\n    R\"({\"WeatherServer\":{\"url\":\"http://18.119.131.41:8006\",\"sse_endpoint\":\"/sse\"}})\");\n</code></pre>"},{"location":"2-7_ApplicationChatBot/#3-get_server_init_status-get_tools_init_status","title":"3. get_server_init_status() / get_tools_init_status()","text":"<p>Get the initialization status of a specific tool server.</p> <p>Function Signature:</p> <pre><code>int get_tools_init_status(const char* name);\n// Or directly use: common_tools::tools::get_server_init_status(const std::string&amp; name)\n</code></pre> <p>Parameters: - <code>name</code>: Name of the tool server</p> <p>Return Value: - Returns initialization status code (0 for success, -1 for error or not found)</p> <p>Example:</p> <pre><code>#include \"src/single_node/koba_agent.h\"\n\nint status = get_server_init_status(\"WeatherServer\");\n</code></pre>"},{"location":"2-7_ApplicationChatBot/#4-koba_agent","title":"4. koba_agent()","text":"<p>Core agent function that processes messages using a single node agent and returns the complete conversation history.</p> <p>Function Signature:</p> <pre><code>const char* koba_agent(const char* message);\n</code></pre> <p>Parameters: - <code>message</code>: JSON string in OpenAI Chat Completion format (array of message objects), including system prompt and conversation history</p> <p>Return Value: - Returns a C-style string containing the complete conversation history (JSON array of messages) - The response includes all previous messages plus new assistant messages</p> <p>Message Format: The input message should be a JSON array following OpenAI Chat Completion format:</p> <pre><code>[\n  {\n    \"role\": \"system\",\n    \"content\": \"You are a helpful assistant...\"\n  },\n  {\n    \"role\": \"user\",\n    \"content\": \"What's the weather today?\"\n  }\n]\n</code></pre> <p>Node Configuration: - Uses a single <code>generate_node</code> (ChatNode) with LLM to generate responses - Supports tool calling (can use <code>search_web2</code> and registered MCP tools) - Returns complete conversation history including new assistant messages</p> <p>Note: Environment variables must be configured as described in the Environment Configuration section above.</p>"},{"location":"2-7_ApplicationChatBot/#5-close_tools","title":"5. close_tools()","text":"<p>Close all registered tool servers and clean up resources.</p> <p>Function Signature:</p> <pre><code>void close_tools();\n</code></pre> <p>Description: - Shuts down all MCP tool servers - Should be called before program exit</p> <p>Example:</p> <pre><code>#include \"src/single_node/koba_agent.h\"\n\nclose_tools();\n</code></pre>"},{"location":"3-1_AndroidPlatform/","title":"Android Platform Development","text":""},{"location":"3-1_AndroidPlatform/#overview","title":"Overview","text":"<p>HADK is a cross-platform development framework. This document provides complete examples for compiling and running agents on the Android platform, including cross-compilation configuration and Android Studio project integration. </p>"},{"location":"3-1_AndroidPlatform/#cross-compilation-reference","title":"Cross-compilation Reference","text":"<p>For complete cross-compilation examples and configurations, please refer to: Cross-Compilation Agent Example</p>"},{"location":"3-1_AndroidPlatform/#agent-development","title":"Agent Development","text":"<p>For the agent implementation used, please refer to: Single Node Agent Development Guide</p>"},{"location":"3-1_AndroidPlatform/#android-studio-integration","title":"Android Studio Integration","text":"<p>We provide a complete Android Studio example project that demonstrates how to call cross-compiled agents through <code>Kotlin + JNI</code> and execute  <code>local tool calls</code> to implement a simple chatbot.</p> <p>For the complete Android Studio example project, please refer to: Android Demo Project</p>"},{"location":"3-1_AndroidPlatform/#local-tools","title":"Local Tools","text":"<p>This demo provides the following local tools that can be called through function call:</p> <ul> <li>Control device Bluetooth on/off</li> <li>Dynamically adjust device screen brightness</li> <li>Control device flashlight on/off</li> <li>Control device volume</li> <li>Screen capture</li> </ul>"}]}