{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"HADK Development Documentation","text":"<p>Welcome to the HADK framework development documentation. </p> <p>We came here to code a Hybrid Agent and chew bubblegum... and I'm all out of bubblegum !</p>"},{"location":"#what-it-is","title":"What It Is","text":"<p>HADK (Hybrid Agent Development Kit) is a C++-based, cross-platform framework for building intelligent agents. Built on computational graph principles, it offers a modular architecture with rich functional nodes and flexible orchestration. Developers can quickly create agents with advanced reasoning, tool integration, and multimodal capabilities that run seamlessly across Windows, Linux, and Android.</p>"},{"location":"#its-advantages","title":"Its Advantages","text":"<ul> <li>Cross-platform: Write once, run on Windows, Linux, and Android with a unified API</li> <li>Node-based Architecture: Computational graph-based orchestration with conditional routing, nested flows, and loops</li> <li>Type Safety: Compile-time type checking via C++ templates to catch errors early</li> <li>High Performance: Native compilation with zero-copy optimization and concurrent processing</li> <li>Tool Ecosystem: Integrate local tools, remote servers (MCP/SSE), or build your own</li> <li>Modular: Clean separation of concerns for easy extension and reuse</li> </ul>"},{"location":"#development-environment","title":"Development Environment","text":"<ul> <li>C++ Compiler: Supports C++17 or higher</li> <li>CMake: Version 3.24 or higher</li> <li>Operating System: Windows 10+, Linux (Ubuntu 20.04+), Android (API 21+)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#basic-concepts","title":"Basic Concepts","text":"<ul> <li>Chat Node - Chat node integrated with large language models</li> <li>Tool Node - Tool management node</li> <li>Custom Node - User-defined node for custom logic</li> <li>CE Node - Context engine node for managing conversation history</li> <li>Chain and Flow - Workflow management</li> <li>Route - Routing node</li> </ul>"},{"location":"#application-development","title":"Application Development","text":"<ul> <li>Single Node Agent - Single node agent development example</li> <li>Normal Agent - Normal agent development example</li> <li>Three Node Agent - Three node agent development example</li> <li>Inja Template Formatting - Inja template engine tutorial</li> <li>CoT Agent - CoT (Chain of Thought) agent development example</li> <li>Batch Node - Batch Node development example</li> <li>Chat Bot - Simple Chat Bot development example</li> </ul>"},{"location":"#runner","title":"Runner","text":"<ul> <li>Runner Introduction - Runner framework overview and core concepts</li> <li>Single Node Agent with Runner - Single node agent development with Runner (class-based)</li> <li>Normal Agent with Runner - Normal agent development with Runner (class-based)</li> <li>Conditional Routing Agent with Runner - Conditional routing agent development with Runner (class-based)</li> </ul>"},{"location":"#android","title":"Android","text":"<ul> <li>Android Platform - Cross Compilation and Android Project</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Please download the HADK dynamic library from this link</p> <p>Selecting the chapter you're interested in from the left navigation bar to start reading. </p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>Thanks to the following open-source projects for their support:</p> <ul> <li>cpr - HTTP client</li> <li>nlohmann/json - JSON library</li> <li>spdlog - Logging library</li> <li>yaml-cpp - YAML parsing library</li> <li>inja - Template engine</li> </ul>"},{"location":"#contact","title":"Contact","text":"<ul> <li>liusong9@lenovo.com</li> <li>zengjl1@lenovo.com</li> <li>moutz1@lenovo.com</li> <li>xufeng8@lenovo.com</li> </ul>"},{"location":"1-1_BasicConceptsChatNode/","title":"Chat Node","text":"<p>Chat Node is a node component that integrates large language models (LLM) and function calling capabilities, supporting both online and offline model invocation. Its interface strictly follows the OpenAI Chat Completion API specification.</p>"},{"location":"1-1_BasicConceptsChatNode/#building-a-chat-node","title":"Building a Chat Node","text":"<p>Build a chat node in the following way, allowing parameter configuration for model invocation:</p> <pre><code>chat_node::chat_node_settings s_generate;\ns_generate.llm_mode = chat_node::LLMMode::OpenAI;  //default\ns_generate.llm_url = \"http://3rd/api/chat\";\ns_generate.llm_key = \"3rdkey\"; \ns_generate.model = \"gpt-4.1\";\ns_generate.temperature = 0.7;\ns_generate.top_p = 0.95;\ns_generate.max_tokens = 4096;\ns_generate.tool_choice = \"none\";\nconst auto generate_node = std::make_shared&lt;chat_node::ChatNode&lt;std::string, std::string&gt;&gt;(s_generate);\n</code></pre> <p>Currently, two LLM Modes are supported, allowing you to use a local Ollama model via:</p> <pre><code>chat_node::chat_node_settings s_generate;\ns_generate.llm_mode = chat_node::LLMMode::Ollama;\ns_generate.model = \"qwen3-vl:4b\"; // model to use\ns_generate.llm_url = \"http://127.0.0.1:11434/api/chat\";\ns_generate.llm_key = \"\";\ns_generate.temperature = 0.7;\ns_generate.top_p = 0.95;\ns_generate.max_tokens = 4096;\ns_generate.tool_choice = \"none\";\nconst auto generate_node = std::make_shared&lt;chat_node::ChatNode&lt;std::string, std::string&gt;&gt;(s_generate);\n</code></pre> <p>Otherwise , you can configure the following environment variables for LLM Model:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\n</code></pre>"},{"location":"1-1_BasicConceptsChatNode/#chat-node-input-and-output","title":"Chat Node Input and Output","text":"<p>The default data type for both input and output is <code>std::string</code> (for custom data types, please refer to <code>Advanced Usage</code>).</p>"},{"location":"1-1_BasicConceptsChatNode/#input-format","title":"Input Format","text":"<p>The input data structure strictly follows the OpenAI Chat Completion API specification, with the following format:</p> <pre><code>[\n  {\"role\":\"system\",\"content\":\"you are a helpful assistant\"},\n  {\"role\":\"user\",\"content\":\"who are you?\"}\n]\n</code></pre> <p>It also supports multimodal input (if the model supports it):</p> <pre><code>[\n    {\n        \"content\": \"You are a helpful assistant with multiple tools.\",\n        \"role\": \"system\"\n    },\n    {\n        \"content\": [\n            {\n                \"text\": \"what is this?\",\n                \"type\": \"text\"\n            },\n            {\n                \"image_url\": {\n                    \"detail\": \"low\",\n                    \"url\": \"https://1.bp.blogspot.com/529.jpg\"\n                },\n                \"type\": \"image_url\"\n            }\n        ],\n        \"role\": \"user\"\n    }\n]\n</code></pre> <p>Alternatively, images can be represented in base64 format:</p> <pre><code>[\n    {\n        \"content\": \"You are a helpful assistant with multiple tools.\",\n        \"role\": \"system\"\n    },\n    {\n        \"content\": [\n            {\n                \"text\": \"what is this?\",\n                \"type\": \"text\"\n            },\n            {\n                \"image_url\": {\n                    \"detail\": \"low\",\n                    \"url\": \"data:image/png;base64,dfadfnakjenqlkmdcklasjdflkadslfkadsokfqmldf\"\n                },\n                \"type\": \"image_url\"\n            }\n        ],\n        \"role\": \"user\"\n    }\n]\n</code></pre>"},{"location":"1-1_BasicConceptsChatNode/#output-format","title":"Output Format","text":"<p>The output data structure strictly follows the OpenAI Chat Completion API specification, with the following format:</p> <pre><code>[\n  {\"role\":\"system\",\"content\":\"you are a helpful assistant\"},\n  {\"role\":\"user\",\"content\":\"who are you?\"},\n  {\"role\":\"assistant\",\"content\":\"my name is bob.\"}\n]\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/","title":"Tool Node","text":"<p>Tool Node is a tool management node that supports the following features:</p> <ul> <li>Tool Registration and Management: Supports registration, execution, and destruction of MCP (Model Context Protocol) tools (SSE/STDIO) and local tools</li> <li>Function Call Integration: Can serve as the foundation for Chat Node's Function Call, or execute independently</li> <li>Custom Extensions: Supports user-defined local functions registered to the Tool Node</li> </ul>"},{"location":"1-2_BasicConceptsToolNode/#registering-tools","title":"Registering Tools","text":""},{"location":"1-2_BasicConceptsToolNode/#registering-local-tools","title":"Registering Local Tools","text":"<p>For local tool development specifications, please refer to <code>Local Tool Development Specifications</code>.</p> <pre><code>common_tools::tools::add_function_call(search_news_tool,\n    R\"({\"type\":\"function\",\"function\":{\"name\":\"search_news\",\"description\":\"Search for the latest news information using keywords\",\"parameters\":{\"type\":\"object\",\"properties\":{\"query\":{\"type\":\"string\",\"description\":\"Search keywords\",\"minLength\":1},\"time_range\":{\"type\":\"string\",\"enum\":[\"day\",\"week\"]},\"country\":{\"type\":\"string\",\"enum\":[\"china\",\"usa\",\"japan\"]}},\"required\":[\"query\"]}}})\"\n);\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/#registering-mcp-tools","title":"Registering MCP Tools","text":"<pre><code>const auto r1 = common_tools::tools::add_server(\n    R\"({\"WeatherServer\":{\"url\":\"http://18.119.131.41:8006\",\"sse_endpoint\":\"/sse\"}})\"\n);\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/#getting-tool-status-mcp-tools-only","title":"Getting Tool Status (MCP Tools Only)","text":"<p>The function to get the initialization status of MCP tools is as follows:</p> <pre><code>int status = common_tools::tools::get_server_init_status(std::string(name));\n</code></pre> <p>Parameter Description: - <code>name</code>: The unique identifier name of the tool (case-sensitive)</p> <p>Return Value: - <code>1</code>: Initialization successful - <code>2</code>: Initialization failed - <code>0</code>: Initialization in progress - <code>-1</code>: Unknown status</p>"},{"location":"1-2_BasicConceptsToolNode/#closing-tools-mcp-tools-only","title":"Closing Tools (MCP Tools Only)","text":"<p>The function to close all MCP tools is as follows:</p> <pre><code>common_tools::tools::shutdown_all_servers();\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/#executing-tools","title":"Executing Tools","text":""},{"location":"1-2_BasicConceptsToolNode/#automatic-invocation-via-chat-node","title":"Automatic Invocation via Chat Node","text":"<p>Tools can serve as Chat Node's Function Call functions, automatically judged and invoked by the model. By configuring as follows, Chat Node will have tool invocation capabilities:</p> <pre><code>chat_node::chat_node_settings s;\ns.model = \"gpt-4o-mini\";\ns.temperature = 0.7;\ns.max_tokens = 4096;\ns.tool_choice = \"auto\";  // Enable automatic tool selection\ns.tools_json = common_tools::tools::get_all_tools_json();  // Get all registered tools\nconst auto node = std::make_shared&lt;chat_node::ChatNode&lt;std::string, std::string&gt;&gt;(s);\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/#manually-invoking-a-specified-tool","title":"Manually Invoking a Specified Tool","text":"<p>Manually execute a specified tool in the following way:</p> <pre><code>std::string ws_out = common_tools::tools::call_tool(\"search_web2\", ws_in_json.dump());\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/#tool-input-format","title":"Tool Input Format","text":"<p>Tool input parameters strictly follow the OpenAI Chat Completion API's Function Call parameter format, passed as <code>std::string</code> type. The input is a JSON-formatted parameter string:</p> <pre><code>{\n  \"query\": \"Search keywords\",\n  \"time_range\": \"day\",\n  \"country\": \"china\"\n}\n</code></pre> <p>Example Code:</p> <pre><code>nlohmann::json params;\nparams[\"query\"] = \"Latest tech news\";\nparams[\"time_range\"] = \"day\";\nparams[\"country\"] = \"china\";\nstd::string arguments = params.dump();\n\nstd::string result = common_tools::tools::call_tool(\"search_news\", arguments);\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/#tool-output-format","title":"Tool Output Format","text":"<p>Tool output results strictly follow the OpenAI Chat Completion API's Function Call response format, returned as <code>std::string</code> type. The output is in JSON format:</p> <pre><code>{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Tool execution result content\"\n    }\n  ],\n  \"isError\": false\n}\n</code></pre> <p>Field Description: - <code>content</code>: Result content array, each element contains <code>type</code> and <code>text</code> fields - <code>isError</code>: Boolean value indicating whether an error occurred</p> <p>For local tool development sample, please refer to link.</p>"},{"location":"1-2_BasicConceptsToolNode/#optional-call-the-tool-in-custom-code","title":"Optional: Call the Tool in Custom Code**","text":"<p>You can invoke the custom tool\u2019s API directly, as shown below, ref link about Custom Code:</p> <pre><code>\n// \u6784\u5efa\u5de5\u5177\u53c2\u6570 JSON\nnlohmann::json arguments = {\n  {\"name\", \"search_web2\"},\n  {\"arguments\", nlohmann::json{{\"query\", in}}.dump()}\n};\n\nstd::string result = common_tools::tools::call_tool(\"search_news\", arguments.dump());\nnlohmann::json result_json = nlohmann::json::parse(result);\n\nif (!result_json[\"isError\"].get&lt;bool&gt;()) {\n    std::string text = result_json[\"content\"][0][\"text\"].get&lt;std::string&gt;();\n    // Process result\n}\n</code></pre> <p>For local tool development specifications, please refer to <code>Local Tool Development Specifications</code>.</p>"},{"location":"1-3_BasicConceptsCustomNode/","title":"Custom Node","text":""},{"location":"1-3_BasicConceptsCustomNode/#building-a-custom-node","title":"Building a Custom Node","text":"<p>You can build custom processing nodes using <code>CustomNode</code> to implement arbitrary business logic. <code>CustomNode</code> requires a callback function and supports preprocessor and postprocessor for flexible data transformation.</p>"},{"location":"1-3_BasicConceptsCustomNode/#basic-usage","title":"Basic Usage","text":"<pre><code>// Define a callback function: const char* (*)(const char*)\nstatic const char* custom_node_callback(const char* input)\n{\n    if (input == nullptr) {\n        return \"\";\n    }\n    static std::string result;\n    // Custom processing logic\n    result = process_input(input);\n    return result.c_str();\n}\n\n// Create CustomNode instance\nauto custom_node = std::make_shared&lt;custom_node::CustomNode&lt;std::string, std::string&gt;&gt;(\n    custom_node_callback\n);\n\n// Optional: Set preprocessor for input transformation\ncustom_node-&gt;setPreprocessor([](const std::string&amp; input) -&gt; std::string {\n    // Preprocess input before passing to callback\n    return input;\n});\n\n// Optional: Set postprocessor for output transformation\ncustom_node-&gt;setPostprocessor([](const std::string&amp; output) -&gt; std::string {\n    // Postprocess output from callback\n    return output;\n});\n\n// Configration of Routing to next Node. \nroute(custom_node_, [&amp;](const std::string&amp;, const std::string&amp; output) -&gt; std::optional&lt;std::string&gt; {\n        // setting routing value 'polish'\n        return \"next_node\";\n    });\n</code></pre> <p>Notes: - <code>CustomNode&lt;IN, OUT&gt;</code>: Template parameters specify input and output types respectively - Callback function: Must be of type <code>const char* (*)(const char*)</code> and handle the core processing logic - Preprocessor: Optional function to transform input before passing to callback - Postprocessor: Optional function to transform output after callback execution - Type safety: Compile-time checking of input and output type matching</p>"},{"location":"1-4_BasicConceptsCENode/","title":"CE Node","text":"<p>CE Node (Context Engine Node) is a context engine node component for managing and compressing conversation history. It supports two context management strategies: Trimming and Summarizing, which effectively control conversation history length and avoid exceeding model context window limits.</p>"},{"location":"1-4_BasicConceptsCENode/#building-a-ce-node","title":"Building a CE Node","text":"<p>Build a CE node as follows, allowing configuration of context management strategy and related parameters:</p> <pre><code>ce_node::ce_node_settings s_ce;\ns_ce.strategy = ContextStrategy::SUMMARIZING;\ns_ce.context_limit = 3; // Compression trigger threshold, number of history turns\ns_ce.keep_last_n_turns = 1; // Number of recent original message turns to keep\ns_ce.tool_trim_limit = 600; // Tool results are not very important in history messages, so the first 600 characters of tool results will be kept\ns_ce.summarizer_model = \"gpt-4o-mini\";\ns_ce.summarizer_max_tokens = 400;\nconst auto ce_node = std::make_shared&lt;ce_node::CeNode&lt;std::string, std::string&gt;&gt;(s_ce);\n</code></pre>"},{"location":"1-4_BasicConceptsCENode/#context-management-strategies","title":"Context Management Strategies","text":"<p>CE Node supports two context management strategies:</p>"},{"location":"1-4_BasicConceptsCENode/#trimming-strategy","title":"TRIMMING Strategy","text":"<p>The trimming strategy directly deletes history messages that exceed the limit, keeping the most recent <code>max_turns</code> conversation turns.</p> <pre><code>ce_node::ce_node_settings s_ce;\ns_ce.strategy = ContextStrategy::TRIMMING;\ns_ce.max_turns = 3; // Maximum number of history turns to keep\nconst auto ce_node = std::make_shared&lt;ce_node::CeNode&lt;std::string, std::string&gt;&gt;(s_ce);\n</code></pre>"},{"location":"1-4_BasicConceptsCENode/#summarizing-strategy","title":"SUMMARIZING Strategy","text":"<p>The summarizing strategy uses LLM to compress old history messages into summaries, keeping the most recent <code>keep_last_n_turns</code> original message turns. When the number of history turns exceeds <code>context_limit</code>, summarization compression is triggered.</p> <pre><code>ce_node::ce_node_settings s_ce;\ns_ce.strategy = ContextStrategy::SUMMARIZING;\ns_ce.context_limit = 3; // Compression trigger threshold, number of history turns\ns_ce.keep_last_n_turns = 1; // Number of recent original message turns to keep\ns_ce.tool_trim_limit = 600; // Number of characters to keep for tool results\ns_ce.summarizer_model = \"gpt-4o-mini\";\ns_ce.summarizer_max_tokens = 400;\nconst auto ce_node = std::make_shared&lt;ce_node::CeNode&lt;std::string, std::string&gt;&gt;(s_ce);\n</code></pre>"},{"location":"1-4_BasicConceptsCENode/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"1-4_BasicConceptsCENode/#common-parameters","title":"Common Parameters","text":"<ul> <li><code>strategy</code>: Context management strategy, options are <code>ContextStrategy::TRIMMING</code> or <code>ContextStrategy::SUMMARIZING</code>, default is <code>SUMMARIZING</code></li> </ul>"},{"location":"1-4_BasicConceptsCENode/#trimming-strategy-parameters","title":"TRIMMING Strategy Parameters","text":"<ul> <li><code>max_turns</code>: Maximum number of history turns to keep, default is 8</li> </ul>"},{"location":"1-4_BasicConceptsCENode/#summarizing-strategy-parameters","title":"SUMMARIZING Strategy Parameters","text":"<ul> <li><code>context_limit</code>: Compression trigger threshold, triggers summarization compression when history turns exceed this value, default is 5</li> <li><code>keep_last_n_turns</code>: Number of recent original message turns to keep, these messages won't be compressed, default is 2</li> <li><code>tool_trim_limit</code>: Number of characters to keep for tool results in history messages, default is 600</li> <li><code>summarizer_model</code>: Model name for summarization, default is \"gpt-4o-mini\"</li> <li><code>summarizer_max_tokens</code>: Maximum tokens for summarizer model, default is 400</li> <li><code>summarizer_tools_json</code>: Tool configuration for summarizer (JSON string), default is \"[]\"</li> <li><code>summarizer_tool_choice</code>: Tool selection strategy for summarizer, default is \"none\"</li> </ul>"},{"location":"1-4_BasicConceptsCENode/#ce-node-input-and-output","title":"CE Node Input and Output","text":"<p>The default data type is <code>std::string</code> (for custom data types, please refer to <code>Advanced Usage</code>).</p>"},{"location":"1-4_BasicConceptsCENode/#input-format","title":"Input Format","text":"<p>Input data format strictly follows the OpenAI Chat Completion API specification, format as follows:</p> <pre><code>[\n  {\"role\":\"system\",\"content\":\"you are a helpful assistant\"},\n  {\"role\":\"user\",\"content\":\"who are you?\"},\n  {\"role\":\"assistant\",\"content\":\"my name is bob.\"},\n  {\"role\":\"user\",\"content\":\"what can you do?\"}\n]\n</code></pre>"},{"location":"1-4_BasicConceptsCENode/#output-format","title":"Output Format","text":"<p>Output data format also follows the OpenAI Chat Completion API specification. Depending on the configured strategy, output may be:</p> <ul> <li>TRIMMING Strategy: Keeps the most recent <code>max_turns</code> original message turns</li> <li>SUMMARIZING Strategy: Compresses old messages beyond <code>keep_last_n_turns</code> into summaries, keeping the most recent <code>keep_last_n_turns</code> original message turns</li> </ul> <p>Output format example:</p> <pre><code>[\n  {\"role\":\"system\",\"content\":\"you are a helpful assistant\"},\n  {\"role\":\"assistant\",\"content\":\"[Summary of previous conversation]\"},\n  {\"role\":\"user\",\"content\":\"what can you do?\"}\n]\n</code></pre>"},{"location":"1-5_BasicConceptsChainFlow/","title":"Chain","text":"<p>The <code>chain</code> function is used to connect two nodes, creating a data flow path between nodes. An optional <code>action</code> parameter can be specified to create conditional connections.</p> <p>Function Signature:</p> <pre><code>template &lt;typename NodeA, typename NodeB&gt;\nvoid chain(\n    const std::shared_ptr&lt;NodeA&gt;&amp; a,      // Source node\n    const std::shared_ptr&lt;NodeB&gt;&amp; b,      // Target node\n    std::optional&lt;std::string&gt; action = std::nullopt  // Optional action identifier\n);\n</code></pre> <p>How It Works:</p> <ul> <li>When the <code>action</code> parameter is <code>std::nullopt</code>, it creates a default connection (unconditional connection)</li> <li>When the <code>action</code> parameter has a value, it creates a conditional connection, which only executes when the action value returned by the source node's <code>route</code> function matches the action specified in <code>chain</code></li> </ul> <p>Usage Example:</p> <pre><code>// Set routing: determine the returned action based on node output\nroute(decide_node, [](const std::string&amp;, const std::string&amp;) -&gt; std::optional&lt;std::string&gt; {\n    // Return different actions based on business logic\n    return \"search\";\n});\n\n// Create conditional connection: execute this connection when route returns \"search\"\nchain(decide_node, search_node, \"search\");\n\n// Create conditional connection: execute this connection when route returns \"answer\"\nchain(decide_node, answer_node, \"answer\");\n\n// Create default connection: execute unconditionally (used when route returns std::nullopt)\nchain(decide_node, default_node);\n</code></pre> <p>Notes: - The route function is called after the node execution completes - The returned action value must match the action specified in <code>chain</code> - When returning <code>std::nullopt</code>, use the default connection (chain without action parameter)</p>"},{"location":"1-5_BasicConceptsChainFlow/#flow","title":"Flow","text":"<p>Flow is the execution container for workflows, used in conjunction with Chain. It is responsible for managing node execution order and data flow. Through Flow, multiple nodes can be organized into a complete execution flow.</p>"},{"location":"1-5_BasicConceptsChainFlow/#basic-concepts-of-flow","title":"Basic Concepts of Flow","text":"<ul> <li>Workflow Container: Flow is the execution container for nodes, managing node lifecycle and execution order</li> <li>Start Node: Each Flow must specify a start node as the workflow entry point</li> <li>Automatic Execution: Flow automatically executes the workflow based on connections between nodes</li> <li>Type Safety: Flow supports typed input and output, ensuring type safety</li> </ul>"},{"location":"1-5_BasicConceptsChainFlow/#creating-and-executing-workflows","title":"Creating and Executing Workflows","text":""},{"location":"1-5_BasicConceptsChainFlow/#1-create-flow-object","title":"1. Create Flow Object","text":"<p>Use <code>std::make_shared</code> to create a Flow object:</p> <pre><code>auto f = std::make_shared&lt;nodeflow::Flow&gt;();\n</code></pre>"},{"location":"1-5_BasicConceptsChainFlow/#2-set-start-node","title":"2. Set Start Node","text":"<p>Specify the workflow's start node through the <code>start</code> method:</p> <pre><code>f-&gt;start(decide_node);  // decide_node as the workflow entry point\n</code></pre>"},{"location":"1-5_BasicConceptsChainFlow/#3-execute-workflow","title":"3. Execute Workflow","text":"<p>Use the <code>runWithInput</code> method to execute the workflow:</p> <pre><code>auto result = f-&gt;runWithInput&lt;std::string, std::string&gt;(input);\n</code></pre> <p>Function Signature:</p> <pre><code>template &lt;typename IN, typename OUT&gt;\nOUT runWithInput(const IN&amp; input);\n</code></pre> <p>Parameter Description: - Template parameter <code>IN</code>: Input data type (i.e., the first node's input) - Template parameter <code>OUT</code>: Output data type - <code>input</code>: Actual input data</p> <p>Return Value: - The workflow's final output result, type is <code>OUT</code></p>"},{"location":"1-6_BasicConceptsRoute/","title":"Route","text":"<p>The routing mechanism is used to dynamically determine the execution path of a workflow based on node input and output. Through the <code>route</code> function combined with the <code>chain</code> function, conditional branching and loop control can be implemented.</p>"},{"location":"1-6_BasicConceptsRoute/#route-function","title":"Route Function","text":"<p>Each node supports setting routing logic through the <code>route</code> function. The route function is implemented based on Lambda expressions, receiving the node's input and output as parameters, and returning the action identifier for the next step.</p>"},{"location":"1-6_BasicConceptsRoute/#function-signature","title":"Function Signature","text":"<pre><code>template &lt;typename Node, typename Selector&gt;\nvoid route(\n    const std::shared_ptr&lt;Node&gt;&amp; node, \n    Selector selector\n);\n</code></pre> <p>Parameter Description: - <code>node</code>: The node to set routing for - <code>selector</code>: Route selector function, type is <code>std::function&lt;std::optional&lt;std::string&gt;(const IN&amp;, const OUT&amp;)&gt;</code></p> <p>Return Value: - <code>std::optional&lt;std::string&gt;</code>: Returns an action string representing the next execution path, returns <code>std::nullopt</code> to use the default connection</p>"},{"location":"1-6_BasicConceptsRoute/#example-1-routing-based-on-decision-results","title":"Example 1: Routing Based on Decision Results","text":"<pre><code>route(decide_node, [&amp;](const std::string&amp; input, const std::string&amp; output) -&gt; std::optional&lt;std::string&gt; {\n    // Parse decision information from output\n    if (g_yaml_node[\"action\"].as&lt;std::string&gt;() == \"search\") {\n        return \"search\";  // Route to search node\n    }\n    if (g_yaml_node[\"action\"].as&lt;std::string&gt;() == \"answer\") {\n        return \"answer\";  // Route to answer node\n    }\n    return std::nullopt;  // Use default route\n});\n</code></pre>"},{"location":"1-6_BasicConceptsRoute/#example-2-routing-based-on-validation-results","title":"Example 2: Routing Based on Validation Results","text":"<pre><code>route(supervisor_node, [&amp;](const std::string&amp; input, const std::string&amp; output) -&gt; std::optional&lt;std::string&gt; {\n    if (g_validation_result.valid) {\n        return \"done\";   // Validation passed, route to completion node\n    } else {\n        return \"retry\"; // Validation failed, route to retry node\n    }\n});\n</code></pre>"},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/","title":"Single Node Agent Development Example","text":""},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/#overview","title":"Overview","text":"<p>In this example, we will construct only one Chat Node from HADK to materialize an Agent that can understand or invoke functions automatically according to user's query. This is a simple agent implementation that creates a single Chat Node and Flow for each request. ref. link</p>"},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/#project-structure","title":"Project Structure","text":"<p>The <code>single_node</code> project consists of: - Library: <code>single_node</code> (shared library containing <code>koba_agent</code> implementation) - Executable: <code>single_node_app</code> (application layer)</p>"},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/#if-you-need-a-more-advanced-implementation-with-task-management-and-runner-pattern-consider-using-single_node_cls-instead","title":"If you need a more advanced implementation with task management and Runner pattern, consider using <code>single_node_cls</code> instead.","text":""},{"location":"2-2_ApplicationDevelopmentNormalAgent/","title":"Normal Agent Development Example","text":""},{"location":"2-2_ApplicationDevelopmentNormalAgent/#overview","title":"Overview","text":"<p>The normal agent demonstrates how to use HADK's sequential workflow with context compression to implement a three-stage question-answering system. This example implements an agent that compresses conversation context, generates responses using LLM with tool support, and then polishes the final answer. ref. link</p> <p>Workflow Diagram:</p> <pre><code>Input \u2192 ce_node (Context Compression) \u2192 generate_node (Generate Response) \u2192 polish_node (Polish Response) \u2192 Output\n</code></pre> <p>Core Features: - Context compression: Automatically compresses conversation history when it exceeds the limit using SUMMARIZING strategy - Tool support: Generate node can use tools (e.g., search_web2) to gather information - Response polishing: Final polish node refines the generated response for better quality - Sequential workflow: Linear flow through three specialized nodes</p>"},{"location":"2-2_ApplicationDevelopmentNormalAgent/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"2-2_ApplicationDevelopmentNormalAgent/#project-structure","title":"Project Structure","text":"<p>The <code>normal_agent</code> project consists of: - Library: <code>normal_agent</code> (shared library containing <code>koba_agent</code> implementation) - Executable: <code>normal_agent_app</code> (application layer)</p>"},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/","title":"Conditional Routing Agent Development Example","text":""},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/#overview","title":"Overview","text":"<p>The conditional routing agent demonstrates how to use HADK's conditional routing (<code>route</code>) functionality to implement complex workflows. This example implements a research assistant agent that can dynamically decide whether to continue searching for information or directly answer questions based on the current context, supporting iterative searches until sufficient information is obtained. ref. link</p> <p>Workflow Diagram:</p> <pre><code>Input \u2192 decide_node (Decision) \u2192 [Conditional Routing]\n                                  \u251c\u2500 \"search\" \u2192 web_search_node (Web Search) \u2192 decide_node (Loop)\n                                  \u2514\u2500 \"answer\" \u2192 answer_node (Generate Answer) \u2192 Output\n</code></pre> <p>Core Features: - Conditional routing: Dynamically select the next node based on the decision node's output - Loop workflow: Supports looping between decision node and search node until sufficient information is obtained - Context accumulation: Each search result accumulates into the context for subsequent decisions</p>"},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/#project-structure","title":"Project Structure","text":"<p>The <code>reflector_agent</code> project consists of: - Library: <code>reflector_agent</code> (shared library containing <code>koba_agent</code> implementation) - Executable: <code>reflector_agent_app</code> (application layer)</p>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/","title":"Inja Template Formatting Tutorial","text":""},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#overview","title":"Overview","text":"<p>Inja is a powerful C++ template engine. The HADK framework provides convenient template formatting functionality through the <code>ChatUtils::format_inja</code> function. Using Inja templates, you can easily insert variable values into template strings to achieve dynamic content generation.</p>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#basic-usage","title":"Basic Usage","text":""},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#function-signature","title":"Function Signature","text":"<pre><code>std::string ChatUtils::format_inja(\n    const std::string&amp; template_str,\n    const std::unordered_map&lt;std::string, std::any&gt;&amp; variables\n);\n</code></pre> <p>Parameter Description: - <code>template_str</code>: Template string using <code>{{variable_name}}</code> syntax to define placeholders - <code>variables</code>: Variable map, where keys are variable names and values are corresponding variable values (supports multiple types)</p> <p>Return Value: - Formatted string</p>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#template-syntax","title":"Template Syntax","text":"<p>Inja templates use double curly braces <code>{{variable_name}}</code> to define placeholders. The template engine replaces placeholders with corresponding variable values.</p>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#examples","title":"Examples","text":""},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#example-1-simple-context-formatting","title":"Example 1: Simple Context Formatting","text":"<p>This is the most basic usage, demonstrating how to use string variables for template replacement:</p> <pre><code>#include &lt;chat_utils.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;unordered_map&gt;\n#include &lt;any&gt;\n\nstd::string simple_template = R\"(\nHello, {{name}}!\n\nToday is a {{weather}} {{time}}.\n\nBased on your {{topic}}, I've prepared the following content for you:\n\n{{content}}\n\nI hope this information is helpful!\n)\";\n\nstd::unordered_map&lt;std::string, std::any&gt; simple_variables;\nsimple_variables[\"name\"] = std::string(\"Xiao Ming\");\nsimple_variables[\"weather\"] = std::string(\"sunny\");\nsimple_variables[\"time\"] = std::string(\"morning\");\nsimple_variables[\"topic\"] = std::string(\"study plan\");\nsimple_variables[\"content\"] = std::string(\"1. Complete math homework\\n2. Read English articles\\n3. Review history knowledge\");\n\ntry {\n    std::string formatted_simple = ChatUtils::format_inja(simple_template, simple_variables);\n    std::cout &lt;&lt; formatted_simple &lt;&lt; \"\\n\";\n} catch (const std::exception&amp; e) {\n    std::cerr &lt;&lt; \"Formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n}\n</code></pre> <p>Output Result:</p> <pre><code>Hello, Xiao Ming!\n\nToday is a sunny morning.\n\nBased on your study plan, I've prepared the following content for you:\n\n1. Complete math homework\n2. Read English articles\n3. Review history knowledge\n\nI hope this information is helpful!\n</code></pre>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#example-2-summary-formatting-reference-deepsearchcpp","title":"Example 2: Summary Formatting (Reference: deepsearch.cpp)","text":"<p>This example demonstrates how to build complex prompt templates, suitable for multi-iteration summary scenarios:</p> <pre><code>std::string summary_template = R\"(\nTask\n\nYou need to summarize information around a theme. Theme: `{{topic}}`\n\nWorkflow\n\n1. Carefully read all information, combine with the theme to understand and fully comprehend the context.\n\n2. Select content related to the theme and summarize the selected content.\n\n3. If the theme is question-based, you need to summarize and infer relevant answers, otherwise summarize normally based on the theme.\n\nRequirements\n\n- The word count must not be less than {{min_words}} words, must be as many as possible.\n\n- The summarized content must be from the information provided, you cannot make things up, especially time-related information.\n\n- The summarized content must be comprehensive enough.\n\n- Logical coherence, smooth sentences.\n\n- Provide the final result directly in markdown format.\n\nInformation to Summarize\n\n```{{summary_search}}```\n)\";\n\nstd::unordered_map&lt;std::string, std::any&gt; summary_variables;\nsummary_variables[\"topic\"] = std::string(\"History of Artificial Intelligence Development\");\nsummary_variables[\"min_words\"] = 3500;\n\n// Simulate multi-iteration summary content\nstd::vector&lt;std::string&gt; all_iteration_summary = {\n    \"First search: The concept of artificial intelligence was first proposed by John McCarthy in 1956.\",\n    \"Second search: Deep learning technology made breakthrough progress in the 2010s.\",\n    \"Third search: Large language models such as the GPT series attracted widespread attention in the 2020s.\"\n};\n\n// Connect multiple summaries with separators\nstd::string all_iteration_summary_str;\nfor (size_t i = 0; i &lt; all_iteration_summary.size(); ++i) {\n    all_iteration_summary_str += all_iteration_summary[i];\n    if (i &lt; all_iteration_summary.size() - 1) {\n        all_iteration_summary_str += \"\\n---\\n\";\n    }\n}\nsummary_variables[\"summary_search\"] = all_iteration_summary_str;\n\ntry {\n    std::string formatted_summary = ChatUtils::format_inja(summary_template, summary_variables);\n    std::cout &lt;&lt; formatted_summary &lt;&lt; \"\\n\";\n} catch (const std::exception&amp; e) {\n    std::cerr &lt;&lt; \"Formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n}\n</code></pre> <p>Output Result:</p> <pre><code>Task\n\nYou need to summarize information around a theme. Theme: `History of Artificial Intelligence Development`\n\nWorkflow\n\n1. Carefully read all information, combine with the theme to understand and fully comprehend the context.\n\n2. Select content related to the theme and summarize the selected content.\n\n3. If the theme is question-based, you need to summarize and infer relevant answers, otherwise summarize normally based on the theme.\n\nRequirements\n\n- The word count must not be less than 3500 words, must be as many as possible.\n\n- The summarized content must be from the information provided, you cannot make things up, especially time-related information.\n\n- The summarized content must be comprehensive enough.\n\n- Logical coherence, smooth sentences.\n\n- Provide the final result directly in markdown format.\n\nInformation to Summarize\n\n```First search: The concept of artificial intelligence was first proposed by John McCarthy in 1956.\n---\nSecond search: Deep learning technology made breakthrough progress in the 2010s.\n---\nThird search: Large language models such as the GPT series attracted widespread attention in the 2020s.```\n</code></pre>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#example-3-using-different-types-of-variables","title":"Example 3: Using Different Types of Variables","text":"<p>Inja templates support variables of multiple data types, including strings, integers, floating-point numbers, booleans, and container types:</p> <pre><code>std::string mixed_template = R\"(\nUser Information:\n\n- Name: {{name}}\n- Age: {{age}}\n- VIP Status: {{is_vip}}\n- Points: {{points}}\n- Tags: {{tags}}\n)\";\n\nstd::unordered_map&lt;std::string, std::any&gt; mixed_variables;\nmixed_variables[\"name\"] = std::string(\"Zhang San\");\nmixed_variables[\"age\"] = 28;\nmixed_variables[\"is_vip\"] = true;\nmixed_variables[\"points\"] = 1250.5;\n\nstd::vector&lt;std::string&gt; tags = {\"Active User\", \"Tech Enthusiast\", \"Early User\"};\nmixed_variables[\"tags\"] = tags;\n\ntry {\n    std::string formatted_mixed = ChatUtils::format_inja(mixed_template, mixed_variables);\n    std::cout &lt;&lt; formatted_mixed &lt;&lt; \"\\n\";\n} catch (const std::exception&amp; e) {\n    std::cerr &lt;&lt; \"Formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n}\n</code></pre> <p>Output Result:</p> <pre><code>User Information:\n\n- Name: Zhang San\n- Age: 28\n- VIP Status: true\n- Points: 1250.5\n- Tags: Active User, Tech Enthusiast, Early User\n</code></pre>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#complete-example-program","title":"Complete Example Program","text":"<p>The following is a complete example program demonstrating all three examples:</p> <pre><code>#include &lt;chat_utils.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;unordered_map&gt;\n#include &lt;any&gt;\n\nint main()\n{\n    std::cout &lt;&lt; \"=== Inja Template Formatting Examples === \\n\";\n\n    // Example 1: Simple context formatting\n    std::cout &lt;&lt; \"\u3010Example 1\u3011Simple Context Formatting\\n\";\n    std::string simple_template = R\"(\nHello, {{name}}!\n\nToday is a {{weather}} {{time}}.\n\nBased on your {{topic}}, I've prepared the following content for you:\n\n{{content}}\n\nI hope this information is helpful!\n\n)\";\n\n    std::unordered_map&lt;std::string, std::any&gt; simple_variables;\n    simple_variables[\"name\"] = std::string(\"Xiao Ming\");\n    simple_variables[\"weather\"] = std::string(\"sunny\");\n    simple_variables[\"time\"] = std::string(\"morning\");\n    simple_variables[\"topic\"] = std::string(\"study plan\");\n    simple_variables[\"content\"] = std::string(\"1. Complete math homework\\n2. Read English articles\\n3. Review history knowledge\");\n\n    try {\n        std::string formatted_simple = ChatUtils::format_inja(simple_template, simple_variables);\n        std::cout &lt;&lt; formatted_simple &lt;&lt; \"\\n\";\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"Formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n    }\n\n    std::cout &lt;&lt; \"\\n\" &lt;&lt; std::string(50, '-') &lt;&lt; \"\\n\";\n\n    // Example 2: Summary formatting similar to deepsearch.cpp\n    std::cout &lt;&lt; \"\u3010Example 2\u3011Summary Formatting (Reference: deepsearch.cpp)\\n\";\n    std::string summary_template = R\"(\nTask\n\nYou need to summarize information around a theme. Theme: `{{topic}}`\n\nWorkflow\n\n1. Carefully read all information, combine with the theme to understand and fully comprehend the context.\n\n2. Select content related to the theme and summarize the selected content.\n\n3. If the theme is question-based, you need to summarize and infer relevant answers, otherwise summarize normally based on the theme.\n\nRequirements\n\n- The word count must not be less than {{min_words}} words, must be as many as possible.\n\n- The summarized content must be from the information provided, you cannot make things up, especially time-related information.\n\n- The summarized content must be comprehensive enough.\n\n- Logical coherence, smooth sentences.\n\n- Provide the final result directly in markdown format.\n\nInformation to Summarize\n\n```{{summary_search}}```\n\n)\";\n\n    std::unordered_map&lt;std::string, std::any&gt; summary_variables;\n    summary_variables[\"topic\"] = std::string(\"History of Artificial Intelligence Development\");\n    summary_variables[\"min_words\"] = 3500;\n\n    // Simulate multi-iteration summary content\n    std::vector&lt;std::string&gt; all_iteration_summary = {\n        \"First search: The concept of artificial intelligence was first proposed by John McCarthy in 1956.\",\n        \"Second search: Deep learning technology made breakthrough progress in the 2010s.\",\n        \"Third search: Large language models such as the GPT series attracted widespread attention in the 2020s.\"\n    };\n\n    // Connect multiple summaries with separators\n    std::string all_iteration_summary_str;\n    for (size_t i = 0; i &lt; all_iteration_summary.size(); ++i) {\n        all_iteration_summary_str += all_iteration_summary[i];\n        if (i &lt; all_iteration_summary.size() - 1) {\n            all_iteration_summary_str += \"\\n---\\n\";\n        }\n    }\n    summary_variables[\"summary_search\"] = all_iteration_summary_str;\n\n    try {\n        std::string formatted_summary = ChatUtils::format_inja(summary_template, summary_variables);\n        std::cout &lt;&lt; formatted_summary &lt;&lt; \"\\n\";\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"Formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n    }\n\n    std::cout &lt;&lt; \"\\n\" &lt;&lt; std::string(50, '-') &lt;&lt; \"\\n\";\n\n    // Example 3: Using different types of variables\n    std::cout &lt;&lt; \"\u3010Example 3\u3011Using Different Types of Variables\\n\";\n    std::string mixed_template = R\"(\nUser Information:\n\n- Name: {{name}}\n- Age: {{age}}\n- VIP Status: {{is_vip}}\n- Points: {{points}}\n- Tags: {{tags}}\n\n)\";\n\n    std::unordered_map&lt;std::string, std::any&gt; mixed_variables;\n    mixed_variables[\"name\"] = std::string(\"Zhang San\");\n    mixed_variables[\"age\"] = 28;\n    mixed_variables[\"is_vip\"] = true;\n    mixed_variables[\"points\"] = 1250.5;\n\n    std::vector&lt;std::string&gt; tags = {\"Active User\", \"Tech Enthusiast\", \"Early User\"};\n    mixed_variables[\"tags\"] = tags;\n\n    try {\n        std::string formatted_mixed = ChatUtils::format_inja(mixed_template, mixed_variables);\n        std::cout &lt;&lt; formatted_mixed &lt;&lt; \"\\n\";\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"Formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n    }\n\n    std::cout &lt;&lt; \"\\n=== Examples Complete ===\\n\";\n\n    return 0;\n}\n</code></pre>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#supported-variable-types","title":"Supported Variable Types","text":"<p>The <code>ChatUtils::format_inja</code> function supports the following variable types:</p> <ul> <li>String (<code>std::string</code>): The most commonly used type, directly replaced in the template</li> <li>Integer (<code>int</code>, <code>long</code>, <code>long long</code>, etc.): Automatically converted to string</li> <li>Floating-point (<code>float</code>, <code>double</code>): Automatically converted to string</li> <li>Boolean (<code>bool</code>): Converted to \"true\" or \"false\"</li> <li>Container Types (<code>std::vector&lt;T&gt;</code>): Automatically converted to comma-separated string</li> </ul>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#error-handling","title":"Error Handling","text":"<p>When template formatting fails, the <code>format_inja</code> function throws a <code>std::exception</code> exception. It's recommended to use <code>try-catch</code> blocks to catch exceptions and perform appropriate error handling:</p> <pre><code>try {\n    std::string result = ChatUtils::format_inja(template_str, variables);\n    // Use formatted result\n} catch (const std::exception&amp; e) {\n    std::cerr &lt;&lt; \"Template formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n    // Handle error situation\n}\n</code></pre>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use Raw String Literals: Use <code>R\"(...)\"</code> syntax to define template strings, which preserves newlines and special characters, improving readability</p> </li> <li> <p>Variable Naming Conventions: Use meaningful variable names for easier understanding and maintenance</p> </li> <li> <p>Error Handling: Always use <code>try-catch</code> blocks to catch possible exceptions</p> </li> <li> <p>Template Reuse: Define commonly used templates as constants or functions for easy reuse</p> </li> <li> <p>Type Safety: Ensure variable types match the usage scenarios in templates</p> </li> </ol>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#application-scenarios","title":"Application Scenarios","text":"<p>Inja template formatting is commonly used in the HADK framework for the following scenarios:</p> <ul> <li>Prompt Construction: Dynamically build LLM prompts, generating different prompts based on context and task requirements</li> <li>Message Formatting: Format user messages, system messages, etc.</li> <li>Summary Generation: Build multi-iteration summary prompts</li> <li>Tool Invocation Parameters: Dynamically generate parameter descriptions for tool invocations</li> </ul>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#reference-resources","title":"Reference Resources","text":"<ul> <li>Inja Official Documentation</li> <li>HADK Framework Documentation</li> </ul>"},{"location":"2-5_ApplicationDevelopmentCoTAgent/","title":"CoT Agent Development Example","text":""},{"location":"2-5_ApplicationDevelopmentCoTAgent/#overview","title":"Overview","text":"<p>This example demonstrates how to use CoT (Chain of Thought) nodes from HADK to create an Agent that can solve complex problems through step-by-step reasoning. ref. link</p> <p>Workflow Diagram:</p> <pre><code>Input \u2192 extra_node (Extract Problem) \u2192 cot_node (Chain of Thought, Loop) \u2192 polish_node (Polish Answer) \u2192 Output\n</code></pre> <p>Core Features: - Chain of Thought reasoning: Uses iterative step-by-step reasoning to solve complex problems - Loop workflow: The cot_node loops until it reaches a conclusion, then routes to polish_node - Problem extraction: Extracts and clarifies the problem before reasoning - Answer polishing: Refines the final answer for better quality</p>"},{"location":"2-5_ApplicationDevelopmentCoTAgent/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"2-5_ApplicationDevelopmentCoTAgent/#project-structure","title":"Project Structure","text":"<p>The <code>cot_agent</code> project consists of: - Library: <code>cot_agent</code> (shared library containing <code>koba_agent</code> implementation) - Executable: <code>cot_agent_app</code> (application layer)</p>"},{"location":"2-6_ApplicationDevelopmentBatchNode/","title":"Batch Node Agent Development Example","text":""},{"location":"2-6_ApplicationDevelopmentBatchNode/#overview","title":"Overview","text":"<p>This example demonstrates how to use HADK <code>BatchFuncNode</code> to build a simple batch processing flow, where multiple text inputs are generated, processed in batch, and then routed to the next node. ref. link</p> <p>Workflow Diagram:</p> <pre><code>Input \u2192 create_node (Generate Batch) \u2192 batch_node (Batch Processing) \u2192 summarize_node (Summarize) \u2192 Output\n</code></pre> <p>Core Features: - Batch processing: Uses <code>BatchFuncNode</code> to process multiple inputs in parallel - Vector transformation: Converts single input to vector of strings for batch processing - Sequential workflow: Linear flow through create, batch, and summarize nodes - Routing support: Uses routing values to connect nodes in the flow</p>"},{"location":"2-6_ApplicationDevelopmentBatchNode/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"2-6_ApplicationDevelopmentBatchNode/#project-structure","title":"Project Structure","text":"<p>The <code>batch_flow</code> project consists of: - Library: <code>batch_flow</code> (shared library containing <code>koba_agent</code> implementation) - Executable: <code>batch_flow_app</code> (application layer)</p>"},{"location":"2-7_ApplicationChatBot/","title":"Chat Bot Development Example","text":""},{"location":"2-7_ApplicationChatBot/#overview","title":"Overview","text":"<p>This example demonstrates how to construct a simple interactive chat bot based on Single Node Agent. The chat bot supports multi-turn conversations with automatic history management using the Runner pattern. ref. link</p> <p>Workflow Diagram:</p> <pre><code>User Input \u2192 Chat History \u2192 koba_agent (Single Node) \u2192 Update History \u2192 Display Response \u2192 Loop\n</code></pre> <p>Core Features: - Multi-turn conversations: Maintains full conversation context across turns - Automatic history management: Tracks and updates conversation history automatically - Runner pattern: Uses Runner framework for task management - Interactive console: Reads user input and displays responses in real-time</p>"},{"location":"2-7_ApplicationChatBot/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"2-7_ApplicationChatBot/#project-structure","title":"Project Structure","text":"<p>The <code>chat_bot</code> project consists of: - Library: <code>chat_bot</code> (shared library containing <code>koba_agent</code> implementation) - Executable: <code>loop_chat_rt_app</code> (interactive chat bot application)</p>"},{"location":"3-1_AndroidPlatform/","title":"Android Platform Development","text":""},{"location":"3-1_AndroidPlatform/#overview","title":"Overview","text":"<p>HADK is a cross-platform development framework. This document provides complete examples for compiling and running agents on the Android platform, including cross-compilation configuration and Android Studio project integration. </p>"},{"location":"3-1_AndroidPlatform/#cross-compilation-reference","title":"Cross-compilation Reference","text":"<p>For complete cross-compilation examples and configurations, please refer to: Cross-Compilation Agent Example</p>"},{"location":"3-1_AndroidPlatform/#agent-development","title":"Agent Development","text":"<p>For the agent implementation used, please refer to: Single Node Agent Development Guide</p>"},{"location":"3-1_AndroidPlatform/#android-studio-integration","title":"Android Studio Integration","text":"<p>We provide a complete Android Studio example project that demonstrates how to call cross-compiled agents through <code>Kotlin + JNI</code> and execute  <code>local tool calls</code> to implement a simple chatbot.</p> <p>For the complete Android Studio example project, please refer to: Android Demo Project</p>"},{"location":"3-1_AndroidPlatform/#local-tools","title":"Local Tools","text":"<p>This demo provides the following local tools that can be called through function call:</p> <ul> <li>Control device Bluetooth on/off</li> <li>Dynamically adjust device screen brightness</li> <li>Control device flashlight on/off</li> <li>Control device volume</li> <li>Screen capture</li> </ul>"},{"location":"4-1_RunnerSingleNodeAgent/","title":"Single Node Agent Development With Runner (Class-based with Runner)","text":""},{"location":"4-1_RunnerSingleNodeAgent/#overview","title":"Overview","text":"<p>This module is a class-based refactoring of single_node, converting the original function-based implementation into an object-oriented class format using the Runner pattern. ref. link</p> <p>In this example, we construct a single Chat Node from HADK to materialize an Agent that can understand or invoke functions automatically according to user's query. This example implements an agent that generates responses using LLM with tool support in a single node workflow.</p> <p>Workflow Diagram:</p> <pre><code>Input \u2192 generate_node (Generate Response) \u2192 Output\n</code></pre> <p>The key difference from <code>single_node</code> is that this implementation uses:</p> <ul> <li>Class-based architecture: The agent logic is encapsulated in a <code>KobaAgentTask</code> class that inherits from <code>hybrid_runner::task_base</code></li> <li>Runner pattern: Uses the Runner framework (<code>runner_init</code>, <code>runner_run</code>, <code>runner_release</code>) for task management and execution</li> <li>Structured history format: Supports the new conversation history format <code>[[S],[U,A],[U,A,T,A]]</code> where each inner array represents a complete conversation turn</li> </ul>"},{"location":"4-1_RunnerSingleNodeAgent/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"4-1_RunnerSingleNodeAgent/#tool-initialization","title":"Tool Initialization","text":"<p>Before using the agent, you need to initialize the tools.</p> <p>Available Tools: - <code>search_web2</code>: Web search tool that supports filtering by country (enum: \"china\", \"germany\", \"italy\", \"united kingdom\", \"united states\")</p>"},{"location":"4-1_RunnerSingleNodeAgent/#c-api-interface","title":"C API Interface","text":"<p>The module provides a C API interface for easy integration:</p> <ul> <li> <p><code>koba_agent_init()</code>: Initialize the agent and return a key for subsequent operations   <code>cpp   const char* key = koba_agent_init();   if (!key) {       // handle initialization error   }</code></p> </li> <li> <p><code>get_koba_agent_result(key, history, input)</code>: Execute the agent with conversation history and current query   <code>cpp   const char* response = get_koba_agent_result(key, history.dump().c_str(), \"what is the weather in beijing ?\");</code></p> </li> <li><code>key</code>: The key returned from <code>koba_agent_init()</code></li> <li><code>history</code>: The conversation history in JSON format <code>[[S],[U,A],[U,A,T,A]]</code></li> <li> <p><code>input</code>: The current user query string</p> </li> <li> <p><code>koba_agent_cleanup(key)</code>: Clean up and release the agent instance   <code>cpp   koba_agent_cleanup(key);</code></p> </li> </ul> <p>Note: These C API functions internally use the Runner framework (<code>runner_init</code>, <code>runner_run</code>, <code>runner_release</code>) for task lifecycle management.</p>"},{"location":"4-1_RunnerSingleNodeAgent/#conversation-history-format","title":"Conversation History Format","text":"<p>The new structured history format groups messages by conversation turns:</p> <ul> <li><code>[[S]]</code>: System message as the first group</li> <li><code>[[S],[U,A]]</code>: System message followed by a user-assistant turn</li> <li><code>[[S],[U,A],[U,A,T,A]]</code>: System message followed by multiple turns, including tool calls</li> </ul> <p>Each inner array represents a complete conversation turn from start to finish.</p>"},{"location":"4-2_RunnerNormalAgent/","title":"Normal Agent Development With Runner (Class-based with Runner)","text":""},{"location":"4-2_RunnerNormalAgent/#overview","title":"Overview","text":"<p>This module is a class-based refactoring of normal_agent, converting the original function-based implementation into an object-oriented class format using the Runner pattern. ref. link</p> <p>The normal agent demonstrates how to use HADK's sequential workflow to implement a two-stage question-answering system. This example implements an agent that generates responses using LLM with tool support, and then polishes the final answer.</p> <p>Workflow Diagram:</p> <pre><code>Input \u2192 generate_node (Generate Response) \u2192 polish_node (Polish Response) \u2192 Output\n</code></pre> <p>The key difference from <code>normal_agent</code> is that this implementation uses:</p> <ul> <li>Class-based architecture: The agent logic is encapsulated in a <code>KobaAgentTask</code> class that inherits from <code>hybrid_runner::task_base</code></li> <li>Runner pattern: Uses the Runner framework (<code>runner_init</code>, <code>runner_run</code>, <code>runner_release</code>) for task management and execution</li> <li>Structured history format: Supports the new conversation history format <code>[[S],[U,A],[U,A,T,A]]</code> where each inner array represents a complete conversation turn</li> </ul>"},{"location":"4-2_RunnerNormalAgent/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"4-2_RunnerNormalAgent/#tool-initialization","title":"Tool Initialization","text":"<p>Before using the agent, you need to initialize the tools.</p> <p>Available Tools: - <code>search_web2</code>: Web search tool that supports filtering by country (enum: \"china\", \"germany\", \"italy\", \"united kingdom\", \"united states\")</p>"},{"location":"4-2_RunnerNormalAgent/#c-api-interface","title":"C API Interface","text":"<p>The module provides a C API interface for easy integration:</p> <ul> <li> <p><code>koba_agent_init()</code>: Initialize the agent and return a key for subsequent operations   <code>cpp   const char* key = koba_agent_init();   if (!key) {       // handle initialization error   }</code></p> </li> <li> <p><code>get_koba_agent_result(key, history, input)</code>: Execute the agent with conversation history and current query   <code>cpp   const char* response = get_koba_agent_result(key, history.dump().c_str(), \"using the tool, tell me some news about MLB .\");</code></p> </li> <li><code>key</code>: The key returned from <code>koba_agent_init()</code></li> <li><code>history</code>: The conversation history in JSON format <code>[[S],[U,A],[U,A,T,A]]</code></li> <li> <p><code>input</code>: The current user query string</p> </li> <li> <p><code>koba_agent_cleanup(key)</code>: Clean up and release the agent instance   <code>cpp   koba_agent_cleanup(key);</code></p> </li> </ul> <p>Note: These C API functions internally use the Runner framework (<code>runner_init</code>, <code>runner_run</code>, <code>runner_release</code>) for task lifecycle management.</p>"},{"location":"4-2_RunnerNormalAgent/#conversation-history-format","title":"Conversation History Format","text":"<p>The new structured history format groups messages by conversation turns:</p> <ul> <li><code>[[S]]</code>: System message as the first group</li> <li><code>[[S],[U,A]]</code>: System message followed by a user-assistant turn</li> <li><code>[[S],[U,A],[U,A,T,A]]</code>: System message followed by multiple turns, including tool calls</li> </ul> <p>Each inner array represents a complete conversation turn from start to finish.</p>"},{"location":"4-3_RunnerThreeNodeAgent/","title":"Conditional Routing Agent Development With Runner (Class-based with Runner)","text":""},{"location":"4-3_RunnerThreeNodeAgent/#overview","title":"Overview","text":"<p>This module is a class-based refactoring of reflector_agent, converting the original function-based implementation into an object-oriented class format using the Runner pattern. ref. link</p> <p>The conditional routing agent demonstrates how to use HADK's conditional routing (<code>route</code>) functionality to implement complex workflows. This example implements a research assistant agent that can dynamically decide whether to continue searching for information or directly answer questions based on the current context, supporting iterative searches until sufficient information is obtained.</p> <p>Workflow Diagram:</p> <pre><code>Input \u2192 decide_node (Decision) \u2192 [Conditional Routing]\n                                  \u251c\u2500 \"search\" \u2192 web_search_node (Web Search) \u2192 decide_node (Loop)\n                                  \u2514\u2500 \"answer\" \u2192 answer_node (Generate Answer) \u2192 Output\n</code></pre> <p>The key difference from <code>reflector_agent</code> is that this implementation uses:</p> <ul> <li>Class-based architecture: The agent logic is encapsulated in a <code>KobaAgentTask</code> class that inherits from <code>hybrid_runner::task_base</code></li> <li>Runner pattern: Uses the Runner framework (<code>runner_init</code>, <code>runner_run</code>, <code>runner_release</code>) for task management and execution</li> <li>Structured history format: Supports the new conversation history format <code>[[S],[U,A],[U,A,T,A]]</code> where each inner array represents a complete conversation turn</li> </ul>"},{"location":"4-3_RunnerThreeNodeAgent/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"4-3_RunnerThreeNodeAgent/#tool-initialization","title":"Tool Initialization","text":"<p>Before using the agent, you need to initialize the tools.</p> <p>Available Tools: - <code>search_web2</code>: Web search tool that supports filtering by country (enum: \"china\", \"germany\", \"italy\", \"united kingdom\", \"united states\")</p>"},{"location":"4-3_RunnerThreeNodeAgent/#c-api-interface","title":"C API Interface","text":"<p>The module provides a C API interface for easy integration:</p> <ul> <li> <p><code>koba_agent_init()</code>: Initialize the agent and return a key for subsequent operations   <code>cpp   const char* key = koba_agent_init();   if (!key) {       // handle initialization error   }</code></p> </li> <li> <p><code>get_koba_agent_result(key, history, input)</code>: Execute the agent with conversation history and current query   <code>cpp   const char* response = get_koba_agent_result(key, history.dump().c_str(), \"using the tool, tell me some news about MLB .\");</code></p> </li> <li><code>key</code>: The key returned from <code>koba_agent_init()</code></li> <li><code>history</code>: The conversation history in JSON format <code>[[S],[U,A],[U,A,T,A]]</code></li> <li> <p><code>input</code>: The current user query string</p> </li> <li> <p><code>koba_agent_cleanup(key)</code>: Clean up and release the agent instance   <code>cpp   koba_agent_cleanup(key);</code></p> </li> </ul> <p>Note: These C API functions internally use the Runner framework (<code>runner_init</code>, <code>runner_run</code>, <code>runner_release</code>) for task lifecycle management.</p>"},{"location":"4-3_RunnerThreeNodeAgent/#conversation-history-format","title":"Conversation History Format","text":"<p>The new structured history format groups messages by conversation turns:</p> <ul> <li><code>[[S]]</code>: System message as the first group</li> <li><code>[[S],[U,A]]</code>: System message followed by a user-assistant turn</li> <li><code>[[S],[U,A],[U,A,T,A]]</code>: System message followed by multiple turns, including tool calls</li> </ul> <p>Each inner array represents a complete conversation turn from start to finish.</p>"},{"location":"4_RunnerIntroduction/","title":"Runner Framework Introduction","text":""},{"location":"4_RunnerIntroduction/#overview","title":"Overview","text":"<p>Runner is a lightweight task management framework for managing and executing Agent tasks in HADK. It provides a unified task lifecycle management interface with safe cross-DLL boundary calls.</p> <p>Core Features: - ABI Safe: Uses pure C interfaces and pure virtual interfaces to avoid ABI compatibility issues across DLL boundaries - Simplified Registration: Automatically registers task classes through the <code>REGISTER_TASK</code> macro - Lifecycle Management: Provides complete task initialization, execution, and cleanup flow - Structured History: Supports structured conversation history format <code>[[S],[U,A],[U,A,T,A]]</code></p>"},{"location":"4_RunnerIntroduction/#core-concepts","title":"Core Concepts","text":""},{"location":"4_RunnerIntroduction/#task-class","title":"Task Class","text":"<p>Task classes need to inherit from <code>hybrid_runner::task_base</code> and implement the <code>run()</code> method:</p> <pre><code>class MyTask : public hybrid_runner::task_base\n{\nprotected:\n    std::string run(const char* input) override\n    {\n        // Process input and return result\n        return \"result\";\n    }\n};\n</code></pre>"},{"location":"4_RunnerIntroduction/#task-registration","title":"Task Registration","text":"<p>Use the <code>REGISTER_TASK</code> macro to register task classes:</p> <pre><code>namespace\n{\n    class MyTask : public hybrid_runner::task_base\n    {\n        // ... implementation\n    };\n\n    REGISTER_TASK(\"MyTask\", MyTask);\n}\n</code></pre>"},{"location":"4_RunnerIntroduction/#c-api-interface","title":"C API Interface","text":"<p>Runner provides pure C interfaces for task management:</p> <ul> <li><code>runner_init(class_name, key)</code>: Initialize a task instance and return a key</li> <li><code>runner_run(key, history, input)</code>: Execute the task and return the result</li> <li><code>runner_release(key)</code>: Release the task instance</li> </ul>"},{"location":"4_RunnerIntroduction/#api-reference","title":"API Reference","text":""},{"location":"4_RunnerIntroduction/#c-api-functions","title":"C API Functions","text":""},{"location":"4_RunnerIntroduction/#runner_init","title":"<code>runner_init</code>","text":"<pre><code>const char* runner_init(const char* class_name, const char* key);\n</code></pre> <p>Initialize a task instance.</p> <ul> <li>Parameters:</li> <li><code>class_name</code>: Task class name (the name used during registration)</li> <li><code>key</code>: Optional, if provided, uses the specified key; otherwise, automatically generates one</li> <li>Returns: The key of the task instance, returns <code>nullptr</code> on failure</li> </ul>"},{"location":"4_RunnerIntroduction/#runner_run","title":"<code>runner_run</code>","text":"<pre><code>const char* runner_run(const char* key, const char* history, const char* input);\n</code></pre> <p>Execute the task.</p> <ul> <li>Parameters:</li> <li><code>key</code>: The key of the task instance (returned by <code>runner_init</code>)</li> <li><code>history</code>: Conversation history in JSON format <code>[[S],[U,A],[U,A,T,A]]</code></li> <li><code>input</code>: Current user input</li> <li>Returns: Task execution result (string pointer)</li> </ul>"},{"location":"4_RunnerIntroduction/#runner_release","title":"<code>runner_release</code>","text":"<pre><code>void runner_release(const char* key);\n</code></pre> <p>Release the task instance.</p> <ul> <li>Parameters:</li> <li><code>key</code>: The key of the task instance</li> </ul>"},{"location":"4_RunnerIntroduction/#c-helper-classes","title":"C++ Helper Classes","text":""},{"location":"4_RunnerIntroduction/#hybrid_runnertask_base","title":"<code>hybrid_runner::task_base</code>","text":"<p>Task base class that simplifies task implementation:</p> <pre><code>namespace hybrid_runner\n{\n    class task_base : public i_task\n    {\n    protected:\n        // Users only need to implement this method\n        virtual std::string run(const char* input) = 0;\n    };\n}\n</code></pre>"},{"location":"4_RunnerIntroduction/#register_task-macro","title":"<code>REGISTER_TASK</code> Macro","text":"<p>Macro that simplifies task registration:</p> <pre><code>#define REGISTER_TASK(NAME, TYPE) \\\n    static hybrid_runner::registrar&lt;TYPE&gt; registrar_##TYPE(NAME);\n</code></pre>"},{"location":"4_RunnerIntroduction/#conversation-history-format","title":"Conversation History Format","text":"<p>Runner supports a structured conversation history format, where each inner array represents a complete conversation turn:</p> <ul> <li><code>[[S]]</code>: System message</li> <li><code>[[S],[U,A]]</code>: System message + user-assistant turn</li> <li><code>[[S],[U,A],[U,A,T,A]]</code>: System message + multiple turns, including tool calls</li> </ul> <p>Where: - <code>S</code>: System message - <code>U</code>: User message - <code>A</code>: Assistant message - <code>T</code>: Tool call</p>"},{"location":"4_RunnerIntroduction/#key-points","title":"Key Points","text":"<ol> <li>Anonymous Namespace: Task classes are typically defined in anonymous namespaces to avoid exposing implementation details</li> <li>Thread Safety: The <code>run()</code> method can use <code>thread_local</code> to store results, ensuring thread safety</li> <li>Memory Management: Runner is responsible for managing the lifecycle of task instances; users don't need to manage them manually</li> <li>Error Handling: <code>runner_init</code> returns <code>nullptr</code> on failure; the return value should be checked</li> <li>History Format: Conversation history must be valid JSON format that meets the structured requirements</li> </ol>"},{"location":"4_RunnerIntroduction/#best-practices","title":"Best Practices","text":"<ol> <li>Task Registration: Use the <code>REGISTER_TASK</code> macro to register tasks in anonymous namespaces</li> <li>Error Handling: Always check the return value of <code>runner_init</code></li> <li>Resource Cleanup: Call <code>runner_release</code> to release resources after using the task</li> <li>Thread Safety: Use <code>thread_local</code> to store temporary results in multi-threaded environments</li> <li>History Management: Properly construct and pass conversation history, ensuring correct format</li> </ol>"}]}