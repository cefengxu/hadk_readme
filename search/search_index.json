{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"HADK Development Documentation","text":"<p>Welcome to the HADK framework development documentation. </p> <p>We came here to code a Hybrid Agent and chew bubblegum... and I'm all out of bubblegum !</p>"},{"location":"#what-it-is","title":"What It Is","text":"<p>HADK (Hybrid Agent Development Kit) is a C++-based, cross-platform framework for building intelligent agents. Built on computational graph principles, it offers a modular architecture with rich functional nodes and flexible orchestration. Developers can quickly create agents with advanced reasoning, tool integration, and multimodal capabilities that run seamlessly across Windows, Linux, and Android.</p>"},{"location":"#its-advantages","title":"Its Advantages","text":"<ul> <li>Cross-platform: Write once, run on Windows, Linux, and Android with a unified API</li> <li>Node-based Architecture: Computational graph-based orchestration with conditional routing, nested flows, and loops</li> <li>Type Safety: Compile-time type checking via C++ templates to catch errors early</li> <li>High Performance: Native compilation with zero-copy optimization and concurrent processing</li> <li>Tool Ecosystem: Integrate local tools, remote servers (MCP/SSE), or build your own</li> <li>Modular: Clean separation of concerns for easy extension and reuse</li> </ul>"},{"location":"#development-environment","title":"Development Environment","text":"<ul> <li>C++ Compiler: Supports C++17 or higher</li> <li>CMake: Version 3.24 or higher</li> <li>Operating System: Windows 10+, Linux (Ubuntu 20.04+), Android (API 21+)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#basic-concepts","title":"Basic Concepts","text":"<ul> <li>Chat Node - Chat node integrated with large language models</li> <li>Tool Node - Tool management node</li> <li>Custom Node - User-defined node for custom logic</li> <li>CE Node - Context engine node for managing conversation history</li> <li>Chain and Flow - Workflow management</li> <li>Route - Routing node</li> </ul>"},{"location":"#application-development","title":"Application Development","text":"<ul> <li>Single Node Agent - Single node agent development example</li> <li>Normal Agent - Normal agent development example</li> <li>Three Node Agent - Three node agent development example</li> <li>Inja Template Formatting - Inja template engine tutorial</li> <li>CoT Agent - CoT (Chain of Thought) agent development example</li> <li>Batch Node - Batch Node development example</li> <li>Chat Bot - Simple Chat Bot development example</li> </ul>"},{"location":"#runner-framework","title":"Runner Framework","text":"<ul> <li>Runner Framework API - Runner Framework API documentation</li> <li>Runner Single Node Agent - Runner single node agent example</li> <li>Runner Normal Agent - Runner normal agent example</li> <li>Runner Three Node Agent - Runner three node agent example</li> <li>Runner Chat Bot - Runner chat bot example</li> </ul>"},{"location":"#android","title":"Android","text":"<ul> <li>Android Platform - Cross Compilation and Android Project</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Please download the HADK dynamic library from this link</p> <p>Selecting the chapter you're interested in from the left navigation bar to start reading. </p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>Thanks to the following open-source projects for their support:</p> <ul> <li>cpr - HTTP client</li> <li>nlohmann/json - JSON library</li> <li>spdlog - Logging library</li> <li>yaml-cpp - YAML parsing library</li> <li>inja - Template engine</li> </ul>"},{"location":"#contact","title":"Contact","text":"<ul> <li>liusong9@lenovo.com</li> <li>zengjl1@lenovo.com</li> <li>moutz1@lenovo.com</li> <li>xufeng8@lenovo.com</li> </ul>"},{"location":"1-1_BasicConceptsChatNode/","title":"Chat Node","text":"<p>Chat Node is a node component that integrates large language models (LLM) and function calling capabilities, supporting both online and offline model invocation. Its interface strictly follows the OpenAI Chat Completion API specification.</p>"},{"location":"1-1_BasicConceptsChatNode/#building-a-chat-node","title":"Building a Chat Node","text":"<p>Build a chat node in the following way, allowing parameter configuration for model invocation:</p> <pre><code>chat_node::chat_node_settings s_generate;\ns_generate.llm_mode = chat_node::LLMMode::OpenAI;  //default\ns_generate.llm_url = \"http://3rd/api/chat\";\ns_generate.llm_key = \"3rdkey\"; \ns_generate.model = \"gpt-4.1\";\ns_generate.temperature = 0.7;\ns_generate.top_p = 0.95;\ns_generate.max_tokens = 4096;\ns_generate.tool_choice = \"none\";\nconst auto generate_node = std::make_shared&lt;chat_node::ChatNode&lt;std::string, std::string&gt;&gt;(s_generate);\n</code></pre> <p>Currently, two LLM Modes are supported, allowing you to use a local Ollama model via:</p> <pre><code>chat_node::chat_node_settings s_generate;\ns_generate.llm_mode = chat_node::LLMMode::Ollama;\ns_generate.model = \"qwen3-vl:4b\"; // model to use\ns_generate.llm_url = \"http://127.0.0.1:11434/api/chat\";\ns_generate.llm_key = \"\";\ns_generate.temperature = 0.7;\ns_generate.top_p = 0.95;\ns_generate.max_tokens = 4096;\ns_generate.tool_choice = \"none\";\nconst auto generate_node = std::make_shared&lt;chat_node::ChatNode&lt;std::string, std::string&gt;&gt;(s_generate);\n</code></pre> <p>Otherwise , you can configure the following environment variables for LLM Model:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\n</code></pre>"},{"location":"1-1_BasicConceptsChatNode/#chat-node-input-and-output","title":"Chat Node Input and Output","text":"<p>The default data type for both input and output is <code>std::string</code> (for custom data types, please refer to <code>Advanced Usage</code>).</p>"},{"location":"1-1_BasicConceptsChatNode/#input-format","title":"Input Format","text":"<p>The input data structure strictly follows the OpenAI Chat Completion API specification, with the following format:</p> <pre><code>[\n  {\"role\":\"system\",\"content\":\"you are a helpful assistant\"},\n  {\"role\":\"user\",\"content\":\"who are you?\"}\n]\n</code></pre> <p>It also supports multimodal input (if the model supports it):</p> <pre><code>[\n    {\n        \"content\": \"You are a helpful assistant with multiple tools.\",\n        \"role\": \"system\"\n    },\n    {\n        \"content\": [\n            {\n                \"text\": \"what is this?\",\n                \"type\": \"text\"\n            },\n            {\n                \"image_url\": {\n                    \"detail\": \"low\",\n                    \"url\": \"https://1.bp.blogspot.com/529.jpg\"\n                },\n                \"type\": \"image_url\"\n            }\n        ],\n        \"role\": \"user\"\n    }\n]\n</code></pre> <p>Alternatively, images can be represented in base64 format:</p> <pre><code>[\n    {\n        \"content\": \"You are a helpful assistant with multiple tools.\",\n        \"role\": \"system\"\n    },\n    {\n        \"content\": [\n            {\n                \"text\": \"what is this?\",\n                \"type\": \"text\"\n            },\n            {\n                \"image_url\": {\n                    \"detail\": \"low\",\n                    \"url\": \"data:image/png;base64,dfadfnakjenqlkmdcklasjdflkadslfkadsokfqmldf\"\n                },\n                \"type\": \"image_url\"\n            }\n        ],\n        \"role\": \"user\"\n    }\n]\n</code></pre>"},{"location":"1-1_BasicConceptsChatNode/#output-format","title":"Output Format","text":"<p>The output data structure strictly follows the OpenAI Chat Completion API specification, with the following format:</p> <pre><code>[\n  {\"role\":\"system\",\"content\":\"you are a helpful assistant\"},\n  {\"role\":\"user\",\"content\":\"who are you?\"},\n  {\"role\":\"assistant\",\"content\":\"my name is bob.\"}\n]\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/","title":"Tool Node","text":"<p>Tool Node is a tool management node that supports the following features:</p> <ul> <li>Tool Registration and Management: Supports registration, execution, and destruction of MCP (Model Context Protocol) tools (SSE/STDIO) and local tools</li> <li>Function Call Integration: Can serve as the foundation for Chat Node's Function Call, or execute independently</li> <li>Custom Extensions: Supports user-defined local functions registered to the Tool Node</li> </ul>"},{"location":"1-2_BasicConceptsToolNode/#registering-tools","title":"Registering Tools","text":""},{"location":"1-2_BasicConceptsToolNode/#registering-local-tools","title":"Registering Local Tools","text":"<p>For local tool development specifications, please refer to <code>Local Tool Development Specifications</code>.</p> <pre><code>common_tools::tools::add_function_call(search_news_tool,\n    R\"({\"type\":\"function\",\"function\":{\"name\":\"search_news\",\"description\":\"Search for the latest news information using keywords\",\"parameters\":{\"type\":\"object\",\"properties\":{\"query\":{\"type\":\"string\",\"description\":\"Search keywords\",\"minLength\":1},\"time_range\":{\"type\":\"string\",\"enum\":[\"day\",\"week\"]},\"country\":{\"type\":\"string\",\"enum\":[\"china\",\"usa\",\"japan\"]}},\"required\":[\"query\"]}}})\"\n);\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/#registering-mcp-tools","title":"Registering MCP Tools","text":"<pre><code>const auto r1 = common_tools::tools::add_server(\n    R\"({\"WeatherServer\":{\"url\":\"http://18.119.131.41:8006\",\"sse_endpoint\":\"/sse\"}})\"\n);\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/#getting-tool-status-mcp-tools-only","title":"Getting Tool Status (MCP Tools Only)","text":"<p>The function to get the initialization status of MCP tools is as follows:</p> <pre><code>int status = common_tools::tools::get_server_init_status(std::string(name));\n</code></pre> <p>Parameter Description: - <code>name</code>: The unique identifier name of the tool (case-sensitive)</p> <p>Return Value: - <code>1</code>: Initialization successful - <code>2</code>: Initialization failed - <code>0</code>: Initialization in progress - <code>-1</code>: Unknown status</p>"},{"location":"1-2_BasicConceptsToolNode/#closing-tools-mcp-tools-only","title":"Closing Tools (MCP Tools Only)","text":"<p>The function to close all MCP tools is as follows:</p> <pre><code>common_tools::tools::shutdown_all_servers();\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/#executing-tools","title":"Executing Tools","text":""},{"location":"1-2_BasicConceptsToolNode/#automatic-invocation-via-chat-node","title":"Automatic Invocation via Chat Node","text":"<p>Tools can serve as Chat Node's Function Call functions, automatically judged and invoked by the model. By configuring as follows, Chat Node will have tool invocation capabilities:</p> <pre><code>chat_node::chat_node_settings s;\ns.model = \"gpt-4o-mini\";\ns.temperature = 0.7;\ns.max_tokens = 4096;\ns.tool_choice = \"auto\";  // Enable automatic tool selection\ns.tools_json = common_tools::tools::get_all_tools_json();  // Get all registered tools\nconst auto node = std::make_shared&lt;chat_node::ChatNode&lt;std::string, std::string&gt;&gt;(s);\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/#manually-invoking-a-specified-tool","title":"Manually Invoking a Specified Tool","text":"<p>Manually execute a specified tool in the following way:</p> <pre><code>std::string ws_out = common_tools::tools::call_tool(\"search_web2\", ws_in_json.dump());\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/#tool-input-format","title":"Tool Input Format","text":"<p>Tool input parameters strictly follow the OpenAI Chat Completion API's Function Call parameter format, passed as <code>std::string</code> type. The input is a JSON-formatted parameter string:</p> <pre><code>{\n  \"query\": \"Search keywords\",\n  \"time_range\": \"day\",\n  \"country\": \"china\"\n}\n</code></pre> <p>Example Code:</p> <pre><code>nlohmann::json params;\nparams[\"query\"] = \"Latest tech news\";\nparams[\"time_range\"] = \"day\";\nparams[\"country\"] = \"china\";\nstd::string arguments = params.dump();\n\nstd::string result = common_tools::tools::call_tool(\"search_news\", arguments);\n</code></pre>"},{"location":"1-2_BasicConceptsToolNode/#tool-output-format","title":"Tool Output Format","text":"<p>Tool output results strictly follow the OpenAI Chat Completion API's Function Call response format, returned as <code>std::string</code> type. The output is in JSON format:</p> <pre><code>{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Tool execution result content\"\n    }\n  ],\n  \"isError\": false\n}\n</code></pre> <p>Field Description: - <code>content</code>: Result content array, each element contains <code>type</code> and <code>text</code> fields - <code>isError</code>: Boolean value indicating whether an error occurred</p> <p>For local tool development sample, please refer to link.</p>"},{"location":"1-2_BasicConceptsToolNode/#optional-call-the-tool-in-custom-code","title":"Optional: Call the Tool in Custom Code**","text":"<p>You can invoke the custom tool\u2019s API directly, as shown below, ref link about Custom Code:</p> <pre><code>\n// \u6784\u5efa\u5de5\u5177\u53c2\u6570 JSON\nnlohmann::json arguments = {\n  {\"name\", \"search_web2\"},\n  {\"arguments\", nlohmann::json{{\"query\", in}}.dump()}\n};\n\nstd::string result = common_tools::tools::call_tool(\"search_news\", arguments.dump());\nnlohmann::json result_json = nlohmann::json::parse(result);\n\nif (!result_json[\"isError\"].get&lt;bool&gt;()) {\n    std::string text = result_json[\"content\"][0][\"text\"].get&lt;std::string&gt;();\n    // Process result\n}\n</code></pre> <p>For local tool development specifications, please refer to <code>Local Tool Development Specifications</code>.</p>"},{"location":"1-3_BasicConceptsDIYNode/","title":"Custom Node","text":""},{"location":"1-3_BasicConceptsDIYNode/#building-a-custom-node","title":"Building a Custom Node","text":"<p>You can build custom processing nodes using <code>OneFuncNode</code> to implement arbitrary business logic. Nodes define processing functions through Lambda expressions and support custom input and output types.</p>"},{"location":"1-3_BasicConceptsDIYNode/#basic-usage","title":"Basic Usage","text":"<pre><code>auto custom_node = std::make_shared&lt;nodeflow::OneFuncNode&lt;std::string, std::string&gt;&gt;(\n    [&amp;](const std::string&amp; input) -&gt; std::string {\n        // Custom processing logic\n        std::string processed = process_input(input);\n        return processed;\n    }\n);\n</code></pre>"},{"location":"1-3_BasicConceptsDIYNode/#practical-example","title":"Practical Example","text":"<p>The following example shows how to build a text polishing prompt generation node:</p> <pre><code>auto polish_prompt_node = std::make_shared&lt;nodeflow::OneFuncNode&lt;std::string, std::string&gt;&gt;(\n    [&amp;](const std::string&amp; draft) -&gt; std::string {\n        std::string polish_prompt = \n            \"Please rewrite the following draft to make it more friendly and engaging:\\n\\n\" +\n            draft + \"\\n\\n\"\n            \"Rewriting requirements:\\n\"\n            \"- The tone should be natural, like chatting, warm and infectious\\n\"\n            \"- You can add some rhetorical questions to guide readers to think\\n\"\n            \"- Appropriately add metaphors or analogies to make the content more vivid\\n\"\n            \"- The opening should catch the eye, and the ending should be powerful and memorable\\n\"\n            \"Final language: English.\\n\";\n\n        return polish_prompt;\n    }\n);\n</code></pre> <p>Notes: - <code>OneFuncNode&lt;IN, OUT&gt;</code>: Template parameters specify input and output types respectively - Lambda expression: Receives input parameters and returns processed output - Type safety: Compile-time checking of input and output type matching</p>"},{"location":"1-4_BasicConceptsCENode/","title":"CE Node","text":"<p>CE Node (Context Engine Node) is a context engine node component for managing and compressing conversation history. It supports two context management strategies: Trimming and Summarizing, which effectively control conversation history length and avoid exceeding model context window limits.</p>"},{"location":"1-4_BasicConceptsCENode/#building-a-ce-node","title":"Building a CE Node","text":"<p>Build a CE node as follows, allowing configuration of context management strategy and related parameters:</p> <pre><code>ce_node::ce_node_settings s_ce;\ns_ce.strategy = ContextStrategy::SUMMARIZING;\ns_ce.context_limit = 3; // Compression trigger threshold, number of history turns\ns_ce.keep_last_n_turns = 1; // Number of recent original message turns to keep\ns_ce.tool_trim_limit = 600; // Tool results are not very important in history messages, so the first 600 characters of tool results will be kept\ns_ce.summarizer_model = \"gpt-4o-mini\";\ns_ce.summarizer_max_tokens = 400;\nconst auto ce_node = std::make_shared&lt;ce_node::CeNode&lt;std::string, std::string&gt;&gt;(s_ce);\n</code></pre>"},{"location":"1-4_BasicConceptsCENode/#context-management-strategies","title":"Context Management Strategies","text":"<p>CE Node supports two context management strategies:</p>"},{"location":"1-4_BasicConceptsCENode/#trimming-strategy","title":"TRIMMING Strategy","text":"<p>The trimming strategy directly deletes history messages that exceed the limit, keeping the most recent <code>max_turns</code> conversation turns.</p> <pre><code>ce_node::ce_node_settings s_ce;\ns_ce.strategy = ContextStrategy::TRIMMING;\ns_ce.max_turns = 3; // Maximum number of history turns to keep\nconst auto ce_node = std::make_shared&lt;ce_node::CeNode&lt;std::string, std::string&gt;&gt;(s_ce);\n</code></pre>"},{"location":"1-4_BasicConceptsCENode/#summarizing-strategy","title":"SUMMARIZING Strategy","text":"<p>The summarizing strategy uses LLM to compress old history messages into summaries, keeping the most recent <code>keep_last_n_turns</code> original message turns. When the number of history turns exceeds <code>context_limit</code>, summarization compression is triggered.</p> <pre><code>ce_node::ce_node_settings s_ce;\ns_ce.strategy = ContextStrategy::SUMMARIZING;\ns_ce.context_limit = 3; // Compression trigger threshold, number of history turns\ns_ce.keep_last_n_turns = 1; // Number of recent original message turns to keep\ns_ce.tool_trim_limit = 600; // Number of characters to keep for tool results\ns_ce.summarizer_model = \"gpt-4o-mini\";\ns_ce.summarizer_max_tokens = 400;\nconst auto ce_node = std::make_shared&lt;ce_node::CeNode&lt;std::string, std::string&gt;&gt;(s_ce);\n</code></pre>"},{"location":"1-4_BasicConceptsCENode/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"1-4_BasicConceptsCENode/#common-parameters","title":"Common Parameters","text":"<ul> <li><code>strategy</code>: Context management strategy, options are <code>ContextStrategy::TRIMMING</code> or <code>ContextStrategy::SUMMARIZING</code>, default is <code>SUMMARIZING</code></li> </ul>"},{"location":"1-4_BasicConceptsCENode/#trimming-strategy-parameters","title":"TRIMMING Strategy Parameters","text":"<ul> <li><code>max_turns</code>: Maximum number of history turns to keep, default is 8</li> </ul>"},{"location":"1-4_BasicConceptsCENode/#summarizing-strategy-parameters","title":"SUMMARIZING Strategy Parameters","text":"<ul> <li><code>context_limit</code>: Compression trigger threshold, triggers summarization compression when history turns exceed this value, default is 5</li> <li><code>keep_last_n_turns</code>: Number of recent original message turns to keep, these messages won't be compressed, default is 2</li> <li><code>tool_trim_limit</code>: Number of characters to keep for tool results in history messages, default is 600</li> <li><code>summarizer_model</code>: Model name for summarization, default is \"gpt-4o-mini\"</li> <li><code>summarizer_max_tokens</code>: Maximum tokens for summarizer model, default is 400</li> <li><code>summarizer_tools_json</code>: Tool configuration for summarizer (JSON string), default is \"[]\"</li> <li><code>summarizer_tool_choice</code>: Tool selection strategy for summarizer, default is \"none\"</li> </ul>"},{"location":"1-4_BasicConceptsCENode/#ce-node-input-and-output","title":"CE Node Input and Output","text":"<p>The default data type is <code>std::string</code> (for custom data types, please refer to <code>Advanced Usage</code>).</p>"},{"location":"1-4_BasicConceptsCENode/#input-format","title":"Input Format","text":"<p>Input data format strictly follows the OpenAI Chat Completion API specification, format as follows:</p> <pre><code>[\n  {\"role\":\"system\",\"content\":\"you are a helpful assistant\"},\n  {\"role\":\"user\",\"content\":\"who are you?\"},\n  {\"role\":\"assistant\",\"content\":\"my name is bob.\"},\n  {\"role\":\"user\",\"content\":\"what can you do?\"}\n]\n</code></pre>"},{"location":"1-4_BasicConceptsCENode/#output-format","title":"Output Format","text":"<p>Output data format also follows the OpenAI Chat Completion API specification. Depending on the configured strategy, output may be:</p> <ul> <li>TRIMMING Strategy: Keeps the most recent <code>max_turns</code> original message turns</li> <li>SUMMARIZING Strategy: Compresses old messages beyond <code>keep_last_n_turns</code> into summaries, keeping the most recent <code>keep_last_n_turns</code> original message turns</li> </ul> <p>Output format example:</p> <pre><code>[\n  {\"role\":\"system\",\"content\":\"you are a helpful assistant\"},\n  {\"role\":\"assistant\",\"content\":\"[Summary of previous conversation]\"},\n  {\"role\":\"user\",\"content\":\"what can you do?\"}\n]\n</code></pre>"},{"location":"1-5_BasicConceptsChainFlow/","title":"Chain","text":"<p>The <code>chain</code> function is used to connect two nodes, creating a data flow path between nodes. An optional <code>action</code> parameter can be specified to create conditional connections.</p> <p>Function Signature:</p> <pre><code>template &lt;typename NodeA, typename NodeB&gt;\nvoid chain(\n    const std::shared_ptr&lt;NodeA&gt;&amp; a,      // Source node\n    const std::shared_ptr&lt;NodeB&gt;&amp; b,      // Target node\n    std::optional&lt;std::string&gt; action = std::nullopt  // Optional action identifier\n);\n</code></pre> <p>How It Works:</p> <ul> <li>When the <code>action</code> parameter is <code>std::nullopt</code>, it creates a default connection (unconditional connection)</li> <li>When the <code>action</code> parameter has a value, it creates a conditional connection, which only executes when the action value returned by the source node's <code>route</code> function matches the action specified in <code>chain</code></li> </ul> <p>Usage Example:</p> <pre><code>// Set routing: determine the returned action based on node output\nroute(decide_node, [](const std::string&amp;, const std::string&amp;) -&gt; std::optional&lt;std::string&gt; {\n    // Return different actions based on business logic\n    return \"search\";\n});\n\n// Create conditional connection: execute this connection when route returns \"search\"\nchain(decide_node, search_node, \"search\");\n\n// Create conditional connection: execute this connection when route returns \"answer\"\nchain(decide_node, answer_node, \"answer\");\n\n// Create default connection: execute unconditionally (used when route returns std::nullopt)\nchain(decide_node, default_node);\n</code></pre> <p>Notes: - The route function is called after the node execution completes - The returned action value must match the action specified in <code>chain</code> - When returning <code>std::nullopt</code>, use the default connection (chain without action parameter)</p>"},{"location":"1-5_BasicConceptsChainFlow/#flow","title":"Flow","text":"<p>Flow is the execution container for workflows, used in conjunction with Chain. It is responsible for managing node execution order and data flow. Through Flow, multiple nodes can be organized into a complete execution flow.</p>"},{"location":"1-5_BasicConceptsChainFlow/#basic-concepts-of-flow","title":"Basic Concepts of Flow","text":"<ul> <li>Workflow Container: Flow is the execution container for nodes, managing node lifecycle and execution order</li> <li>Start Node: Each Flow must specify a start node as the workflow entry point</li> <li>Automatic Execution: Flow automatically executes the workflow based on connections between nodes</li> <li>Type Safety: Flow supports typed input and output, ensuring type safety</li> </ul>"},{"location":"1-5_BasicConceptsChainFlow/#creating-and-executing-workflows","title":"Creating and Executing Workflows","text":""},{"location":"1-5_BasicConceptsChainFlow/#1-create-flow-object","title":"1. Create Flow Object","text":"<p>Use <code>std::make_shared</code> to create a Flow object:</p> <pre><code>auto f = std::make_shared&lt;nodeflow::Flow&gt;();\n</code></pre>"},{"location":"1-5_BasicConceptsChainFlow/#2-set-start-node","title":"2. Set Start Node","text":"<p>Specify the workflow's start node through the <code>start</code> method:</p> <pre><code>f-&gt;start(decide_node);  // decide_node as the workflow entry point\n</code></pre>"},{"location":"1-5_BasicConceptsChainFlow/#3-execute-workflow","title":"3. Execute Workflow","text":"<p>Use the <code>runWithInput</code> method to execute the workflow:</p> <pre><code>auto result = f-&gt;runWithInput&lt;std::string, std::string&gt;(input);\n</code></pre> <p>Function Signature:</p> <pre><code>template &lt;typename IN, typename OUT&gt;\nOUT runWithInput(const IN&amp; input);\n</code></pre> <p>Parameter Description: - Template parameter <code>IN</code>: Input data type (i.e., the first node's input) - Template parameter <code>OUT</code>: Output data type - <code>input</code>: Actual input data</p> <p>Return Value: - The workflow's final output result, type is <code>OUT</code></p>"},{"location":"1-6_BasicConceptsRoute/","title":"Route","text":"<p>The routing mechanism is used to dynamically determine the execution path of a workflow based on node input and output. Through the <code>route</code> function combined with the <code>chain</code> function, conditional branching and loop control can be implemented.</p>"},{"location":"1-6_BasicConceptsRoute/#route-function","title":"Route Function","text":"<p>Each node supports setting routing logic through the <code>route</code> function. The route function is implemented based on Lambda expressions, receiving the node's input and output as parameters, and returning the action identifier for the next step.</p>"},{"location":"1-6_BasicConceptsRoute/#function-signature","title":"Function Signature","text":"<pre><code>template &lt;typename Node, typename Selector&gt;\nvoid route(\n    const std::shared_ptr&lt;Node&gt;&amp; node, \n    Selector selector\n);\n</code></pre> <p>Parameter Description: - <code>node</code>: The node to set routing for - <code>selector</code>: Route selector function, type is <code>std::function&lt;std::optional&lt;std::string&gt;(const IN&amp;, const OUT&amp;)&gt;</code></p> <p>Return Value: - <code>std::optional&lt;std::string&gt;</code>: Returns an action string representing the next execution path, returns <code>std::nullopt</code> to use the default connection</p>"},{"location":"1-6_BasicConceptsRoute/#example-1-routing-based-on-decision-results","title":"Example 1: Routing Based on Decision Results","text":"<pre><code>route(decide_node, [&amp;](const std::string&amp; input, const std::string&amp; output) -&gt; std::optional&lt;std::string&gt; {\n    // Parse decision information from output\n    if (g_yaml_node[\"action\"].as&lt;std::string&gt;() == \"search\") {\n        return \"search\";  // Route to search node\n    }\n    if (g_yaml_node[\"action\"].as&lt;std::string&gt;() == \"answer\") {\n        return \"answer\";  // Route to answer node\n    }\n    return std::nullopt;  // Use default route\n});\n</code></pre>"},{"location":"1-6_BasicConceptsRoute/#example-2-routing-based-on-validation-results","title":"Example 2: Routing Based on Validation Results","text":"<pre><code>route(supervisor_node, [&amp;](const std::string&amp; input, const std::string&amp; output) -&gt; std::optional&lt;std::string&gt; {\n    if (g_validation_result.valid) {\n        return \"done\";   // Validation passed, route to completion node\n    } else {\n        return \"retry\"; // Validation failed, route to retry node\n    }\n});\n</code></pre>"},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/","title":"Single Node Agent Development Example","text":""},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/#overview","title":"Overview","text":"<p>In this example, we will construct only one Chat Node from HADK to materialize an Agent that can understand or invoke functions automatically according to user's query. This is a simple agent implementation that creates a single Chat Node and Flow for each request. ref. link</p>"},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"2-1_ApplicationDevelopmentSingleNodeAgent/#project-structure","title":"Project Structure","text":"<p>The <code>single_node</code> project consists of: - Library: <code>single_node</code> (shared library containing <code>koba_agent</code> implementation) - Executable: <code>single_node_app</code> (application layer)</p> <p>If you need a more advanced implementation with task management and Runner pattern, consider using <code>single_node_cls</code> instead.</p>"},{"location":"2-2_ApplicationDevelopmentNormalAgent/","title":"Normal Agent Development Example","text":""},{"location":"2-2_ApplicationDevelopmentNormalAgent/#overview","title":"Overview","text":"<p>The normal agent demonstrates how to use HADK's sequential workflow with context compression to implement a three-stage question-answering system. This example implements an agent that compresses conversation context, generates responses using LLM with tool support, and then polishes the final answer. ref. link</p> <p>Workflow Diagram:</p> <pre><code>Input \u2192 ce_node (Context Compression) \u2192 generate_node (Generate Response) \u2192 polish_node (Polish Response) \u2192 Output\n</code></pre> <p>Core Features: - Context compression: Automatically compresses conversation history when it exceeds the limit using SUMMARIZING strategy - Tool support: Generate node can use tools (e.g., search_web2) to gather information - Response polishing: Final polish node refines the generated response for better quality - Sequential workflow: Linear flow through three specialized nodes</p>"},{"location":"2-2_ApplicationDevelopmentNormalAgent/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"2-2_ApplicationDevelopmentNormalAgent/#project-structure","title":"Project Structure","text":"<p>The <code>normal_agent</code> project consists of: - Library: <code>normal_agent</code> (shared library containing <code>koba_agent</code> implementation) - Executable: <code>normal_agent_app</code> (application layer)</p>"},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/","title":"Conditional Routing Agent Development Example","text":""},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/#overview","title":"Overview","text":"<p>The conditional routing agent demonstrates how to use HADK's conditional routing (<code>route</code>) functionality to implement complex workflows. This example implements a research assistant agent that can dynamically decide whether to continue searching for information or directly answer questions based on the current context, supporting iterative searches until sufficient information is obtained. ref. link</p> <p>Workflow Diagram:</p> <pre><code>Input \u2192 decide_node (Decision) \u2192 [Conditional Routing]\n                                  \u251c\u2500 \"search\" \u2192 web_search_node (Web Search) \u2192 decide_node (Loop)\n                                  \u2514\u2500 \"answer\" \u2192 answer_node (Generate Answer) \u2192 Output\n</code></pre> <p>Core Features: - Conditional routing: Dynamically select the next node based on the decision node's output - Loop workflow: Supports looping between decision node and search node until sufficient information is obtained - Context accumulation: Each search result accumulates into the context for subsequent decisions</p>"},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"2-3_ApplicationDevelopmentThreeNodeAgent/#project-structure","title":"Project Structure","text":"<p>The <code>reflector_agent</code> project consists of: - Library: <code>reflector_agent</code> (shared library containing <code>koba_agent</code> implementation) - Executable: <code>reflector_agent_app</code> (application layer)</p>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/","title":"Inja Template Formatting Tutorial","text":""},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#overview","title":"Overview","text":"<p>Inja is a powerful C++ template engine. The HADK framework provides convenient template formatting functionality through the <code>ChatUtils::format_inja</code> function. Using Inja templates, you can easily insert variable values into template strings to achieve dynamic content generation.</p>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#basic-usage","title":"Basic Usage","text":""},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#function-signature","title":"Function Signature","text":"<pre><code>std::string ChatUtils::format_inja(\n    const std::string&amp; template_str,\n    const std::unordered_map&lt;std::string, std::any&gt;&amp; variables\n);\n</code></pre> <p>Parameter Description: - <code>template_str</code>: Template string using <code>{{variable_name}}</code> syntax to define placeholders - <code>variables</code>: Variable map, where keys are variable names and values are corresponding variable values (supports multiple types)</p> <p>Return Value: - Formatted string</p>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#template-syntax","title":"Template Syntax","text":"<p>Inja templates use double curly braces <code>{{variable_name}}</code> to define placeholders. The template engine replaces placeholders with corresponding variable values.</p>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#examples","title":"Examples","text":""},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#example-1-simple-context-formatting","title":"Example 1: Simple Context Formatting","text":"<p>This is the most basic usage, demonstrating how to use string variables for template replacement:</p> <pre><code>#include &lt;chat_utils.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;unordered_map&gt;\n#include &lt;any&gt;\n\nstd::string simple_template = R\"(\nHello, {{name}}!\n\nToday is a {{weather}} {{time}}.\n\nBased on your {{topic}}, I've prepared the following content for you:\n\n{{content}}\n\nI hope this information is helpful!\n)\";\n\nstd::unordered_map&lt;std::string, std::any&gt; simple_variables;\nsimple_variables[\"name\"] = std::string(\"Xiao Ming\");\nsimple_variables[\"weather\"] = std::string(\"sunny\");\nsimple_variables[\"time\"] = std::string(\"morning\");\nsimple_variables[\"topic\"] = std::string(\"study plan\");\nsimple_variables[\"content\"] = std::string(\"1. Complete math homework\\n2. Read English articles\\n3. Review history knowledge\");\n\ntry {\n    std::string formatted_simple = ChatUtils::format_inja(simple_template, simple_variables);\n    std::cout &lt;&lt; formatted_simple &lt;&lt; \"\\n\";\n} catch (const std::exception&amp; e) {\n    std::cerr &lt;&lt; \"Formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n}\n</code></pre> <p>Output Result:</p> <pre><code>Hello, Xiao Ming!\n\nToday is a sunny morning.\n\nBased on your study plan, I've prepared the following content for you:\n\n1. Complete math homework\n2. Read English articles\n3. Review history knowledge\n\nI hope this information is helpful!\n</code></pre>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#example-2-summary-formatting-reference-deepsearchcpp","title":"Example 2: Summary Formatting (Reference: deepsearch.cpp)","text":"<p>This example demonstrates how to build complex prompt templates, suitable for multi-iteration summary scenarios:</p> <pre><code>std::string summary_template = R\"(\nTask\n\nYou need to summarize information around a theme. Theme: `{{topic}}`\n\nWorkflow\n\n1. Carefully read all information, combine with the theme to understand and fully comprehend the context.\n\n2. Select content related to the theme and summarize the selected content.\n\n3. If the theme is question-based, you need to summarize and infer relevant answers, otherwise summarize normally based on the theme.\n\nRequirements\n\n- The word count must not be less than {{min_words}} words, must be as many as possible.\n\n- The summarized content must be from the information provided, you cannot make things up, especially time-related information.\n\n- The summarized content must be comprehensive enough.\n\n- Logical coherence, smooth sentences.\n\n- Provide the final result directly in markdown format.\n\nInformation to Summarize\n\n```{{summary_search}}```\n)\";\n\nstd::unordered_map&lt;std::string, std::any&gt; summary_variables;\nsummary_variables[\"topic\"] = std::string(\"History of Artificial Intelligence Development\");\nsummary_variables[\"min_words\"] = 3500;\n\n// Simulate multi-iteration summary content\nstd::vector&lt;std::string&gt; all_iteration_summary = {\n    \"First search: The concept of artificial intelligence was first proposed by John McCarthy in 1956.\",\n    \"Second search: Deep learning technology made breakthrough progress in the 2010s.\",\n    \"Third search: Large language models such as the GPT series attracted widespread attention in the 2020s.\"\n};\n\n// Connect multiple summaries with separators\nstd::string all_iteration_summary_str;\nfor (size_t i = 0; i &lt; all_iteration_summary.size(); ++i) {\n    all_iteration_summary_str += all_iteration_summary[i];\n    if (i &lt; all_iteration_summary.size() - 1) {\n        all_iteration_summary_str += \"\\n---\\n\";\n    }\n}\nsummary_variables[\"summary_search\"] = all_iteration_summary_str;\n\ntry {\n    std::string formatted_summary = ChatUtils::format_inja(summary_template, summary_variables);\n    std::cout &lt;&lt; formatted_summary &lt;&lt; \"\\n\";\n} catch (const std::exception&amp; e) {\n    std::cerr &lt;&lt; \"Formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n}\n</code></pre> <p>Output Result:</p> <pre><code>Task\n\nYou need to summarize information around a theme. Theme: `History of Artificial Intelligence Development`\n\nWorkflow\n\n1. Carefully read all information, combine with the theme to understand and fully comprehend the context.\n\n2. Select content related to the theme and summarize the selected content.\n\n3. If the theme is question-based, you need to summarize and infer relevant answers, otherwise summarize normally based on the theme.\n\nRequirements\n\n- The word count must not be less than 3500 words, must be as many as possible.\n\n- The summarized content must be from the information provided, you cannot make things up, especially time-related information.\n\n- The summarized content must be comprehensive enough.\n\n- Logical coherence, smooth sentences.\n\n- Provide the final result directly in markdown format.\n\nInformation to Summarize\n\n```First search: The concept of artificial intelligence was first proposed by John McCarthy in 1956.\n---\nSecond search: Deep learning technology made breakthrough progress in the 2010s.\n---\nThird search: Large language models such as the GPT series attracted widespread attention in the 2020s.```\n</code></pre>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#example-3-using-different-types-of-variables","title":"Example 3: Using Different Types of Variables","text":"<p>Inja templates support variables of multiple data types, including strings, integers, floating-point numbers, booleans, and container types:</p> <pre><code>std::string mixed_template = R\"(\nUser Information:\n\n- Name: {{name}}\n- Age: {{age}}\n- VIP Status: {{is_vip}}\n- Points: {{points}}\n- Tags: {{tags}}\n)\";\n\nstd::unordered_map&lt;std::string, std::any&gt; mixed_variables;\nmixed_variables[\"name\"] = std::string(\"Zhang San\");\nmixed_variables[\"age\"] = 28;\nmixed_variables[\"is_vip\"] = true;\nmixed_variables[\"points\"] = 1250.5;\n\nstd::vector&lt;std::string&gt; tags = {\"Active User\", \"Tech Enthusiast\", \"Early User\"};\nmixed_variables[\"tags\"] = tags;\n\ntry {\n    std::string formatted_mixed = ChatUtils::format_inja(mixed_template, mixed_variables);\n    std::cout &lt;&lt; formatted_mixed &lt;&lt; \"\\n\";\n} catch (const std::exception&amp; e) {\n    std::cerr &lt;&lt; \"Formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n}\n</code></pre> <p>Output Result:</p> <pre><code>User Information:\n\n- Name: Zhang San\n- Age: 28\n- VIP Status: true\n- Points: 1250.5\n- Tags: Active User, Tech Enthusiast, Early User\n</code></pre>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#complete-example-program","title":"Complete Example Program","text":"<p>The following is a complete example program demonstrating all three examples:</p> <pre><code>#include &lt;chat_utils.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;unordered_map&gt;\n#include &lt;any&gt;\n\nint main()\n{\n    std::cout &lt;&lt; \"=== Inja Template Formatting Examples === \\n\";\n\n    // Example 1: Simple context formatting\n    std::cout &lt;&lt; \"\u3010Example 1\u3011Simple Context Formatting\\n\";\n    std::string simple_template = R\"(\nHello, {{name}}!\n\nToday is a {{weather}} {{time}}.\n\nBased on your {{topic}}, I've prepared the following content for you:\n\n{{content}}\n\nI hope this information is helpful!\n\n)\";\n\n    std::unordered_map&lt;std::string, std::any&gt; simple_variables;\n    simple_variables[\"name\"] = std::string(\"Xiao Ming\");\n    simple_variables[\"weather\"] = std::string(\"sunny\");\n    simple_variables[\"time\"] = std::string(\"morning\");\n    simple_variables[\"topic\"] = std::string(\"study plan\");\n    simple_variables[\"content\"] = std::string(\"1. Complete math homework\\n2. Read English articles\\n3. Review history knowledge\");\n\n    try {\n        std::string formatted_simple = ChatUtils::format_inja(simple_template, simple_variables);\n        std::cout &lt;&lt; formatted_simple &lt;&lt; \"\\n\";\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"Formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n    }\n\n    std::cout &lt;&lt; \"\\n\" &lt;&lt; std::string(50, '-') &lt;&lt; \"\\n\";\n\n    // Example 2: Summary formatting similar to deepsearch.cpp\n    std::cout &lt;&lt; \"\u3010Example 2\u3011Summary Formatting (Reference: deepsearch.cpp)\\n\";\n    std::string summary_template = R\"(\nTask\n\nYou need to summarize information around a theme. Theme: `{{topic}}`\n\nWorkflow\n\n1. Carefully read all information, combine with the theme to understand and fully comprehend the context.\n\n2. Select content related to the theme and summarize the selected content.\n\n3. If the theme is question-based, you need to summarize and infer relevant answers, otherwise summarize normally based on the theme.\n\nRequirements\n\n- The word count must not be less than {{min_words}} words, must be as many as possible.\n\n- The summarized content must be from the information provided, you cannot make things up, especially time-related information.\n\n- The summarized content must be comprehensive enough.\n\n- Logical coherence, smooth sentences.\n\n- Provide the final result directly in markdown format.\n\nInformation to Summarize\n\n```{{summary_search}}```\n\n)\";\n\n    std::unordered_map&lt;std::string, std::any&gt; summary_variables;\n    summary_variables[\"topic\"] = std::string(\"History of Artificial Intelligence Development\");\n    summary_variables[\"min_words\"] = 3500;\n\n    // Simulate multi-iteration summary content\n    std::vector&lt;std::string&gt; all_iteration_summary = {\n        \"First search: The concept of artificial intelligence was first proposed by John McCarthy in 1956.\",\n        \"Second search: Deep learning technology made breakthrough progress in the 2010s.\",\n        \"Third search: Large language models such as the GPT series attracted widespread attention in the 2020s.\"\n    };\n\n    // Connect multiple summaries with separators\n    std::string all_iteration_summary_str;\n    for (size_t i = 0; i &lt; all_iteration_summary.size(); ++i) {\n        all_iteration_summary_str += all_iteration_summary[i];\n        if (i &lt; all_iteration_summary.size() - 1) {\n            all_iteration_summary_str += \"\\n---\\n\";\n        }\n    }\n    summary_variables[\"summary_search\"] = all_iteration_summary_str;\n\n    try {\n        std::string formatted_summary = ChatUtils::format_inja(summary_template, summary_variables);\n        std::cout &lt;&lt; formatted_summary &lt;&lt; \"\\n\";\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"Formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n    }\n\n    std::cout &lt;&lt; \"\\n\" &lt;&lt; std::string(50, '-') &lt;&lt; \"\\n\";\n\n    // Example 3: Using different types of variables\n    std::cout &lt;&lt; \"\u3010Example 3\u3011Using Different Types of Variables\\n\";\n    std::string mixed_template = R\"(\nUser Information:\n\n- Name: {{name}}\n- Age: {{age}}\n- VIP Status: {{is_vip}}\n- Points: {{points}}\n- Tags: {{tags}}\n\n)\";\n\n    std::unordered_map&lt;std::string, std::any&gt; mixed_variables;\n    mixed_variables[\"name\"] = std::string(\"Zhang San\");\n    mixed_variables[\"age\"] = 28;\n    mixed_variables[\"is_vip\"] = true;\n    mixed_variables[\"points\"] = 1250.5;\n\n    std::vector&lt;std::string&gt; tags = {\"Active User\", \"Tech Enthusiast\", \"Early User\"};\n    mixed_variables[\"tags\"] = tags;\n\n    try {\n        std::string formatted_mixed = ChatUtils::format_inja(mixed_template, mixed_variables);\n        std::cout &lt;&lt; formatted_mixed &lt;&lt; \"\\n\";\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"Formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n    }\n\n    std::cout &lt;&lt; \"\\n=== Examples Complete ===\\n\";\n\n    return 0;\n}\n</code></pre>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#supported-variable-types","title":"Supported Variable Types","text":"<p>The <code>ChatUtils::format_inja</code> function supports the following variable types:</p> <ul> <li>String (<code>std::string</code>): The most commonly used type, directly replaced in the template</li> <li>Integer (<code>int</code>, <code>long</code>, <code>long long</code>, etc.): Automatically converted to string</li> <li>Floating-point (<code>float</code>, <code>double</code>): Automatically converted to string</li> <li>Boolean (<code>bool</code>): Converted to \"true\" or \"false\"</li> <li>Container Types (<code>std::vector&lt;T&gt;</code>): Automatically converted to comma-separated string</li> </ul>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#error-handling","title":"Error Handling","text":"<p>When template formatting fails, the <code>format_inja</code> function throws a <code>std::exception</code> exception. It's recommended to use <code>try-catch</code> blocks to catch exceptions and perform appropriate error handling:</p> <pre><code>try {\n    std::string result = ChatUtils::format_inja(template_str, variables);\n    // Use formatted result\n} catch (const std::exception&amp; e) {\n    std::cerr &lt;&lt; \"Template formatting error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n    // Handle error situation\n}\n</code></pre>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use Raw String Literals: Use <code>R\"(...)\"</code> syntax to define template strings, which preserves newlines and special characters, improving readability</p> </li> <li> <p>Variable Naming Conventions: Use meaningful variable names for easier understanding and maintenance</p> </li> <li> <p>Error Handling: Always use <code>try-catch</code> blocks to catch possible exceptions</p> </li> <li> <p>Template Reuse: Define commonly used templates as constants or functions for easy reuse</p> </li> <li> <p>Type Safety: Ensure variable types match the usage scenarios in templates</p> </li> </ol>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#application-scenarios","title":"Application Scenarios","text":"<p>Inja template formatting is commonly used in the HADK framework for the following scenarios:</p> <ul> <li>Prompt Construction: Dynamically build LLM prompts, generating different prompts based on context and task requirements</li> <li>Message Formatting: Format user messages, system messages, etc.</li> <li>Summary Generation: Build multi-iteration summary prompts</li> <li>Tool Invocation Parameters: Dynamically generate parameter descriptions for tool invocations</li> </ul>"},{"location":"2-4_ApplicationDevelopmentInjaTemplateFormatting/#reference-resources","title":"Reference Resources","text":"<ul> <li>Inja Official Documentation</li> <li>HADK Framework Documentation</li> </ul>"},{"location":"2-5_ApplicationDevelopmentCoTAgent/","title":"CoT Agent Development Example","text":""},{"location":"2-5_ApplicationDevelopmentCoTAgent/#overview","title":"Overview","text":"<p>This example demonstrates how to use CoT (Chain of Thought) nodes from HADK to create an Agent that can solve complex problems through step-by-step reasoning. ref. link</p> <p>Workflow Diagram:</p> <pre><code>Input \u2192 extra_node (Extract Problem) \u2192 cot_node (Chain of Thought, Loop) \u2192 polish_node (Polish Answer) \u2192 Output\n</code></pre> <p>Core Features: - Chain of Thought reasoning: Uses iterative step-by-step reasoning to solve complex problems - Loop workflow: The cot_node loops until it reaches a conclusion, then routes to polish_node - Problem extraction: Extracts and clarifies the problem before reasoning - Answer polishing: Refines the final answer for better quality</p>"},{"location":"2-5_ApplicationDevelopmentCoTAgent/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"2-5_ApplicationDevelopmentCoTAgent/#project-structure","title":"Project Structure","text":"<p>The <code>cot_agent</code> project consists of: - Library: <code>cot_agent</code> (shared library containing <code>koba_agent</code> implementation) - Executable: <code>cot_agent_app</code> (application layer)</p>"},{"location":"2-6_ApplicationDevelopmentBatchNode/","title":"Batch Node Agent Development Example","text":""},{"location":"2-6_ApplicationDevelopmentBatchNode/#overview","title":"Overview","text":"<p>This example demonstrates how to use HADK <code>BatchFuncNode</code> to build a simple batch processing flow, where multiple text inputs are generated, processed in batch, and then routed to the next node. ref. link</p> <p>Workflow Diagram:</p> <pre><code>Input \u2192 create_node (Generate Batch) \u2192 batch_node (Batch Processing) \u2192 summarize_node (Summarize) \u2192 Output\n</code></pre> <p>Core Features: - Batch processing: Uses <code>BatchFuncNode</code> to process multiple inputs in parallel - Vector transformation: Converts single input to vector of strings for batch processing - Sequential workflow: Linear flow through create, batch, and summarize nodes - Routing support: Uses routing values to connect nodes in the flow</p>"},{"location":"2-6_ApplicationDevelopmentBatchNode/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"2-6_ApplicationDevelopmentBatchNode/#project-structure","title":"Project Structure","text":"<p>The <code>batch_flow</code> project consists of: - Library: <code>batch_flow</code> (shared library containing <code>koba_agent</code> implementation) - Executable: <code>batch_flow_app</code> (application layer)</p>"},{"location":"2-7_ApplicationChatBot/","title":"Chat Bot Development Example","text":""},{"location":"2-7_ApplicationChatBot/#overview","title":"Overview","text":"<p>This example demonstrates how to construct a simple interactive chat bot based on Single Node Agent. The chat bot supports multi-turn conversations with automatic history management using the Runner pattern. ref. link</p> <p>Workflow Diagram:</p> <pre><code>User Input \u2192 Chat History \u2192 koba_agent (Single Node) \u2192 Update History \u2192 Display Response \u2192 Loop\n</code></pre> <p>Core Features: - Multi-turn conversations: Maintains full conversation context across turns - Automatic history management: Tracks and updates conversation history automatically - Runner pattern: Uses Runner framework for task management - Interactive console: Reads user input and displays responses in real-time</p>"},{"location":"2-7_ApplicationChatBot/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"2-7_ApplicationChatBot/#project-structure","title":"Project Structure","text":"<p>The <code>chat_bot</code> project consists of: - Library: <code>chat_bot</code> (shared library containing <code>koba_agent</code> implementation) - Executable: <code>loop_chat_rt_app</code> (interactive chat bot application)</p>"},{"location":"3-0_RunnerFrameworkAPI/","title":"Runner Framework API Documentation","text":""},{"location":"3-0_RunnerFrameworkAPI/#overview","title":"Overview","text":"<p>The Runner framework provides a unified interface for task management and execution in HADK. It allows you to define task classes, register them with the framework, and execute them through a simple C-style API. The framework handles task lifecycle management, including initialization, execution, and cleanup. ref. link</p> <p>Key Features: - Class-based task definition: Define tasks by inheriting from <code>hybrid_runner::task_base</code> - Automatic registration: Use <code>REGISTER_TASK</code> macro to register task classes - C-style API: Simple C functions for task lifecycle management - ABI-safe interface: Pure virtual interface ensures compatibility across DLL boundaries - History and input separation: Supports conversation history and current query as separate parameters</p>"},{"location":"3-0_RunnerFrameworkAPI/#core-components","title":"Core Components","text":""},{"location":"3-0_RunnerFrameworkAPI/#task_base-class","title":"task_base Class","text":"<p>The <code>hybrid_runner::task_base</code> class is the base class for all tasks:</p> <pre><code>namespace hybrid_runner {\n    class task_base : public i_task {\n    public:\n        void execute(const char* history, const char* input, void* user_data, runner_result_callback callback) final;\n\n    protected:\n        virtual std::string run(const char* input) = 0;\n    };\n}\n</code></pre> <p>Key Points: - Inherits from <code>i_task</code> interface for ABI safety - Implements <code>execute()</code> method that handles callback conversion - Users only need to implement <code>run(const char* input)</code> which returns <code>std::string</code> - The <code>history</code> parameter is handled internally by the framework</p>"},{"location":"3-0_RunnerFrameworkAPI/#register_task-macro","title":"REGISTER_TASK Macro","text":"<p>The <code>REGISTER_TASK</code> macro simplifies task registration:</p> <pre><code>#define REGISTER_TASK(NAME, TYPE) \\\n    static hybrid_runner::registrar&lt;TYPE&gt; registrar_##TYPE(NAME);\n</code></pre> <p>Usage:</p> <pre><code>class MyTask : public hybrid_runner::task_base {\nprotected:\n    std::string run(const char* input) override {\n        return \"Result\";\n    }\n};\n\nREGISTER_TASK(\"MyTask\", MyTask);\n</code></pre>"},{"location":"3-0_RunnerFrameworkAPI/#api-functions","title":"API Functions","text":""},{"location":"3-0_RunnerFrameworkAPI/#1-runner_init","title":"1. runner_init()","text":"<p>Initialize a task instance.</p> <p>Function Signature:</p> <pre><code>const char* runner_init(const char* class_name, const char* key);\n</code></pre> <p>Parameters: - <code>class_name</code>: Name of the registered task class (must match the name used in <code>REGISTER_TASK</code>) - <code>key</code>: Optional custom key for the task instance. If <code>nullptr</code>, the framework will auto-generate a unique key</p> <p>Return Value: - Returns the key string for the initialized task instance (either the provided key or auto-generated) - Returns <code>nullptr</code> if initialization fails (e.g., class name not found)</p> <p>Example:</p> <pre><code>// Auto-generated key\nconst char* key1 = runner_init(\"MyTask\", nullptr);\n\n// Custom key\nconst char* key2 = runner_init(\"MyTask\", \"custom_key_123\");\n</code></pre>"},{"location":"3-0_RunnerFrameworkAPI/#2-runner_run","title":"2. runner_run()","text":"<p>Execute a task with conversation history and current input.</p> <p>Function Signature:</p> <pre><code>const char* runner_run(const char* key, const char* history, const char* input);\n</code></pre> <p>Parameters: - <code>key</code>: The key returned by <code>runner_init()</code> identifying the task instance - <code>history</code>: JSON string containing conversation history in format <code>[[S],[U,A],[U,A,T,A]]</code> where each inner array represents a complete conversation turn - <code>input</code>: Current user query string</p> <p>Return Value: - Returns a JSON string containing the task execution result - Returns <code>nullptr</code> if execution fails</p> <p>History Format: The history parameter should be a JSON array of arrays, where: - <code>[[S]]</code>: System message as the first group - <code>[[S],[U,A]]</code>: System message followed by a user-assistant turn - <code>[[S],[U,A],[U,A,T,A]]</code>: System message followed by multiple turns, including tool calls</p> <p>Each inner array represents a complete conversation turn from start to finish.</p> <p>Example:</p> <pre><code>nlohmann::json history = nlohmann::json::array({\n    nlohmann::json::array({\n        {{\"role\", \"system\"}, {\"content\", \"You are a helpful assistant.\"}}\n    }),\n    nlohmann::json::array({\n        {{\"role\", \"user\"}, {\"content\", \"Hello\"}},\n        {{\"role\", \"assistant\"}, {\"content\", \"Hi there!\"}}\n    })\n});\n\nconst char* response = runner_run(key, history.dump().c_str(), \"What is the weather?\");\n</code></pre>"},{"location":"3-0_RunnerFrameworkAPI/#3-runner_release","title":"3. runner_release()","text":"<p>Release and clean up a task instance.</p> <p>Function Signature:</p> <pre><code>void runner_release(const char* key);\n</code></pre> <p>Parameters: - <code>key</code>: The key returned by <code>runner_init()</code> identifying the task instance to release</p> <p>Description: - Releases all resources associated with the task instance - Should be called when the task is no longer needed - After calling <code>runner_release()</code>, the key becomes invalid and should not be used again</p> <p>Example:</p> <pre><code>runner_release(key);\n</code></pre>"},{"location":"3-0_RunnerFrameworkAPI/#4-runner_register","title":"4. runner_register()","text":"<p>Register a task class with the framework (typically used internally by <code>REGISTER_TASK</code> macro).</p> <p>Function Signature:</p> <pre><code>void runner_register(const char* name, i_task* (*creator)(), void (*deleter)(i_task*));\n</code></pre> <p>Parameters: - <code>name</code>: Name to register the task class under - <code>creator</code>: Function pointer that creates a new task instance - <code>deleter</code>: Function pointer that deletes a task instance</p> <p>Description: - Low-level registration function - Usually not called directly; use <code>REGISTER_TASK</code> macro instead</p>"},{"location":"3-0_RunnerFrameworkAPI/#usage-example","title":"Usage Example","text":"<pre><code>#include &lt;runner/runner.h&gt;\n#include &lt;iostream&gt;\n#include &lt;nlohmann/json.hpp&gt;\n\n// 1. Define a task class\nclass MyTask : public hybrid_runner::task_base {\npublic:\n    MyTask() {\n        std::cout &lt;&lt; \"MyTask constructed\\n\";\n    }\n\n    ~MyTask() override {\n        std::cout &lt;&lt; \"MyTask destroyed\\n\";\n    }\n\nprotected:\n    std::string run(const char* input) override {\n        std::cout &lt;&lt; \"MyTask is running with input: \" &lt;&lt; (input ? input : \"null\") &lt;&lt; '\\n';\n        return \"MyTask Result\";\n    }\n};\n\n// 2. Register the task\nREGISTER_TASK(\"MyTask\", MyTask);\n\nint main() {\n    // 3. Initialize task\n    const char* key = runner_init(\"MyTask\", nullptr);\n    if (!key) {\n        std::cerr &lt;&lt; \"Failed to init MyTask\" &lt;&lt; std::endl;\n        return -1;\n    }\n\n    std::cout &lt;&lt; \"Initialized MyTask with key: \" &lt;&lt; key &lt;&lt; std::endl;\n\n    // 4. Prepare conversation history\n    nlohmann::json history = nlohmann::json::array({\n        nlohmann::json::array({\n            {{\"role\", \"system\"}, {\"content\", \"You are a helpful assistant.\"}}\n        })\n    });\n\n    // 5. Run task\n    const char* response = runner_run(key, history.dump().c_str(), \"Hello World\");\n    if (response) {\n        std::cout &lt;&lt; \"Result: \" &lt;&lt; response &lt;&lt; std::endl;\n    }\n\n    // 6. Release task\n    runner_release(key);\n\n    return 0;\n}\n</code></pre>"},{"location":"3-0_RunnerFrameworkAPI/#key-points","title":"Key Points","text":"<ol> <li>Task Definition: Inherit from <code>hybrid_runner::task_base</code> and implement the <code>run()</code> method</li> <li>Registration: Use <code>REGISTER_TASK</code> macro to register your task class with a unique name</li> <li>Lifecycle Management: Always call <code>runner_release()</code> after finishing with a task instance</li> <li>History Format: Use structured history format <code>[[S],[U,A],[U,A,T,A]]</code> for conversation context</li> <li>Key Management: Keys can be auto-generated (pass <code>nullptr</code>) or custom-specified</li> <li>Error Handling: Always check return values from <code>runner_init()</code> and <code>runner_run()</code> for <code>nullptr</code></li> </ol>"},{"location":"3-0_RunnerFrameworkAPI/#project-structure","title":"Project Structure","text":"<p>The Runner framework consists of: - Header: <code>hadk/include/runner/runner.h</code> (framework interface definition) - Implementation: Runner framework implementation (handles task registration and execution) - Example: <code>example/main.cpp</code> (demonstrates framework usage)</p>"},{"location":"3-1_AndroidPlatform/","title":"Android Platform Development","text":""},{"location":"3-1_AndroidPlatform/#overview","title":"Overview","text":"<p>HADK is a cross-platform development framework. This document provides complete examples for compiling and running agents on the Android platform, including cross-compilation configuration and Android Studio project integration. </p>"},{"location":"3-1_AndroidPlatform/#cross-compilation-reference","title":"Cross-compilation Reference","text":"<p>For complete cross-compilation examples and configurations, please refer to: Cross-Compilation Agent Example</p>"},{"location":"3-1_AndroidPlatform/#agent-development","title":"Agent Development","text":"<p>For the agent implementation used, please refer to: Single Node Agent Development Guide</p>"},{"location":"3-1_AndroidPlatform/#android-studio-integration","title":"Android Studio Integration","text":"<p>We provide a complete Android Studio example project that demonstrates how to call cross-compiled agents through <code>Kotlin + JNI</code> and execute  <code>local tool calls</code> to implement a simple chatbot.</p> <p>For the complete Android Studio example project, please refer to: Android Demo Project</p>"},{"location":"3-1_AndroidPlatform/#local-tools","title":"Local Tools","text":"<p>This demo provides the following local tools that can be called through function call:</p> <ul> <li>Control device Bluetooth on/off</li> <li>Dynamically adjust device screen brightness</li> <li>Control device flashlight on/off</li> <li>Control device volume</li> <li>Screen capture</li> </ul>"},{"location":"3-1_RunnerSingleNodeAgent/","title":"Single Node Agent Development With Runner (Class-based with Runner)","text":""},{"location":"3-1_RunnerSingleNodeAgent/#overview","title":"Overview","text":"<p>This module is a class-based refactoring of single_node, converting the original function-based implementation into an object-oriented class format using the Runner pattern. ref. link</p> <p>In this example, we construct a single Chat Node from HADK to materialize an Agent that can understand or invoke functions automatically according to user's query. The key difference from <code>single_node</code> is that this implementation uses:</p> <ul> <li>Class-based architecture: The agent logic is encapsulated in a <code>KobaAgentTask</code> class that inherits from <code>hybrid_runner::task_base</code></li> <li>Runner pattern: Uses the Runner framework (<code>runner_init</code>, <code>runner_run</code>, <code>runner_release</code>) for task management and execution</li> <li>Structured history format: Supports the new conversation history format <code>[[S],[U,A],[U,A,T,A]]</code> where each inner array represents a complete conversation turn</li> </ul>"},{"location":"3-1_RunnerSingleNodeAgent/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"3-1_RunnerSingleNodeAgent/#key-components","title":"Key Components","text":""},{"location":"3-1_RunnerSingleNodeAgent/#kobaagenttask-class","title":"KobaAgentTask Class","text":"<p>The <code>KobaAgentTask</code> class implements the core agent functionality:</p> <ul> <li>Inheritance: Inherits from <code>hybrid_runner::task_base</code> to integrate with the Runner framework</li> <li>Core method: Implements <code>run(const char* input)</code> which processes user queries and returns responses</li> <li>Node configuration: Uses a single Chat Node with configurable settings (model, temperature, tool choice, etc.)</li> <li>Tool management: Dynamically updates available tools on each run</li> </ul>"},{"location":"3-1_RunnerSingleNodeAgent/#runner-api","title":"Runner API","text":"<p>The module uses the Runner framework for task lifecycle management:</p> <ul> <li><code>runner_init()</code>: Initialize a task instance</li> <li><code>runner_run()</code>: Execute the task with conversation history and current query   <code>cpp   const char* response = runner_run(key, history.dump().c_str(), \"what is the weather in beijing ?\");</code>   The second parameter is the conversation history in JSON format <code>[[S],[U,A],[U,A,T,A]]</code>, and the third parameter is the current user query string.</li> <li><code>runner_release()</code>: Clean up and release the task instance</li> </ul>"},{"location":"3-1_RunnerSingleNodeAgent/#conversation-history-format","title":"Conversation History Format","text":"<p>The new structured history format groups messages by conversation turns:</p> <ul> <li><code>[[S]]</code>: System message as the first group</li> <li><code>[[S],[U,A]]</code>: System message followed by a user-assistant turn</li> <li><code>[[S],[U,A],[U,A,T,A]]</code>: System message followed by multiple turns, including tool calls</li> </ul> <p>Each inner array represents a complete conversation turn from start to finish.</p>"},{"location":"3-1_RunnerSingleNodeAgent/#differences-from-single_node","title":"Differences from single_node","text":"<ul> <li>Architecture: Class-based instead of function-based</li> <li>Task management: Uses Runner pattern for better lifecycle management</li> <li>History format: Supports structured history format <code>[[S],[U,A],[U,A,T,A]]</code></li> <li>Registration: Uses <code>REGISTER_TASK</code> macro to register the task class with the Runner framework</li> <li>Memory: Coming soon</li> <li>Context Maneager: Coming soon</li> </ul>"},{"location":"3-2_RunnerNormalAgent/","title":"Normal Agent Development With Runner (Class-based with Runner)","text":""},{"location":"3-2_RunnerNormalAgent/#overview","title":"Overview","text":"<p>This module is a class-based refactoring of normal_agent, converting the original function-based implementation into an object-oriented class format using the Runner pattern. ref. link</p> <p>The normal agent demonstrates how to use HADK's sequential workflow with context compression to implement a three-stage question-answering system. This example implements an agent that compresses conversation context, generates responses using LLM with tool support, and then polishes the final answer.</p> <p>Workflow Diagram:</p> <pre><code>Input \u2192 ce_node (Context Compression) \u2192 generate_node (Generate Response) \u2192 polish_node (Polish Response) \u2192 Output\n</code></pre> <p>The key difference from <code>normal_agent</code> is that this implementation uses:</p> <ul> <li>Class-based architecture: The agent logic is encapsulated in a <code>KobaAgentTask</code> class that inherits from <code>hybrid_runner::task_base</code></li> <li>Runner pattern: Uses the Runner framework (<code>runner_init</code>, <code>runner_run</code>, <code>runner_release</code>) for task management and execution</li> <li>Structured history format: Supports the new conversation history format <code>[[S],[U,A],[U,A,T,A]]</code> where each inner array represents a complete conversation turn</li> </ul> <p>Core Features: - Context compression: Automatically compresses conversation history when it exceeds the limit using SUMMARIZING strategy - Tool support: Generate node can use tools (e.g., search_web2) to gather information - Response polishing: Final polish node refines the generated response for better quality - Sequential workflow: Linear flow through three specialized nodes</p>"},{"location":"3-2_RunnerNormalAgent/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"3-2_RunnerNormalAgent/#key-components","title":"Key Components","text":""},{"location":"3-2_RunnerNormalAgent/#kobaagenttask-class","title":"KobaAgentTask Class","text":"<p>The <code>KobaAgentTask</code> class implements the core agent functionality:</p> <ul> <li>Inheritance: Inherits from <code>hybrid_runner::task_base</code> to integrate with the Runner framework</li> <li>Core method: Implements <code>run(const char* input)</code> which processes user queries through a three-node workflow and returns responses</li> <li>Three-node workflow: </li> <li>CE Node: Compresses conversation history when it exceeds the limit</li> <li>Generate Node: Generates responses using LLM with tool support</li> <li>Polish Node: Refines the generated response</li> <li>Tool management: Dynamically updates available tools on each run</li> </ul>"},{"location":"3-2_RunnerNormalAgent/#runner-api","title":"Runner API","text":"<p>The module uses the Runner framework for task lifecycle management:</p> <ul> <li><code>runner_init()</code>: Initialize a task instance</li> <li><code>runner_run()</code>: Execute the task with conversation history and current query   <code>cpp   const char* response = runner_run(key, history.dump().c_str(), \"using the tool, tell me some news about MLB .\");</code>   The second parameter is the conversation history in JSON format <code>[[S],[U,A],[U,A,T,A]]</code>, and the third parameter is the current user query string.</li> <li><code>runner_release()</code>: Clean up and release the task instance</li> </ul>"},{"location":"3-2_RunnerNormalAgent/#conversation-history-format","title":"Conversation History Format","text":"<p>The new structured history format groups messages by conversation turns:</p> <ul> <li><code>[[S]]</code>: System message as the first group</li> <li><code>[[S],[U,A]]</code>: System message followed by a user-assistant turn</li> <li><code>[[S],[U,A],[U,A,T,A]]</code>: System message followed by multiple turns, including tool calls</li> </ul> <p>Each inner array represents a complete conversation turn from start to finish.</p>"},{"location":"3-2_RunnerNormalAgent/#three-node-workflow-details","title":"Three-Node Workflow Details","text":"<p>The agent implements a three-node sequential workflow:</p> <ol> <li>CE Node (Context Compression): Compresses conversation history when it exceeds the limit</li> <li>Strategy: <code>SUMMARIZING</code> (alternative: <code>TRIMMING</code>)</li> <li>Context limit: 3 turns</li> <li>Keeps last 1 turn in original form</li> <li>Tool trim limit: 600 characters</li> <li>Summarizer model: <code>gpt-4o-mini</code></li> <li> <p>Summarizer max tokens: 400</p> </li> <li> <p>Generate Node: Generates responses using LLM with tool support</p> </li> <li>Model: <code>gpt-4o-mini</code></li> <li>Temperature: <code>0.7</code></li> <li>Top-p: <code>0.95</code></li> <li>Max tokens: <code>4096</code></li> <li> <p>Tool choice: <code>auto</code> (automatically decides when to use tools)</p> </li> <li> <p>Polish Node: Refines the generated response</p> </li> <li>Model: <code>gpt-4o-mini</code></li> <li>Temperature: <code>0.7</code></li> <li>Top-p: <code>0.95</code></li> <li>Max tokens: <code>4096</code></li> <li>Tool choice: <code>none</code> (no tool support for polishing)</li> <li>Uses a preprocessor to modify the input prompt for polishing</li> </ol> <p>Node Connection: - Nodes are connected sequentially using the <code>chain</code> function - Routing values are set using the <code>route</code> function - Flow: <code>ce_node</code> \u2192 <code>generate_node</code> \u2192 <code>polish_node</code></p>"},{"location":"3-2_RunnerNormalAgent/#differences-from-normal_agent","title":"Differences from normal_agent","text":"<ul> <li>Architecture: Class-based instead of function-based</li> <li>Task management: Uses Runner pattern for better lifecycle management</li> <li>History format: Supports structured history format <code>[[S],[U,A],[U,A,T,A]]</code></li> <li>Registration: Uses <code>REGISTER_TASK</code> macro to register the task class with the Runner framework</li> <li>Memory: Coming soon</li> <li>Context Manager: Coming soon</li> </ul>"},{"location":"3-2_RunnerNormalAgent/#key-points","title":"Key Points","text":"<ol> <li>Context Compression: The CE node automatically compresses conversation history when it exceeds the limit, using either SUMMARIZING or TRIMMING strategy</li> <li>Sequential Workflow: Nodes are connected in a linear sequence using the <code>chain</code> function</li> <li>Routing Values: Use the <code>route</code> function to set routing values for node outputs, which are used by <code>chain</code> to determine connections</li> <li>Tool Support: The generate node supports tool calling, allowing it to use tools like <code>search_web2</code> to gather information</li> <li>Response Polishing: The polish node refines the generated response without tool support, ensuring high-quality final output</li> <li>Preprocessor Usage: The polish node uses a preprocessor to modify the input prompt, instructing the LLM to polish the content</li> </ol>"},{"location":"3-3_RunnerThreeNodeAgent/","title":"Conditional Routing Agent Development With Runner (Class-based with Runner)","text":""},{"location":"3-3_RunnerThreeNodeAgent/#overview","title":"Overview","text":"<p>This module is a class-based refactoring of reflector_agent, converting the original function-based implementation into an object-oriented class format using the Runner pattern. ref. link</p> <p>The conditional routing agent demonstrates how to use HADK's conditional routing (<code>route</code>) functionality to implement complex workflows. This example implements a research assistant agent that can dynamically decide whether to continue searching for information or directly answer questions based on the current context, supporting iterative searches until sufficient information is obtained.</p> <p>Workflow Diagram:</p> <pre><code>Input \u2192 decide_node (Decision) \u2192 [Conditional Routing]\n                                  \u251c\u2500 \"search\" \u2192 web_search_node (Web Search) \u2192 decide_node (Loop)\n                                  \u2514\u2500 \"answer\" \u2192 answer_node (Generate Answer) \u2192 Output\n</code></pre> <p>The key difference from <code>reflector_agent</code> is that this implementation uses:</p> <ul> <li>Class-based architecture: The agent logic is encapsulated in a <code>KobaAgentTask</code> class that inherits from <code>hybrid_runner::task_base</code></li> <li>Runner pattern: Uses the Runner framework (<code>runner_init</code>, <code>runner_run</code>, <code>runner_release</code>) for task management and execution</li> <li>Structured history format: Supports the new conversation history format <code>[[S],[U,A],[U,A,T,A]]</code> where each inner array represents a complete conversation turn</li> </ul> <p>Core Features: - Conditional routing: Dynamically select the next node based on the decision node's output - Loop workflow: Supports looping between decision node and search node until sufficient information is obtained - Context accumulation: Each search result accumulates into the context for subsequent decisions</p>"},{"location":"3-3_RunnerThreeNodeAgent/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"3-3_RunnerThreeNodeAgent/#key-components","title":"Key Components","text":""},{"location":"3-3_RunnerThreeNodeAgent/#kobaagenttask-class","title":"KobaAgentTask Class","text":"<p>The <code>KobaAgentTask</code> class implements the core agent functionality:</p> <ul> <li>Inheritance: Inherits from <code>hybrid_runner::task_base</code> to integrate with the Runner framework</li> <li>Core method: Implements <code>run(const char* input)</code> which processes user queries through a conditional routing workflow and returns responses</li> <li>Three-node workflow with conditional routing: </li> <li>Decide Node: Analyzes context and decides next action (search or answer)</li> <li>Web Search Node: Executes web search using <code>search_web2</code> tool</li> <li>Answer Node: Generates final answer based on accumulated research</li> <li>State management: Uses member variables to track context, question, query, and YAML node during execution</li> </ul>"},{"location":"3-3_RunnerThreeNodeAgent/#runner-api","title":"Runner API","text":"<p>The module uses the Runner framework for task lifecycle management:</p> <ul> <li><code>runner_init()</code>: Initialize a task instance</li> <li><code>runner_run()</code>: Execute the task with conversation history and current query   <code>cpp   const char* response = runner_run(key, history.dump().c_str(), \"using the tool, tell me some news about MLB .\");</code>   The second parameter is the conversation history in JSON format <code>[[S],[U,A],[U,A,T,A]]</code>, and the third parameter is the current user query string.</li> <li><code>runner_release()</code>: Clean up and release the task instance</li> </ul>"},{"location":"3-3_RunnerThreeNodeAgent/#conversation-history-format","title":"Conversation History Format","text":"<p>The new structured history format groups messages by conversation turns:</p> <ul> <li><code>[[S]]</code>: System message as the first group</li> <li><code>[[S],[U,A]]</code>: System message followed by a user-assistant turn</li> <li><code>[[S],[U,A],[U,A,T,A]]</code>: System message followed by multiple turns, including tool calls</li> </ul> <p>Each inner array represents a complete conversation turn from start to finish.</p>"},{"location":"3-3_RunnerThreeNodeAgent/#conditional-routing-workflow-details","title":"Conditional Routing Workflow Details","text":"<p>The agent implements a three-node workflow with conditional routing and loops:</p> <ol> <li>Decide Node (Decision Node): Analyzes context and decides next action</li> <li>Model: <code>gpt-4o-mini</code></li> <li>Temperature: <code>0.7</code></li> <li>Top-p: <code>0.95</code></li> <li>Max tokens: <code>2048</code></li> <li>Tool choice: <code>none</code></li> <li>Output format: YAML with fields <code>thinking</code>, <code>action</code>, <code>reason</code>, <code>search_query</code>/<code>answer</code></li> <li>Preprocessor: Builds decision prompt with question, previous research context, and current time</li> <li> <p>Postprocessor: Parses YAML response and routes to search or answer based on action</p> </li> <li> <p>Web Search Node (Tool Node): Executes web search using <code>search_web2</code> tool</p> </li> <li>Uses <code>ToolNode</code> for structured tool invocation</li> <li>Preprocessor: Converts search query to tool input format</li> <li>Postprocessor: Extracts search results (title, URL, snippet) and accumulates context</li> <li> <p>Automatically routes back to decide_node after search completion</p> </li> <li> <p>Answer Node: Generates final answer based on accumulated research</p> </li> <li>Model: <code>gpt-4o-mini</code></li> <li>Temperature: <code>0.7</code></li> <li>Top-p: <code>0.95</code></li> <li>Max tokens: <code>4096</code></li> <li>Tool choice: <code>none</code></li> <li>Preprocessor: Converts input to Chat Completion format</li> </ol> <p>Node Connection: - Conditional routing: <code>decide_node</code> routes to <code>web_search_node</code> (route: \"search\") or <code>answer_node</code> (route: \"answer\") - Loop: <code>web_search_node</code> routes back to <code>decide_node</code> (route: \"decide\") - Flow: <code>decide_node</code> \u2192 (conditional) \u2192 <code>web_search_node</code> \u2192 <code>decide_node</code> (loop) OR <code>answer_node</code> \u2192 output</p> <p>Key Implementation Details: - Uses YAML parsing to extract decision information from decide node output - Context accumulation: Each search result is appended to member context variable - State management: Uses member variables to track question, context, and YAML node</p>"},{"location":"3-3_RunnerThreeNodeAgent/#differences-from-reflector_agent","title":"Differences from reflector_agent","text":"<ul> <li>Architecture: Class-based instead of function-based</li> <li>Task management: Uses Runner pattern for better lifecycle management</li> <li>History format: Supports structured history format <code>[[S],[U,A],[U,A,T,A]]</code></li> <li>Registration: Uses <code>REGISTER_TASK</code> macro to register the task class with the Runner framework</li> <li>State management: Uses member variables instead of global variables for state tracking</li> <li>Memory: Coming soon</li> <li>Context Manager: Coming soon</li> </ul>"},{"location":"3-3_RunnerThreeNodeAgent/#key-points","title":"Key Points","text":"<ol> <li>Conditional Routing: Use the <code>route</code> function to dynamically select the next node based on node output</li> <li>Loop Workflow: Routing can implement loops between nodes, supporting iterative search</li> <li>Context Management: Use member variables to manage context information in workflows</li> <li>YAML Parsing: Decision node returns YAML format, needs to be parsed to extract action type and parameters</li> <li>Tool Invocation: Use <code>ToolNode</code> to invoke tools with structured preprocessor and postprocessor functions</li> <li>ToolNode vs OneFuncNode: <code>ToolNode</code> provides a more structured way to call tools, with automatic handling of tool invocation format, while <code>OneFuncNode</code> requires manual tool invocation using <code>call_tool</code></li> </ol>"},{"location":"3-4_ApplicationChatBot/","title":"Chat Bot Development Example (Class-based with Runner)","text":""},{"location":"3-4_ApplicationChatBot/#overview","title":"Overview","text":"<p>This example demonstrates how to construct a simple interactive chat bot based on Single Node Agent using the Runner pattern. The chat bot supports multi-turn conversations with automatic history management and uses the structured conversation history format. ref. link</p> <p>Workflow Diagram:</p> <pre><code>User Input \u2192 Chat History \u2192 build_grouped_history() \u2192 runner_run() \u2192 Update History \u2192 Display Response \u2192 Loop\n</code></pre> <p>Core Features: - Multi-turn conversations: Maintains full conversation context across turns - Automatic history management: Tracks and updates conversation history automatically using <code>KBChatHistory</code> structure - Runner pattern: Uses Runner framework (<code>runner_init</code>, <code>runner_run</code>, <code>runner_release</code>) for task management - Structured history format: Converts flat message history to grouped format <code>[[S],[U,A],[U,A,T,A]]</code> for Runner API - Interactive console: Reads user input and displays responses in real-time</p>"},{"location":"3-4_ApplicationChatBot/#environment-configuration","title":"Environment Configuration","text":"<p>Before using the agent, you need to configure the following environment variables:</p> <pre><code>export LLM_API_URL=\"https://xxx/v1/chat/completions\"\nexport LLM_API_KEY=\"sk-zkxxxa5944\"\nexport TAVILY_API_URL=\"https://api.tavily.com/search\"\nexport TAVILY_API_KEY=\"tvly-dev-DPxxxwkFxVHPIf4D\"\n</code></pre> <p>Or configure directly in code:</p> <pre><code>chat_node::chat_node_settings model_setting;\nmodel_setting.llm_url = \"https://api.xxx.com/v1/chat/completions\";\nmodel_setting.llm_key = \"sk-you-key\";\n</code></pre>"},{"location":"3-4_ApplicationChatBot/#key-components","title":"Key Components","text":""},{"location":"3-4_ApplicationChatBot/#kbchathistory-structure","title":"KBChatHistory Structure","text":"<p>The <code>KBChatHistory</code> structure manages conversation history:</p> <ul> <li>Internal storage: Maintains flat message array <code>[S, U, A, T, A, ...]</code> internally</li> <li>History conversion: Uses <code>build_grouped_history()</code> to convert flat history to grouped format <code>[[S],[U,A],[U,A,T,A]]</code> when calling Runner API</li> <li>Automatic updates: Adds user input and updates history from agent responses</li> <li>History printing: Displays conversation history in grouped format when exiting</li> </ul>"},{"location":"3-4_ApplicationChatBot/#runner-api","title":"Runner API","text":"<p>The chat bot uses the Runner framework for task lifecycle management:</p> <ul> <li><code>runner_init()</code>: Initialize a task instance at startup</li> <li><code>runner_run()</code>: Execute the task with conversation history and current query   <code>cpp   const char* response = runner_run(key, history_json.dump().c_str(), input.c_str());</code>   The second parameter is the conversation history in JSON format <code>[[S],[U,A],[U,A,T,A]]</code>, and the third parameter is the current user query string.</li> <li><code>runner_release()</code>: Clean up and release the task instance on exit</li> </ul>"},{"location":"3-4_ApplicationChatBot/#conversation-history-format","title":"Conversation History Format","text":"<p>The chat bot uses the structured history format that groups messages by conversation turns:</p> <ul> <li><code>[[S]]</code>: System message as the first group</li> <li><code>[[S],[U,A]]</code>: System message followed by a user-assistant turn</li> <li><code>[[S],[U,A],[U,A,T,A]]</code>: System message followed by multiple turns, including tool calls</li> </ul> <p>Each inner array represents a complete conversation turn from start to finish. The <code>build_grouped_history()</code> function automatically groups messages by detecting user message boundaries.</p>"},{"location":"3-4_ApplicationChatBot/#workflow-details","title":"Workflow Details","text":"<p>The chat bot implements an interactive loop:</p> <ol> <li>Initialization: Creates <code>KBChatHistory</code> with system prompt, initializes tools, and creates Runner task instance</li> <li>Input Loop: </li> <li>Reads user input from console</li> <li>Adds user input to flat history</li> <li>Converts flat history to grouped format using <code>build_grouped_history()</code></li> <li>Calls <code>runner_run()</code> with grouped history and current query</li> <li>Updates flat history with assistant response</li> <li>Displays response to user</li> <li>Exit: Prints complete conversation history in grouped format and releases resources</li> </ol>"},{"location":"3-4_ApplicationChatBot/#project-structure","title":"Project Structure","text":"<p>The <code>chat_bot</code> project consists of: - Library: <code>chat_bot</code> (shared library containing <code>KobaAgentTask</code> class implementation) - Executable: <code>loop_chat_rt_app</code> (interactive chat bot application)</p>"},{"location":"3-4_ApplicationChatBot/#key-points","title":"Key Points","text":"<ol> <li>History Management: The chat bot maintains flat history internally but converts to grouped format when calling Runner API</li> <li>Automatic Grouping: The <code>build_grouped_history()</code> function automatically groups messages by detecting user message boundaries</li> <li>Incremental Updates: Only the latest assistant response is added to history after each turn, avoiding duplicate messages</li> <li>Runner Pattern: Uses Runner framework for better task lifecycle management compared to direct function calls</li> <li>Based on Single Node: The underlying agent uses a single Chat Node, same as <code>single_node_cls</code></li> </ol>"}]}